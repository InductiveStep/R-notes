# Categorical predictors and interactions

By the end of this chapter you will:

* Understand how to use R *factors*, which automatically deal with fiddly aspects of using categorical predictors in statistical models.
* Be able to relate R output to what is going on behind the scenes, i.e., coding of a category with $n$-levels in terms of $n-1$ binary 0/1 predictors.
* See how R does interactions effects (the `*` and `:` operators).

The lovely thing about the topics covered in this chapter is that they all transfer to every other model, e.g., logistic regression and other generalised linear models, multilevel models, and a huge number of models which you may encounter in future.


## Before we begin

We are going to persist with the prestige dataset another time. (Last chapter, promise.) You could continue on the end of last week's Markdown file or make a new one. 

Ensure these handy packages are installed (you will only have to do that once) and included:

```{r message=FALSE}
library(car)
library(tidyverse)
```


## The dataset

For ease of reference. Each row describes an occupation and aggregated data about that occupation (in 1970s Canada).

| Variable name | Description |
|---------------|-------------|
| occ       | Occupation |
| education | Average years of education for people in the job |
| income    | Average income in dollars |
| women     | Percentage of women in occupation |
| prestige  | A Pineo-Porter prestige score for the occupation with a possible range of 0-100 (higher = more prestigious)  |
| type      | "bc" = blue collar <br> "wc" = white collar <br> "prof" = professional, managerial, or technical |

Read it in:

```{r}
dat <- read.csv("prestige.csv")
```


This week we will explore whether there are mean differences in prestige between the three types of profession: blue collar, white collar, and professional.



## Factors

R has a special kind of object called a *factor* for representing categorical variables in statistical models. Ensure that R "knows" that you want to treat `type` as a factor by doing the following:

```{r}
dat$type <- factor(dat$type)
```


You can check whether a variable is a factor by doing:

```{r}
is.factor(dat$type)
```

Here's a variable that isn't:

```{r}
is.factor(dat$prestige)
```

It's numeric...

```{r}
is.numeric(dat$prestige)
```

You can also look up the "class" of a variable:

```{r}
class(dat$type)
```


You can check the levels of a factor by using the `levels` function:

```{r}
levels(dat$type)
```

The order is alphabetical by default and can easily be changed.

The `levels` function only works for factors:

```{r}
levels(dat$prestige)
```




## Visualising the data

(I go through this in some detail, for information. Feel free to skip over it on first reading.)

We will be exploring whether there are differences in a continuous variable (`prestige`) between levels of a categorical variable (`type`).

A traditional visualisation for this genre of comparison is the bar plot, with either standard error of the mean (SE mean) or standard deviation (SD) error bars.

These days, the recommendation is to look at the raw data before reducing it to means, for instance with a jitter plot.

```{r fig.height=3, fig.width=3}
set.seed(100) # Optional
dat %>%
  ggplot(aes(type, prestige, colour = type)) +
  geom_jitter(height = 0,
              width = .1,
              show.legend = FALSE) +
  ylim(0,100)
```

We can see 4 observations in an "NA" category. Those are missing data, i.e., values in the dataset which have not been assigned a type.

```{r}
dat %>%
  filter(is.na(type))
```


Also note that I have fixed the y-axis scale so that it covers the theoretical range of the prestige measure (0-100) rather than only the values present in the data.

Here is how to make the traditional bar plot. First calculate the means and standard error of the means for each group. The standard error is calculated as

$$
\frac{\mathit{SD}}{\sqrt{N}}
$$
where $N$ is the number of observations. R has a function called `sqrt` for calculating the square root:

```{r}
sqrt(4)
```


We use `group_by` as before to ask tidyverse to group observations by `type` of occupation, and then summarise the data. The `n` function simply counts the number of observations, which will also be calculated per group following the `group_by`.

```{r message=FALSE}
dat_means <- dat %>%
  group_by(type) %>%
  summarise(mean = mean(prestige),
            SD   = sd(prestige),
            SE   = SD/sqrt(n()),
            n    = n())
dat_means
```


Here is how I went about plotting these values.

I began by plotting the means:

```{r}
dat_means %>%
  ggplot(aes(type, mean)) +
  geom_bar(stat = "identity")
```

Then I added error bars:

```{r}
dat_means %>%
  ggplot(aes(type, mean)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = mean - SE,
                    ymax = mean + SE))
```

Following this, I fiddled around for fifteen minutes, wishing I had just looked up a bar plot I had made previously... (The lesson here is, build a stash of example R code you can quickly find, adapt, and reuse.)

```{r fig.height=4, fig.width=3}
dat_means %>%
  filter(!is.na(type)) %>%
  ggplot(aes(type, mean, fill = type)) +
  geom_bar(stat = "identity", show.legend = F) +
  geom_errorbar(aes(ymin = mean - SE,
                    ymax = mean + SE), width = 1/4) +
  xlab("Occupation type") +
  ylab("Mean prestige") +
  ylim(0, 100) +
  theme_classic() 
```

You might want to add standard deviation bars instead:

```{r fig.height=3, fig.width=3}
dat_means %>%
  filter(!is.na(type)) %>%
  ggplot(aes(type, mean, fill = type)) +
  geom_bar(stat = "identity", show.legend = F) +
  geom_errorbar(aes(ymin = mean - SD,
                    ymax = mean + SD), width = 1/4) +
  xlab("Occupation type") +
  ylab("Mean prestige") +
  ylim(0, 100) + 
  theme_classic() 
```

You might also want less vibrant colours, e.g., if you wish to include the graph in a publication:

```{r fig.height=3, fig.width=3}
dat_means %>%
  filter(!is.na(type)) %>%
  ggplot(aes(type, mean)) +
  geom_bar(stat = "identity", fill = "grey") +
  geom_errorbar(aes(ymin = mean - SD,
                    ymax = mean + SD), width = 1/4) +
  xlab("Occupation type") +
  ylab("Mean prestige") +
  ylim(0,100) +
  theme_classic() 
```



These are the chunk options I used to change the dimensions of the figure, which might be handy to know about too (they go at the top of the chunk):

```
{r fig.height=3, fig.width=3}
```


## The punchline: occupation type does predict prestige

This is where we are heading.

First, select the variables you want to model and remove missing values:

```{r}
dat_no_NAs <- dat %>%
  select(occ, prestige, type) %>%
  na.omit()
```

We could just have removed missing variables from the whole dataset -- it would have worked for the prestige dataset since there are only four missing values and they are all in one variable. However, for larger datasets doing so might easily wipe out everything! For more on handling missing data correctly, try the book by van Buuren (2018).

Now, fit an intercept-only model and compare it with a model which has `type` as a predictor. R will automatically do something sensible with `type` since it knows that it is a factor.

```{r}
intercept_only <- lm(prestige ~ 1, data = dat_no_NAs)
type_mod       <- lm(prestige ~ type, data = dat_no_NAs)
                             # (the 1 is implicit)

anova(intercept_only, type_mod)
```

Occupation type does indeed explain variation in prestige, $F(2,95) = 109.59$, $p < .001$.

An even easier way to do this is using `car`'s `Anova` command (uppercase *A*). This does not require you to fit an intercept-only model yourself and deals with multiple predictors (handy later):

```{r}
type_mod <- lm(prestige ~ type, data = dat_no_NAs)
Anova(type_mod)
```

Read on to see why this has worked and also to work out which of the three occupations differ from each other in prestige...


## Understanding factors in regression models

### How are categorical variables encoded? {.tabset}

Let's begin with an easier example with only two groups: blue collar and white collar occupations:

```{r}
just_two <- dat %>%
  filter(type %in% c("bc","wc")) %>%
  select(occ, prestige, type)
```

Here are the means of prestige for both groups:

```{r message=FALSE}
just_two %>%
  group_by(type) %>%
  summarise(mean_prestige = mean(prestige))
```


Next, let's fit a regression model, predicting prestige from occupation type:

```{r}
easier <- lm(prestige ~ type, data = just_two)
summary(easier)
```

Note how we told `lm` that the predictor is `type`; however, the summary is displaying a slope for `typewc`. Curious... 


#### Activity

Examine the estimates (the intercept and the slope for `typewc`) in this model and compare them with the means of the two groups. Can you see where the numbers come from?

#### Answer

The intercept is the same as the mean prestige for `bc` ($35.527$) and the slope for `typewc` is equal to the mean *difference* in prestige between the two groups: white collar minus blue collar ($42.243 - 35.527 = 6.716$).



### How are binary (two-level) categorical predictors encoded?

**First let me emphasise that you do not need to do this coding yourself; this section aims to demonstrate how the coding works, not how to carry out an analysis.**

When you have a binary categorical variable (one with two levels), one level is coded as 0 and the other as 1. By default, R chooses alphabetically.

Here is something equivalent -- a "hand coded" variable which is 1 for white collar occupations and 0 for blue collar:

```{r}
just_two$type_wc <- as.numeric(just_two$type == "wc")
```

Have a look, comparing the `type` column with `type_wc`:

```{r}
just_two
```

Now let's fit a regression model with this binary 0/1 variable as a predictor:

```{r}
the_binary <- lm(prestige ~ type_wc, data = just_two)
summary(the_binary)
```

Again, the intercept provides the mean prestige for blue collar and the slope for `type_wc` gives the mean *difference* between blue collar and white collar. This model is mathematically identical to the one that R built using the factor variable.

A (slightly jittered) picture might help you see why this is.

```{r}
set.seed(45)
just_two %>%
  ggplot(aes(type_wc,prestige)) +
  geom_jitter(width = .02, height = 0) +
  geom_abline(intercept = coef(the_binary)[1],
              slope     = coef(the_binary)[2])
```


The values at the left hand side, above 0, are for blue collar occupations and at the right hand size, above 1, are for white collar occupations.

We asked `lm` to estimate an intercept ($\beta_0$) and slope for `type_wc` ($\beta_1$).

$$
\mathtt{prestige} = \beta_0 + \beta_1 \times \mathtt{type\_wc}
$$

This is what it found:

$$
\mathtt{prestige} = 35.5  + 6.7 \times \mathtt{type\_wc}
$$

Interpret the regression model as usual: for every unit increase in `type_wc`, prestige increases by 6.7. That's the slope in the picture above.

But the `type_wc` variable only has two possible values, so this is the simplest example of "unit increase". It can be 1, in which case we get the mean of white collar:

$$
\begin{array}{rcl}
\mathtt{prestige} & = &  35.5  + 6.7 \times \mathtt{type\_wc}\\
                  & = &  35.5  + 6.7 \times 1 \\
                  & = &  35.5  + 6.7 \\   
                  & = &  42.2  
\end{array}
$$

Alternatively, the predictor can be zero, in which case we get the mean of blue collar:

$$
\begin{array}{rcl}
\mathtt{prestige} & = &  35.5  + 6.7 \times \mathtt{type\_wc}\\
                  & = &  35.5  + 6.7 \times 0 \\
                  & = &  35.5  + 0 \\   
                  & = &  35.5
\end{array}
$$


### Categorical predictors with 3 or more levels

In general, if a categorical variable has $n$ levels, then there will be $n-1$ binary predictors added to the regression model. One level must be chosen as the comparison level. Slopes for the predictors represent *differences* from that comparison level.

Here is a model, fitted using R's automatic coding (and summarised with the aid of a pipe, just to add a little variation):

```{r}
lm(prestige ~ type, data = dat_no_NAs) %>%
  summary()
```

Now let's do the same again "by hand", to show the coding:

```{r}
dat_no_NAs$type_prof <- as.numeric(dat_no_NAs$type == "prof")
dat_no_NAs$type_wc   <- as.numeric(dat_no_NAs$type == "wc")
```

There are now two predictors: `type_prof` is 1 when the occupation type is `prof` and 0 otherwise; `type_wc` is 1 when the occupation type is `wc` and 0 otherwise. Whenever the occupation type is `bc`, both of these predictors are zero.

Have a look to see what this has done, comparing the `type` column with `type_prof` and `type_wc`:

```{r}
dat_no_NAs
```


If we fit a model with these two "hand coded" predictors, the results are identical to what R did automatically:


```{r}
lm(prestige ~ type_prof + type_wc, data = dat_no_NAs) %>%
  summary()
```

Now the formula is:

$$
\begin{array}{rcl}
\mathtt{prestige} & = &  35.5  + 32.3 \times \mathtt{type\_prof} + 6.7 \times  \mathtt{type\_wc}
\end{array}
$$

There are three possibilities:

* `type_prof` and `type_wc` are both zero, so the model computes the mean prestige for blue collar workers.

$$
\begin{array}{rcl}
\mathtt{prestige} & = &  35.5  + 32.3 \times 0 + 6.7 \times  0\\
                  & = &  35.5
\end{array}
$$

* `type_prof` is 1 and `type_wc` is 0, so the model computes the mean prestige for professionals.

$$
\begin{array}{rcl}
\mathtt{prestige} & = &  35.5 + 32.3 \times 1 + 6.7 \times  0\\
                  & = &  35.5 + 32.3\\
                  & = &  67.8
\end{array}
$$

* `type_prof` is 0 and `type_wc` is 1, so the model computes the mean prestige for white collar workers.

$$
\begin{array}{rcl}
\mathtt{prestige} & = &  35.5 + 32.3 \times 0 + 6.7 \times  1\\
                  & = &  35.5 + 6.7\\
                  & = &  42.2 
\end{array}
$$


## Interpreting the coefficients {.tabset}

Here's the model again:

```{r}
lm(prestige ~ type, data = dat_no_NAs) %>%
  summary()
```

From this we can see that the prestige for professionals is 32.3 points more than for blue collar workers ($t = 14.5$, $p < .001$) and prestige for white collar workers is 6.7 points more than for blue collar ($t = 2.7$, $p = .007$); both differences are statistically significant.

We have a variable with three levels and this has ended up the regression model as two predictors, (1) `typewc` and (2) `typeprof`, the slopes of which represent the *difference* between (1) `wc` and `bc` and (2) `prof` and `bc`, respectively.

But this does not currently show us all combinations. What is the difference between `wc` and `prof`?

Here is `relevel` to the rescue. It relevels a factor so that whichever level you name comes first. You can add the command directly to a model specification like this:

```{r}
lm(prestige ~ relevel(type, "prof"), data = dat_no_NAs) %>%
  summary()
```


Or, do it a longer but clearer way, e.g.,

```{r}
dat_no_NAs$type_vs_prof <- relevel(dat_no_NAs$type, "prof")
myMod <- lm(prestige ~ type_vs_prof, data = dat_no_NAs)
summary(myMod)
```


The advantage of this is that we can have a look at the levels and check that `prof` comes first:

```{r}
levels(dat_no_NAs$type_vs_prof)
```



### Activity

Interpret the coefficients in this `myMod` model.


### Answer

Prestige for professionals is 32.3 points greater than for blue collar workers ($t = 14.5$, $p < .001$) and 25.6 points greater than for white collar workers ($t = 9.8$, $p < .001$).


## Checking all combinations

It can quickly become a pain to use `relevel` like this and there are various alternatives.

At this point, I wish I hadn't been so clever using a pipe and instead just saved the model. I shall do so now.

```{r}
das_Model <- lm(prestige ~ type, data = dat_no_NAs)
```


We will use the `emmeans` package.

```{r warning=FALSE}
library(emmeans)
```

The main function therein is `emmeans`. Fox and Weisberg (2019, p. 203) comment that the formula argument is "idiosyncratic". Anyway, here is how to use it:

```{r}
emmeans(das_Model, pairwise ~ type)
```

The first parameter is the name of the model, the second is a formula specifying which comparisons are required. The idiosyncracy in the model specification is that "pairwise" is not a variable in the data -- it's a type of comparison.

Compare and contrast with the results obtained by using `relevel` above.

The estimates are identical. One difference you will observe is that the p-values are adjusted for the number of comparisons. There has been an enormous quantity of ink spilled on the problem of multiple comparisons. You may recall a variety of corrections, often named after their creators, e.g., Bonferroni, Bonferroni-Holm (my favourite), and Šidák.

Here, the p-values are adjusted by "tukey method" which is Tukey's Honest Significant Differences (HSD) adjustment. (Tukey is another statistician.)



## The intercept is not always the mean of the comparison group

The intercept always works the same way: it is the mean of the outcome variable when the predictors are zero. In the examples we have looked at, this was the mean of the comparison group -- whatever happened to be the first level of the factor.

Let's try another model:

```{r}
more_data <- dat %>%
  select(occ, prestige, type, income) %>%
  na.omit()
type_and_income <- lm(prestige ~ I(income/1000) + type, data = more_data)
summary(type_and_income)
```


Now the intercept is *not* the mean prestige for blue collar workers, but rather it is predicted mean prestige for blue collar workers when income is zero.

By the way, we can use `emmeans` again to compare occupation types, holding income constant:

```{r}
emmeans(type_and_income, pairwise ~ type)
```



## Recap

Okay, we have covered rather a lot of ground there so let me summarise a swift method for testing whether a categorical predictor explains variance in some outcome. I'll use an  example where we also statistically adjust for another predictor (known as a covariate).

First, do whatever you need to do to obtain and tidy the data:

```{r}
more_data <- dat %>%
  select(occ, prestige, type, income) %>%
  na.omit()
```

Fit the model of interest:

```{r}
my_model <- lm(prestige ~ I(income/1000) + type, data = more_data)
```

Use `Anova` to see which predictors are statistically significant

```{r}
Anova(my_model)
```

Look at a summary:

```{r}
summary(my_model)
```

Use `emmeans` to check for any comparisons of interest which were not included in the way the factor was initially setup:

```{r}
emmeans(my_model, pairwise ~ type)
```

Don't forget to check model diagnostics...




## Challenge {.tabset}



### Activity

a. Fit a regression model with income as the outcome variable and occupation type as a predictor. Does type explain variance in outcome?

b. Is there a statistically significant difference between blue collar and white collar salaries?

c. Is there a statistically significant difference between white collar and professional salaries?

d. Are the residuals normally distributed?

e. Try logging income to see what impact it has on the residual distribution

f. Now compare all differences between groups


### Answers

**a. Fit a regression model with income as the outcome variable and occupation type as a predictor. Does type explain variance in outcome?**


```{r}
income_mod <- lm(income ~ type, data = dat)
Anova(income_mod)
```

Yes: $F(2,95) = 24.87$, $p < .001$


**b. Is there a statistically significant difference between blue collar and white collar salaries?**

First, let's have a look at the means, to help aid interpretation:

```{r message=FALSE}
dat %>%
  group_by(type) %>%
  summarise(mean_income = mean(income))
```

The means for blue collar and white collar look very similar, about \$300 difference. Let's test that difference:

```{r}
summary(income_mod)
```

There is no statistically significant difference, $t = 0.361$, $p = .71$.


**c. Is there a statistically significant difference between white collar and professional salaries?**

We can refit the model using `relevel` like so:

```{r}
income_mod_wc <- lm(income ~ relevel(type, "wc"), data = dat)
summary(income_mod_wc)
```

Yes, professional salaries are a mean \$5507 more than white collar salaries ($t = 5.78$, $p < .001$).


**d. Are the residuals normally distributed?**

The order of levels does not affect the residuals, so either model would do:

```{r}
qqPlot(resid(income_mod_wc))
```

Hmmmm something fishy for residuals above 5000. Let's look at the histogram in case it is more obvious:


```{r}
hist(resid(income_mod_wc))
```

Looks like they are no normally distributed, though maybe driven by outliers?



**e. Try logging income to see what impact it has on the residual distribution**


```{r}
dat$log_income <- log(dat$income)
log_income_mod <- lm(log_income ~ type, data = dat)
```

Now the residuals look much better:

```{r}
qqPlot(resid(log_income_mod))
```

**f. Now compare all differences between groups**


Let's use `emmeans`:

```{r}
emmeans(log_income_mod, pairwise ~ type)
```

Note that the estimates are on the log scale, e.g., the difference between professional and white collar salaries is 0.7 on the logged \$1000s scale.


## Brief introduction to interactions

Fox and Weisberg (2019, pp. 207-224) say lots about interactions. In this tutorial I just want to mention briefly what they are and how to ask R to model them.


### What is an interaction?

An interaction between two variables simply means that the relationship between the outcome and one of the variables is changed (or "moderated") by the other variable.
For example, we found that occupation type explains variation in prestige. It is possible that the magnitude of differences between occupation types depend on mean income.

Interactions can also be used to test for intersectional effects. Discrimination may, for instance, depend on sexism and racism -- both operate independently. However, additionally the differences between men and women on some variable like salary might also depend on ethnicity and/or on whether you are cisgender or transgender.

Arithmetically, an interaction between two variables, *A* and *B* is represented as a new variable which is *A* and *B* multiplied together. R does the coding for us.


###  How to test for interactions in R

This shows how to test for an interaction between income and occupation type.

Let's select some data again:

```{r}
dat_for_int <- dat %>%
  select(occ, prestige, type, income) %>%
  na.omit()
```


First, predict prestige from income (in thousands) and occupation type. This model has the so-called "main effects", i.e., without any interaction.

```{r}
dat$income_1000s <- dat$income/1000
main_effect_mod <- lm(prestige ~ income_1000s + type, data = dat)
summary(main_effect_mod)
```

We can then use `Anova` as before:

```{r}
Anova(main_effect_mod)
```

There are effects for both income ($F(1,94) = 33.1$, $p < .001$) and occupation type ($F(2,94) = 59.3$, $p < .001$).

Now, do differences between occupation types depend on income?

We can check as follows:

```{r}
interaction_mod <- lm(prestige ~ income_1000s + type +
                        income_1000s:type,
                      data = dat)
anova(main_effect_mod, interaction_mod)
```

And the answer is, yes; $F(2,92) = 14.0$, $p < .001$.

It is also possible to check both main effects (without interactions) and the interaction effect in one go:

```{r}
Anova(interaction_mod)
```

The line beginning `income_1000s:type` shows the test of the interaction. To find out more about the `Anova` command and the Type II sum of squares it calculates by default, see Fox and Weisberg (2019, p. 262-264).

### Understanding interactions

The most challenging aspect of interactions is that one can rapidly end up with enormously complex models which are difficult to interpret, even if everything is "statistically significant". The secret to making sense of them is to plot model predictions. We will explore the `ggeffects` package -- one easy way to do this -- in more depth when examining logistic regression, but here is a preview of how it helps:

```{r}
library(ggeffects)
ggpredict(interaction_mod, terms = c("income_1000s", "type")) %>%
  plot(add.data = TRUE, ci = FALSE)
```

Hopefully this graph illustrates what it means that the relationship between income and prestige depends on occupation type -- the slopes are different for each type.


### Further reading

You might be interested in Johnson-Neyman intervals. See the [interactions](https://github.com/jacob-long/interactions/) package.



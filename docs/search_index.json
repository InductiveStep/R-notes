[["index.html", "Using R for social research Preface", " Using R for social research Andi Fugard (they/them) (almost@gmail.com, @InductiveStep) 09 December 2020 Preface These notes are based on weekly tutorial sheets I developed for a postgraduate social science course in 2020 to complement weekly lectures and online seminars. They were written in R Markdown, using the bookdown package.1 The core texts we used were Fox and Weisberg (2019), An R Companion to Applied Regression (Third Edition) and Healy (2019) Data Visualization: A Practical Introduction  both fabulous for learning applied statistics using R. The main aim of these tutorials is to help people understand the books. All the datasets used are available in CRAN packages, e.g., car; however, I think it helps to learn how to read in csv files, so that is what I ask readers to do. Its easy but fiddly, e.g., because Windows hides file extensions by default so users dont always know what files are really called. All the necessary files are linked in each chapter. I have included exercises which all come with solutions; to hide these, simply use the scrollbar to ensure that they are off the edge of the visible part of the screen2 Feedback welcome. Using R for social research by Andi Fugard is licensed under a Creative Commons Attribution 4.0 International License. The GitHub repository is over here. I originally used the .tabset attribute to hide answers; however, it doesnt work for bookdown. "],["starting-rstudio.html", "Chapter 1 Starting RStudio 1.1 Download, install, and run 1.2 Make a new R Notebook file 1.3 Read the clues it provides 1.4 Save the file 1.5 Run the example R command 1.6 Clear it and try a sum 1.7 Did that help?", " Chapter 1 Starting RStudio This is an attempt to explain how to get going super-fast, mostly by using animated GIFs. For something more substantial, try Healys intro over here. 1.1 Download, install, and run First ensure that you have installed both RStudio and R. RStudio is a friendly user interface which makes it easier to run analyses. R is the software which does the number crunching. Once both are installed, run RStudio. 1.2 Make a new R Notebook file 1.3 Read the clues it provides Here they are: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Ctrl+Shift+Enter. In a couple of steps time, you are going to try this. 1.4 Save the file 1.5 Run the example R command Now try what the clue suggested: 1.6 Clear it and try a sum This step involves adding a chunk which is where R code goes  see the GIF below for how to do this 1.7 Did that help? If so, try the next chapter! Otherwise, I can again recommend Healys intro. "],["starting-r.html", "Chapter 2 Starting R 2.1 Arithmetic 2.2 Variables 2.3 A note on variable names 2.4 Vectors 2.5 Functions 2.6 More functions 2.7 Data frames 2.8 Loading data frames from a file 2.9 Packages 2.10 The end!", " Chapter 2 Starting R This chapter is a tour of some of the basics of the R language. Please follow along in RStudio as you read and try not to peek at the answers until you have given the activities a good go! By the end of this tutorial you will know how to: Do basic stuff in R like arithmetic. Assign results of calculations to variables and refer to these. Calculate descriptive statistics like mean and standard deviation (SD). Install add-on packages from the internet and include them. Open a dataset in R. This will prepare you for your first analysis of data. 2.1 Arithmetic You can use R as a calculator: + is addition - is subtraction * is multiplication / is division ^ is exponentiation So, for instance to calculate \\(\\frac{1}{2}\\) you do 1/2 To work out \\(2^3\\) do: 2^3 The usual mathematical order of precedence rules apply, so 1 + 2 * 3 gives different result to (1 + 2) * 3 (Try it!) 2.1.1 Activities Easing in tremendously gently What is the R code to calculate 2 + 2 How do you multiply 6 and 7? 2.1.2 Answers a. What is the R code to calculate 2 + 2 2 + 2 ## [1] 4 b. How do you multiply 6 and 7? 6 * 7 ## [1] 42 2.2 Variables R has a flexible way of creating and naming objects of various kinds, so that you can refer to them later. These objects may be things like the answer to a sum, a whole dataset, or the result of a regression analysis. Lets start with a sum  we will get to the more complex examples soon! Look at this R code: first_bit &lt;- 2 + 2 second_bit &lt;- first_bit / 8 Read as set first_bit to 2 plus 2 set second_bit to first_bit divided by 8 The &lt;- symbol is called an assignment statement and first_bit and second_bit are variable names. 2.2.1 Activity When you run the code above, you will see that nothing actually seems to happen. Can you guess what you need to do to get the answer to the second_bit sum? 2.2.2 Answer You just have to type and run the name of the variable. second_bit ## [1] 0.5 2.3 A note on variable names There are limitations to the names you can use for variables (but ways to get around those  for another time). To keep life simple: make sure variable names always start with a character, a to z. they can have a number after the first character you can use an underscore, \"_\", as a sort of space remember that variables are cAsE sEnSiTiVe and consider always using lower case try to make variable names meaningful 2.4 Vectors Vectors are a list of values of the same type of information. They usually come from a dataset, but you can also build them directly in R. Vectors can be of whole numbers c(3,1,4,1,3,1,1,3,2,1)  or (approximations of) real numbers c(1.2, 2.4, -2.3)  or character strings, which are often used to represent categorical information: gender &lt;- c(&quot;woman&quot;, &quot;woman&quot;, &quot;man&quot;, &quot;woman&quot;, &quot;genderqueer&quot;, &quot;genderqueer&quot;, &quot;gender fluid&quot;) In this last example, I have saved the vector in a variable called gender. 2.4.1 Activity Try making a vector of the members of your favourite band (or other grouping of people that makes sense to you) and save the result in a variable with a meaningful name. 2.4.2 Answer stereo_total &lt;- c(&quot;Françoise Cactus&quot;, &quot;Brezel Göring&quot;) 2.5 Functions A function in R is a magical computational machine which takes an input and transforms it in some way, giving an output. (This is not a formal definition.) Heres a vector: c(2,4,12) One of Rs functions calculates the mean of a vector of numbers: mean(c(2,4,12)) ## [1] 6 There is another for the median: median(c(2,4,12)) ## [1] 4 Since these are built into R, there is also help. Try typing: ?mean We can also apply functions to variables. Another useful function is table which creates a table of counts. So, in summary, to use a function you use its name and then tell it the inputs in parentheses. 2.5.1 Activity Make a table of the gender variable we created earlier. 2.5.2 Answer table(gender) ## gender ## gender fluid genderqueer man woman ## 1 2 1 3 Note how this gives the same result as: table(c(&quot;woman&quot;, &quot;woman&quot;, &quot;man&quot;, &quot;woman&quot;, &quot;genderqueer&quot;, &quot;genderqueer&quot;, &quot;gender fluid&quot;)) ## ## gender fluid genderqueer man woman ## 1 2 1 3 But without having to copy and paste the vector. 2.6 More functions Functions can have zero inputs. Try citation which tells you how to cite R in write-ups. citation() (You could also try without the parentheses  not something youd generally want to do on purpose  to see what happens.) Functions can have more than one input. Another handy function, called rnorm, generates random data from a normal distribution, with a given number of values, mean, and SD. Here is an example with 50 values with a mean of 20 and an SD of 5: random_values &lt;- rnorm(50, 20, 5) random_values ## [1] 17.936898 13.661468 18.299670 22.353483 23.613234 23.489314 20.176084 ## [8] 18.991016 20.297336 19.421076 16.228409 9.679832 22.818685 17.784582 ## [15] 15.677661 16.682719 15.277020 22.615267 30.458067 18.143680 17.104054 ## [22] 19.729769 21.150979 16.615123 23.119212 15.338861 22.717791 19.515446 ## [29] 16.541693 20.011065 14.588855 19.882224 25.225578 22.606624 19.320099 ## [36] 15.456918 18.440037 20.030812 23.397467 23.629645 20.306022 18.132887 ## [43] 20.422348 23.532638 19.367769 17.901262 18.316020 26.738726 17.147467 ## [50] 18.189729 When you run it you will probably get different numbers to me. Here is a histogram of values I got: hist(random_values) 2.6.1 Activity Try calculating the mean and standard deviation (the command is called sd) of the data you generated. What do you notice? 2.6.2 Answer Heres what I got: mean(random_values) ## [1] 19.56165 sd(random_values) ## [1] 3.613532 Unless you were very lucky, you probably discovered like me that the mean was different to 20 and the SD was different to 5. This is something we will return to in another session, but can you guess now why this has happened? 2.7 Data frames Data frames are tightly coupled collections of variables used as the fundamental data structure by most of Rs modeling software. In practice, data frames are usually read in from files (next section); however, it can be helpful to see how to make them by hand within R. To make them, use the function data.frame with named vectors comprising the data. Here is an example, created with copy and paste from Wikipedia. I have used newlines and space to make it readable. stereo_total_albums &lt;- data.frame(year = c(1995, 1997, 1998, 1999, 2001, 2002, 2005, 2007, 2009, 2010, 2011, 2012, 2015, 2016, 2019), album = c(&quot;Oh Ah!&quot;, &quot;Monokini&quot;, &quot;Juke-Box Alarm&quot;, &quot;My Melody&quot;, &quot;Musique Automatique&quot;, &quot;Trésors cachés&quot;, &quot;Do the Bambi&quot;, &quot;Paris-Berlin&quot;, &quot;No Controles&quot;, &quot;Baby ouh!&quot;, &quot;Underwater Love&quot;, &quot;Cactus versus Brezel&quot;, &quot;Yéyé Existentialiste&quot;, &quot;Les hormones&quot;, &quot;Ah! Quel Cinéma!&quot;)) Do not build data frames like this with real data as it is painfully dull. However, it did get the data into R. R will print this in a sensible way if you run the data frames name: stereo_total_albums ## year album ## 1 1995 Oh Ah! ## 2 1997 Monokini ## 3 1998 Juke-Box Alarm ## 4 1999 My Melody ## 5 2001 Musique Automatique ## 6 2002 Trésors cachés ## 7 2005 Do the Bambi ## 8 2007 Paris-Berlin ## 9 2009 No Controles ## 10 2010 Baby ouh! ## 11 2011 Underwater Love ## 12 2012 Cactus versus Brezel ## 13 2015 Yéyé Existentialiste ## 14 2016 Les hormones ## 15 2019 Ah! Quel Cinéma! A few things you might want to do with data frames include: Listing the variables: names(stereo_total_albums) ## [1] &quot;year&quot; &quot;album&quot; Finding out how many rows it has: nrow(stereo_total_albums) ## [1] 15 You will often want to refer to a vector within the data frame, which you do with the $ operator: stereo_total_albums$year ## [1] 1995 1997 1998 1999 2001 2002 2005 2007 2009 2010 2011 2012 2015 2016 2019 This is just like any other vector, so you could draw a histogram of the years of album releases: hist(stereo_total_albums$year, xlab = &quot;Year&quot;, main = &quot;Stereo Total albums&quot;) hist has automatically chosen to group years in fives based on how much data there was and how its distributed. Note also how I changed the default settings on x-axis label and title. 2.7.1 Activity Whats the median of the years albums were released? 2.7.2 Answer median(stereo_total_albums$year) ## [1] 2007 2.8 Loading data frames from a file It is easy to read files into R. This command loads an excerpt from the Gapminder dataset (extracted and tidied by Jennifer Bryan), which you can find here. Save the file in the same folder as your R or Rmd file and run: gap &lt;- read.csv(&quot;gapminder.csv&quot;) Here csv stands for comma separated values. If you open the file in, say, Notepad, then you will see why &quot;country&quot;,&quot;continent&quot;,&quot;year&quot;,&quot;lifeExp&quot;,&quot;pop&quot;,&quot;gdpPercap&quot; &quot;Afghanistan&quot;,&quot;Asia&quot;,1952,28.801,8425333,779.4453145 &quot;Afghanistan&quot;,&quot;Asia&quot;,1957,30.332,9240934,820.8530296 &quot;Afghanistan&quot;,&quot;Asia&quot;,1962,31.997,10267083,853.10071 &quot;Afghanistan&quot;,&quot;Asia&quot;,1967,34.02,11537966,836.1971382 &quot;Afghanistan&quot;,&quot;Asia&quot;,1972,36.088,13079460,739.9811058 &quot;Afghanistan&quot;,&quot;Asia&quot;,1977,38.438,14880372,786.11336 ... The values are separated by commas. If it worked, then the following command will show the top 5 rows: head(gap, 5) ## country continent year lifeExp pop gdpPercap ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 If it didnt work, then youre probably not using R markdown. You can try changing the working directory to the same place as your R source file. The variables in the dataset are as follows: country: The country continent: The continent year: ranges from 1952 to 2007 in increments of 5 years lifeExp: life expectancy at birth, in years pop: population gdpPercap: GDP per capita (US$, inflation-adjusted) We can plot life expectancy against the GDP per capita as follows: plot(lifeExp ~ gdpPercap, data = gap) The ~ is read as tilde (or sometimes twiddle). The x-axis labels may not be readable at first but we shall return to this in a later session. (If you cant wait, try this.) 2.8.1 Activity Plot life expectancy at birth (on the y-axis) against year (on the x-axis). 2.8.2 Answer plot(lifeExp ~ year, data = gap, xlab = &quot;Year&quot;, ylab = &quot;Life expectancy at birth&quot;) Again I have changed the axis label defaults. We will dramatically improve upon this in the next chapter. 2.9 Packages When you install R, it comes with a small collection of essential packages which consist of functions like mean, sd, and hist that we have used above. But a strength of R is that there are a HUGE number of packages written by people across the world who use R and these can easily be downloaded from the interweb and added using R. These add-ons do things like produce pretty data visualisations (e.g., used by the BBC Data and Visualisation Journalism team) and statistical models required by social scientists. The core recommended text by Fox and Weisberg (2019) also has an R package. Here is an example, using the praise package. To download and install the package, use this command: install.packages(&quot;praise&quot;) If this has worked, you will have seen text something like: trying URL &#39;https://cran.rstudio.com/bin/windows/contrib/4.0/praise_1.0.0.zip&#39; Content type &#39;application/zip&#39; length 19729 bytes (19 KB) downloaded 19 KB package praise successfully unpacked and MD5 sums checked Now the package is on your local computer but it isnt available to use yet. You also have to do: library(praise) (Note how there were no quotation marks here.) If this worked, then well not much will have happened. But R wont have complained. 2.9.1 Activity The praise packages provides a function also called praise, which requires no input to run. Give it a go  what happens? 2.9.2 Answer Ill run it a few times praise() ## [1] &quot;You are wonderful!&quot; praise() ## [1] &quot;You are stunning!&quot; praise() ## [1] &quot;You are hunky-dory!&quot; Not a big data science algorithm, but you may find it helpful as the course progresses :) 2.10 The end! praise() ## [1] &quot;You are awesome!&quot; "],["visualising-data-in-the-tidyverse.html", "Chapter 3 Visualising data in the tidyverse 3.1 Getting setup 3.2 An interlude on functions 3.3 A scatterplot in ggplot 3.4 Another aesthetic: colour 3.5 Another geom: jitter 3.6 Aggregating/summarising data by group 3.7 Pipes 3.8 Plot the mean life expectancy by continent 3.9 Yet another geom: line 3.10 Filtering data along the pipeline 3.11 Other handy tools: select, slice, bind, and arrange 3.12 Filtering for members of a vector 3.13 Final challenge 3.14 More ideas for visualisations", " Chapter 3 Visualising data in the tidyverse By the end of this chapter you will: Have explored a key example visualisation in depth, using packages in the tidyverse. Understand what happens when you tweak this example in various ways. Know where to look for ideas and code for other visualisations. 3.1 Getting setup You will need to create a new R or Markdown (Rmd) file (depending on your preference  I recommend Markdown) and save it somewhere sensible where you can find it again in a few months time. We will be using the Gapminder dataset dataset from last time. gap &lt;- read.csv(&quot;gapminder.csv&quot;) Previously we used the base plot function: plot(lifeExp ~ year, data = gap, xlab = &quot;Year&quot;, ylab = &quot;Life expectancy at birth&quot;) We can do better than this. A collection of packages called the tidyverse has become an industry standard in R (though see also an alternate view). This command will include tidyverse and make a bit of noise as it arrives library(tidyverse) If that didnt work, there are two things you can do. You could try saving your R/Rmd file. This may prompt RStudio to notice that the package isnt installed and ask you if you want to install it. Alternatively, just use the install.packages command as per last chapter: install.packages(&quot;tidyverse&quot;) To see if that worked, run this: ggplot(data = gap, mapping = aes(x = year, y = lifeExp)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;) Ta-da: a graph! This used ggplot, which is part of the ggplot2 package which, in turn, is part of tidyverse. The rest of this tutorial will explore how to develop this into a more useful visualisation. 3.2 An interlude on functions Previously, I described R functions as magical computational machines which take inputs and transform them in some way, giving an output. Above, we have seen that the output of a function can be a picture. It can also be a vibration (thats how sounds are made) or anything else that can be plugged into a computer. It might be a humble number, like a mean. Sometimes Ill call functions commands and sometimes Ill call the inputs options or parameters or arguments. Hopefully it will be clear from the context what I mean. If not, even after scratching your head, then do ask! 3.3 A scatterplot in ggplot Lets build the previous example step-by-step. ggplot(data = gap, mapping = aes(x = year, y = lifeExp)) This first part tells ggplot what data to use and an aesthetic mapping. Aesthetics in tidyverse are properties of the objects in your plot and the mapping tells ggplot how those objects relate to your data. Two basic properties are x and y locations on a plot. Here they have been mapped to year and life expectancy, respectively. When you run that code, you will see that nothing was actually done with the mappings. The next stage is to add a geom  a geometric object  for each row of data. Thats where the point geom, geom_point, comes in: ggplot(data = gap, mapping = aes(x = year, y = lifeExp)) + geom_point() Note how the + symbol is used here to mean adding elements to a plot. The meaning of + depends on context. I could also have made this plot by giving a name to the first part: the_basic_plot &lt;- ggplot(data = gap, mapping = aes(x = year, y = lifeExp)) Then added this to the geom: plot_with_stuff_on &lt;- the_basic_plot + geom_point() The plot hasnt displayed yet, though. 3.3.1 Warm-up activity What do you need to do to get the plot_with_stuff_on plot to display? How could you change the axis labels on plot_with_stuff_on? (Look up for a clue!) 3.3.2 Answer a. What do you need to do to get the plot_with_stuff_on plot to display? plot_with_stuff_on b. How could you change the axis labels on plot_with_stuff_on? Either: plot_with_stuff_on + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;) Or: final_plot &lt;- plot_with_stuff_on + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;) final_plot 3.4 Another aesthetic: colour This is a simple change, but begins to highlight patterns in the data. Here I have just copied and pasted a chunk from above and added the mapping colour = continent. ggplot(data = gap, mapping = aes(x = year, y = lifeExp, colour = continent)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;) Can you spot any patterns in the graph? A legend has appeared at the right hand side explaining what the colours represent. By default the legend title is the same as the variable name. In this case its continent which is clear, but sometimes it will be something like group_2_id which is less pleasing on the eye (and I cringe when I see something like this in a journal article). The legend title is easy to change by adding another option to labs: ggplot(data = gap, mapping = aes(x = year, y = lifeExp, colour = continent)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;, colour = &quot;Continent&quot;) Now the legend has an uppercase C. 3.5 Another geom: jitter Making graphs often involves playing around with different ways of showing the information. Heres the jitter geom, which is the same as the point geom but with a small amount of random variation to the location of each point (see ?geom_jitter). ggplot(data = gap, mapping = aes(x = year, y = lifeExp, colour = continent)) + geom_jitter() + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;, colour = &quot;Continent&quot;) 3.5.1 Activity to develop your help-searching skill! How can you vary the amount of jitter? Tip: you might find the help useful: ?geom_jitter Alternatively, try Google. If that doesnt deliver anything useful, try this reference link. 3.5.2 Answer There are two options, width and height, which specify how wide the jitteriness is. Set these to zero, and the plot is indistinguishable from the point geom: ggplot(data = gap, mapping = aes(x = year, y = lifeExp, colour = continent)) + geom_jitter(width = 0, height = 0) + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;, color = &quot;Continent&quot;) Heres a little jitter added only to the width: ggplot(data = gap, mapping = aes(x = year, y = lifeExp, colour = continent)) + geom_jitter(width = 1, height = 0) + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;, color = &quot;Continent&quot;) 3.6 Aggregating/summarising data by group Last time, we saw how to calculate the mean of a variable. Heres the mean of life expectancy, across all countries and years: mean(gap$lifeExp) ## [1] 59.47444 I dont know what to make of that! Typically we want to calculate means by group rather than for a whole variable. This is known as aggregating or summarising by group. For instance, looking at the plots above it seems that there will be a mean difference in life expectancy between continents, and it would be interesting to see that. For this, we will use dplyr (pronounced DEE-ply-er). Its part of tidyverse so already included, but its useful to know the name of this specific part for when you are searching for help. Im going to work through an example in excruciating detail, but it will be worth it I promise. The punchline is that to calculate mean life expectancy by year and continent, you do this: mean_life_exp_gap &lt;- gap %&gt;% group_by(year, continent) %&gt;% summarise(mean_life_exp = mean(lifeExp)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) (Have a look and see.) Heres a longer worked example. Step 1. Use group_by to tell R what variables you want to group the data by. The first parameter of group_by is the dataset you want to group. The remaining parameters are the variables in that dataset to group by: grouped_gap &lt;- group_by(gap, year, continent) So this says, group the gap data frame by year and continent. This new variable, grouped_gap is a grouped data frame. It has all the same information as before, plus a little note (semi-hidden) to say that analyses on this should be grouped. Heres how to peek at this note: group_vars(grouped_gap) ## [1] &quot;year&quot; &quot;continent&quot; Step 2. Use summarise on this grouped data frame to calculate what you want. The first argument of summarise is the data frame (grouped or otherwise) followed by new variable names and what you want them to contain. summarised_grouped_gap &lt;- summarise(grouped_gap, mean_life_exp = mean(lifeExp)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) Lets have a look at the top 10 rows: head(summarised_grouped_gap, 10) ## # A tibble: 10 x 3 ## # Groups: year [2] ## year continent mean_life_exp ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1952 Africa 39.1 ## 2 1952 Americas 53.3 ## 3 1952 Asia 46.3 ## 4 1952 Europe 64.4 ## 5 1952 Oceania 69.3 ## 6 1957 Africa 41.3 ## 7 1957 Americas 56.0 ## 8 1957 Asia 49.3 ## 9 1957 Europe 66.7 ## 10 1957 Oceania 70.3 It worked! We could now use this in ggplot (and shall do so below). 3.6.1 Activity Do the same again but this time calculate means only by year, averaging across continents. 3.6.2 Answer grouped_gap_year &lt;- group_by(gap, year) summarised_grouped_year &lt;- summarise(grouped_gap_year, mean_life_exp = mean(lifeExp)) ## `summarise()` ungrouping output (override with `.groups` argument) summarised_grouped_year ## # A tibble: 12 x 2 ## year mean_life_exp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 49.1 ## 2 1957 51.5 ## 3 1962 53.6 ## 4 1967 55.7 ## 5 1972 57.6 ## 6 1977 59.6 ## 7 1982 61.5 ## 8 1987 63.2 ## 9 1992 64.2 ## 10 1997 65.0 ## 11 2002 65.7 ## 12 2007 67.0 3.7 Pipes R analyses often feel like making information flow along a pipe, transforming it in various ways as it goes. Maybe reshaping it, selecting some variables, filtering, grouping, calculating. Finally, out flows an answer. This leads to another member of the tidyverse family, magrittr, named after René Magritte because of his 1929 painting showing a pipe and a caption Ceci nest pas une pipe (This is not a pipe). You may have noticed that both group_by and summarise had a data frame as their first argument. They also both outputted a data frame. The forward pipe operator, %&gt;%, allows you to pass the data frame along your information flow, without having to save results in interim variables. You start with the name of the input data frame and then pipe it into the first function. For example, here is how to group the data: gap %&gt;% group_by(year, continent) As before you can then save the result: grouped &lt;- gap %&gt;% group_by(year, continent) To flow this onto summarise, just add another pipe like so: grouped &lt;- gap %&gt;% group_by(year, continent) %&gt;% summarise(mean_life_exp = mean(lifeExp)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) The %&gt;% is purely designed to make the flow of information easier to see and hopefully also easier to design. 3.8 Plot the mean life expectancy by continent By here you hopefully get the gist of how to use pipes to group data frames and summarise them. There will be further opportunities to practice this skill. Heres an aggregated data frame with mean life expectancy by year and continent: mean_life_exp_gap &lt;- gap %&gt;% group_by(year, continent) %&gt;% summarise(mean_life_exp = mean(lifeExp)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) You can view this to check the information is as you expect: View(mean_life_exp_gap) Here are the variable names, for ease of reference. names(mean_life_exp_gap) ## [1] &quot;year&quot; &quot;continent&quot; &quot;mean_life_exp&quot; 3.8.1 Actvity Now your challenge is to plot the mean life expectancy by year, with colour showing the continent. You could try adapting an example from above to help you. 3.8.2 Answer ggplot(mean_life_exp_gap, aes(x = year, y = mean_life_exp, colour = continent)) + geom_point() 3.9 Yet another geom: line Instead of plotting points for each year, you may wish to join the data with lines. Heres how  just use geom_line: ggplot(mean_life_exp_gap, aes(x = year, y = mean_life_exp, colour = continent)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth&quot;, colour = &quot;Continent&quot;) 3.9.1 Activity How could you add points back to the lines? 3.9.2 Answer Simply use + again: ggplot(mean_life_exp_gap, aes(x = year, y = mean_life_exp, colour = continent)) + geom_point() + geom_line() Ive been a bit lazy here and havent bothered changing the axis labels and legend title. That is fine when playing around with different visualisations and learning. Just remember to tidy it all up before adding to a written report! 3.10 Filtering data along the pipeline Analysing by continent clearly doesnt do the data justice: in the jittered points we saw there was loads of variation within continent. The mean plots highlighted that improvement in life expectancy in Africa stalled around 1990. I wonder if this was the same for all countries therein? The next tidyverse function we will explore to help us is called filter. (See the help for lots of examples using a Star Wars dataset.) Here is how to filter the data so we only have rows for Africa: gap %&gt;% filter(continent == &quot;Africa&quot;) %&gt;% head(10) ## country continent year lifeExp pop gdpPercap ## 1 Algeria Africa 1952 43.077 9279525 2449.008 ## 2 Algeria Africa 1957 45.685 10270856 3013.976 ## 3 Algeria Africa 1962 48.303 11000948 2550.817 ## 4 Algeria Africa 1967 51.407 12760499 3246.992 ## 5 Algeria Africa 1972 54.518 14760787 4182.664 ## 6 Algeria Africa 1977 58.014 17152804 4910.417 ## 7 Algeria Africa 1982 61.368 20033753 5745.160 ## 8 Algeria Africa 1987 65.799 23254956 5681.359 ## 9 Algeria Africa 1992 67.744 26298373 5023.217 ## 10 Algeria Africa 1997 69.152 29072015 4797.295 Note the double equals, ==, not to be confused with = which is used to set inputs (also known as arguments). To see how == works, compare: 11 + 3 == 14 ## [1] TRUE And: 11 + 3 == 2 ## [1] FALSE Now Im going to try piping this filtered data frame directly into ggplot, without saving it. This should work because ggplots first argument is the data frame. gap %&gt;% filter(continent == &quot;Africa&quot;) %&gt;% ggplot(aes(x = year, y = lifeExp, colour = country)) + geom_point() + geom_line() Well it did but the plot is very busy and Im not sure I could distinguish between all those colours! Lets try again without the legend to see whats going on. At this point you may wonder, How on earth will I be able to remember all these commands? I will share a trick. Attempt 2: gap %&gt;% filter(continent == &quot;Africa&quot;) %&gt;% ggplot(aes(x = year, y = lifeExp, colour = country)) + geom_point() + geom_line() + theme(legend.position = &quot;none&quot;) 3.10.1 Activity One of the countries life expectancies dropped below 25. Can you work out which one it was by using filter? Tip: == was equals. You can use &lt; for less than. 2 &lt; 3 ## [1] TRUE 3.10.2 Answer gap %&gt;% filter(lifeExp &lt; 25) ## country continent year lifeExp pop gdpPercap ## 1 Rwanda Africa 1992 23.599 7290203 737.0686 So the answer is Rwanda. 3.11 Other handy tools: select, slice, bind, and arrange Often you will have datasets with a huge number of variables and will want to select a few of those to make the tables easier to read. The command for that is select; give it the names of the variables you want. Another useful function is arrange which sorts a data frames by the variable(s) you provide. Here is an example illustrating both. I have also added the operator &amp; for and. gap %&gt;% filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% arrange(lifeExp) %&gt;% select(country, lifeExp) ## country lifeExp ## 1 Swaziland 39.613 ## 2 Mozambique 42.082 ## 3 Zambia 42.384 ## 4 Sierra Leone 42.568 ## 5 Lesotho 42.592 ## 6 Angola 42.731 ## 7 Zimbabwe 43.487 ## 8 Central African Republic 44.741 ## 9 Liberia 45.678 ## 10 Rwanda 46.242 ## 11 Guinea-Bissau 46.388 ## 12 Congo, Dem. Rep. 46.462 ## 13 Nigeria 46.859 ## 14 Somalia 48.159 ## 15 Malawi 48.303 ## 16 Cote d&#39;Ivoire 48.328 ## 17 South Africa 49.339 ## 18 Burundi 49.580 ## 19 Cameroon 50.430 ## 20 Chad 50.651 ## 21 Botswana 50.728 ## 22 Uganda 51.542 ## 23 Equatorial Guinea 51.579 ## 24 Burkina Faso 52.295 ## 25 Tanzania 52.517 ## 26 Namibia 52.906 ## 27 Ethiopia 52.947 ## 28 Kenya 54.110 ## 29 Mali 54.467 ## 30 Djibouti 54.791 ## 31 Congo, Rep. 55.322 ## 32 Guinea 56.007 ## 33 Benin 56.728 ## 34 Gabon 56.735 ## 35 Niger 56.867 ## 36 Eritrea 58.040 ## 37 Togo 58.420 ## 38 Sudan 58.556 ## 39 Madagascar 59.443 ## 40 Gambia 59.448 ## 41 Ghana 60.022 ## 42 Senegal 63.062 ## 43 Mauritania 64.164 ## 44 Comoros 65.152 ## 45 Sao Tome and Principe 65.528 ## 46 Morocco 71.164 ## 47 Egypt 71.338 ## 48 Algeria 72.301 ## 49 Mauritius 72.801 ## 50 Tunisia 73.923 ## 51 Libya 73.952 ## 52 Reunion 76.442 This filters gap to data from 2007 and Africa, sorts it by life expectancy, and then selects the country and life expectancy variables. The slice family of functions can be used to zoom into the top or bottom slices of rows, particular rows, or a random sample. Heres an example. First save the previous chunk results above in africa2007: africa2007 &lt;- gap %&gt;% filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% arrange(lifeExp) %&gt;% select(country, lifeExp) The following R code saves the head of the dataset, which has the lowest life expectancies. The n is 3, so three rows are returned. Note the single = here: its an parameter setting n to 3 rather than an equality == checking whether n is 3. africa2007min &lt;- africa2007 %&gt;% slice_head(n = 3) africa2007min ## country lifeExp ## 1 Swaziland 39.613 ## 2 Mozambique 42.082 ## 3 Zambia 42.384 Do this again for the tail, i.e., the bottom of the dataset, which has the highest values for life expectancy. africa2007max &lt;- africa2007 %&gt;% slice_tail(n = 3) africa2007max ## country lifeExp ## 1 Tunisia 73.923 ## 2 Libya 73.952 ## 3 Reunion 76.442 We can bind the two data frames together again using bind_rows: top_and_bottom &lt;- bind_rows(africa2007min, africa2007max) top_and_bottom ## country lifeExp ## 1 Swaziland 39.613 ## 2 Mozambique 42.082 ## 3 Zambia 42.384 ## 4 Tunisia 73.923 ## 5 Libya 73.952 ## 6 Reunion 76.442 3.12 Filtering for members of a vector The top_and_bottom data frame has the names of countries with the top and bottom three life expectancies. top_and_bottom$country ## [1] &quot;Swaziland&quot; &quot;Mozambique&quot; &quot;Zambia&quot; &quot;Tunisia&quot; &quot;Libya&quot; ## [6] &quot;Reunion&quot; Next we are going to filter the data set to only these countries, using the %in% operator which returns TRUE if a value is in the vector you provide and FALSE otherwise. Here are two examples: &quot;Libya&quot; %in% top_and_bottom$country ## [1] TRUE &quot;Uganda&quot; %in% top_and_bottom$country ## [1] FALSE gap %&gt;% filter(country %in% top_and_bottom$country) %&gt;% ggplot(aes(x = year, y = lifeExp, colour = country)) + geom_line() We can add Rwanda back in by using the c operator (c for combine). Heres an example to show how c works: some_numbers &lt;- c(1,2,3) c(some_numbers,4) ## [1] 1 2 3 4 Back to the graph. Below I have also enlarged the size of the lines to make the colours easier to distinguish. gap %&gt;% filter(country %in% c(top_and_bottom$country, &quot;Rwanda&quot;)) %&gt;% ggplot(aes(x = year, y = lifeExp, colour = country)) + geom_line(size = 1) + labs(x = &quot;Year&quot;, y = &quot;Mean life expectancy (years)&quot;, colour = &quot;Country&quot;) You might now consider a qualitative analysis of these countries (or lookup Wikipedia, for the purposes of a weekly R exercise) to conjecture why there are these differences. 3.13 Final challenge 3.13.1 Activity Plot life expectancy against GDP per capita for all countries in the dataset at the most recent time point. Colour the points by continent. 3.13.2 Answer Heres how I did it. First, check the variable names: names(gap) ## [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; So we want lifeExp and gdpPercap. The most recent year is: max(gap$year) ## [1] 2007 (You could also find that by looking at the data frame using View.) Now make filter and make the graph in one go: gap %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, colour = continent)) + geom_point() + labs(y = &quot;Mean life expectancy (years)&quot;, x = &quot;GDP per capita (US$, inflation-adjusted)&quot;, colour = &quot;Continent&quot;, title = &quot;Life expectancy and GDP per capita in 2007&quot;) 3.14 More ideas for visualisations Check out these references, all available for free online: Chang, W. (2020). R Graphics Cookbook (2nd ed.)  huge selection of examples to adapt and use. Healy, K. (2019). Data Visualization: A Practical Introduction  more explanations of creating effect visualisation, using social science examples (including Gapminder). Wickham, H., &amp; Grolemund, G. (2017). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data  a systematic introduction to the tidyverse. "],["p-values-and-confidence-intervals.html", "Chapter 4 P-values and confidence intervals 4.1 Correlation recap 4.2 Testing null-hypotheses 4.3 Confidence intervals 4.4 Optional extra: s-values 4.5 Further reading", " Chapter 4 P-values and confidence intervals P-values are notoriously difficult to comprehend and controversial. Occasionally journals even try to ban them. But they are pervasive in the literature and statistical software often presents them so it is important to understand what they mean. Confidence intervals, slightly less controversial but similarly often misunderstood, are important too. By the end of this chapter you will: Have an intuitive grasp of how sample data can look, given particular assumptions about true population values. Understand p-values in relation to this intuitive grasp. Have an intuitive grasp of how confidence intervals look across a range of studies. Understand confidence intervals in relation to this intuitive grasp. Know where to look for formal definitions. 4.1 Correlation recap We will focus on correlations in this chapter, so here is a refresher of what they are. Pearsons correlation coefficient, \\(r\\), is an indicator of how linearly correlated two variables are. It is bounded between \\(-1\\) and \\(1\\), where \\(-1\\) denotes a perfect negative correlation (one variable goes up, the other down), \\(1\\) denotes a perfect positive correlation (one variable goes up, so does the other), and \\(0\\) denotes a perfect absence of correlation. The picture below shows some examples. The quantitative interpretation of a correlation, \\(r\\), is as follows: if one variable is increased by 1 standard deviation (SD), then the other changes by a mean of \\(r\\) SDs. So, for instance, a correlation of \\(-0.42\\) indicates if one variable goes up by 1 SD, then the other decreases by a mean of \\(0.42\\) SDs. Correlations are a special case of linear regression (see next chapter) but you can also estimate them directly using the R command cor.test or cor. 4.2 Testing null-hypotheses The basic idea is simple, though feels back-to-front. Its popular because it is easier to work with than alternatives. Heres the gist for testing a correlation. Build a model of what the world could look like if there were no true correlation between your variables of interest. Calculate the actual correlation in the sample you have. Work out how probable the correlation you got, or greater, in either direction would be if there were no true correlation. The argument is similar to the structure of modus tollens from classical propositional logic. If A, then B. Not-B. Therefore, not-A. For example: If Rex were a duck, then Rex would quack. Rex doesnt quack. Therefore, Rex isnt a duck. Or: If there were no true correlation, then the sample correlation would probably be close to zero. The correlation isnt close to zero. Therefore, there probably is a correlation. Except its not quite this, because I have slipped the word probably in a couple of times and the two probabilities arent the same. Also I havent defined close to zero. Onwards, to define close to zero. 4.2.1 What can samples look like when the true correlation is 0? We can explore this by simulation. Here is an animation of 100 simulated studies, each with 20 simulated participants. By design, there is no true correlation between the variables. But lets see what happens As you will have seen, sometimes the correlation is positive, other times, negative, and occasionally it is quite big. We can run this simulation 10,000 times and draw histograms of the correlation coefficient (r) and the p-value (as calculated by cor.test) for each simulated study. Here is how they look: 10,000 studies (true r = 0, each sample size 20) Some observations: Most values of r are close to zero but some are quite far on either side of it. The p-values are uniformly spread from 0 to 1. Lets try again, this time with each study sample size set to 200. 10,000 studies (true r = 0, each sample size 200) Most values of r are close to zero and now the spread of values is much smaller. The p-values are still uniformly spread from 0 to 1. 4.2.2 Understanding actual data in relation to these simulations Above we explored what correlations can look like in a sample when the true correlation is 0. We can use this knowledge to understand the p-value of correlations for actual data. Here is actual data: the subjective prestige of white collar occupations is apparently positively correlated with the mean number of years education of people with that occupation. (We explore the dataset further in the next chapter.) There are 23 observations in (this subset of) the data and the correlation is 0.62. To work out the p-value for this correlation, Im first going to simulate 100,000 studies in which there is no correlation between the two variables and where each simulated study has 23 observations. If you look very closely at the histogram, you will see I have shaded the edges (also known as the tails) in red. Thats where the simulated \\(r\\) is above \\(0.62\\) or below \\(-0.62\\), i.e., at least as big in magnitude as the sample correlation we got for the prestige data above. Of these simulations, 71 had a correlation greater than or equal to \\(0.62\\) and 74 were less than or equal to \\(-0.62\\). As a proportion of the total number of simulations, that is 0.00145. The p-value worked out by cor.test command without simulation is 0.0014513. 4.2.3 So, what is a p-value? The p-value is the probability of getting a statistic in the sample at least as big as the one you got, if the null hypothesis and assumptions about how the data were created were true. In the example above, we are testing for the absolute value of the correlation (i.e., ignoring its sign) under the null hypothesis that the true correlation is zero and making assumptions such as the data being a simple random sample. Note how p-value is not probability of the null hypothesis. We have assumed, for the sake of argument, that the null hypothesis is true. Under this assumption, the probability of the null hypothesis is 1. And herein lies everyones beef with p-values  they tell you the probability of your data given the null but not the probability of the null given the data. 4.3 Confidence intervals We can also calculate confidence intervals for correlations and a range of other statistics we will encounter. 4.3.1 Simulating confidence Suppose the true correlation between two variables were 0.5 and you ran 100 studies, each with 20 participants, sampling randomly from the population. Lets do it by simulation Below shows the results for each study. The true value is shown as the vertical dashed line. Each horizontal line gives the confidence interval for a particular study and the blob gives the correlation coefficient. The colour depends on whether the interval includes the true value. If you count them, you will see that 95 included the true value, i.e., 95% of the studies had an interval including the true value. I should add that it doesnt always work out as exactly 95% for a given collection of studies, simulated or real, but it will approach this percentage if the model is correct as the number of studies increases. 4.3.2 What is a confidence interval, then? If you run loads of studies with random samples from the same population, then the \\(c\\%\\) confidence interval will include the true value for the statistic in around \\(c\\%\\) of studies, assuming that you are using the correct model. This is not the same as saying that, for a particular study, there is a 95% probability that it includes the true value. 4.4 Optional extra: s-values S-values are a neat idea for helping to think about  maybe even feel  the meaning of p-values. Theyre described by Rafi and Greenland (2020). Let me try to illustrate the gist, starting with flips of a coin. Suppose you flip an apparently fair coin and the outcome is heads. How surprised would you feel? Now flip it a couple of times: both outcomes are heads. How surprised would you feel now? Flip the coin 40 times. All of the outcomes are heads. How surprised now? I suspect your level of surprise has gone from something like meh through to WOW WHAT?! S-values provide a way to think about p-values in terms of how likely it would be to get a sequence of all heads from a number of flips of a fair coin. That number of flips is the s-value and is calculated from the p-value. In other words, s-values and p-values are linked by this statement: the probability of getting all heads by flipping a fair coin \\(s\\) times is \\(p\\). Here is an example of a coin flipped three times. There are \\(2^3 = 8\\) possibilities, enumerated in the table below: First coin flip Second coin flip Third coin flip H H H H H T H T H H T T T H H T H T T T H T T T If the coin is fair, then the probability of each outcome is \\(\\frac{1}{8}\\). In particular, the probability of all heads is also \\(\\frac{1}{8}\\), or 0.125. More generally, the probability of getting all heads from \\(n\\) fair coin flips is \\(\\frac{1}{2^n}\\). Here is a table showing some examples: Flips Probability all heads 1 0.50000 2 0.25000 3 0.12500 4 0.06250 5 0.03125 6 0.01562 7 0.00781 8 0.00391 9 0.00195 10 0.00098 Now here is the connection with p-values. Suppose you run a statistical test and get \\(p = 0.03125\\); thats the same probability as that of obtaining five heads in a row from five coin tosses. The s-value is 5. Or suppose you merely got \\(p = 0.5\\). Thats the probability of obtaining heads after one flip of the coin. The s-value is 1. The larger the s-value, the more surprised you would be if the coin were fair. To convert p-values to s-values, we want to find an \\(n\\), such that \\[ \\frac{1}{2^n} = p \\] The log function (which we encounter again later, repeatedly) does this for us: \\[ s = -\\log_2(p) \\] In R, here is the s-value for \\(p = 0.5\\). -log(0.5, 2) ## [1] 1 And \\(p = 0.03125\\) -log(0.03125, 2) ## [1] 5 What about the traditional (or notorious?) 0.05 level? -log(0.05, 2) ## [1] 4.321928 So thats the same as getting all heads when you flip a coin 4.32 times  which isnt entirely intuitive when expressed as coin flips. But you could think of it being at least as surprising as getting four heads in a row if you flipped a coin four times. Here are examples for common thresholds for \\(p\\): p-value s-value 1.00000 0.00 0.50000 1.00 0.05000 4.32 0.01000 6.64 0.00100 9.97 0.00010 13.29 0.00001 16.61 4.5 Further reading Try Greenland et al. (2016) who have a long list of misconceptions to try to avoid. I also liked Colquhouns (2014) explanation of p-values by analogy with the properties of diagnostic tests. "],["linear-regression.html", "Chapter 5 Linear regression 5.1 Before we begin 5.2 The dataset 5.3 Interlude on methodology 5.4 Descriptives 5.5 Prep to understand the simplest regression model 5.6 The simplest regression model: intercept-only model 5.7 Adding a slope to the regression model 5.8 Residuals 5.9 Comparing models 5.10 Regression with two or more predictors 5.11 Interpreting regression models with two or more predictors 5.12 Optional: that pesky negative intercept 5.13 Finally: confidence intervals 5.14 Very optional extras", " Chapter 5 Linear regression By the end of this chapter you will: Know how fit and compare regression models using R (three commands: lm, summary, anova). Understand how to interpret the output from these commands. Have explored some more examples of tidyverse analyses. Next chapter, we will explore how to check model assumptions, which may or may not hold for the models we fit here. 5.1 Before we begin Download the prestige dataset, create an R Markdown file, and ensure they are both saved in a folder you will find again. I recommend writing down any questions you have as you go, perhaps in your Markdown file, so that you can discuss them with whoever else you have roped into learning R. 5.2 The dataset We will use a dataset on occupational prestige from Canada, 1971, which is used by Fox and Weisberg (2019). Each row in the dataset describes an occupation, and aggregated data about that occupation. Variable name Description occ Occupation education Average years of education for people in job income Average income in dollars women Percentage of women in occupation prestige A Pineo-Porter prestige score for the occupation with a possible range of 0-100 (higher = more prestigious) type bc = blue collar wc = white collar prof = professional, managerial, or technical Im going to read it in as dat but feel free to choose any name you desire. dat &lt;- read.csv(&quot;prestige.csv&quot;) Here are the top 5 rows: head(dat, 5) ## occ education income women prestige type ## 1 gov administrators 13.11 12351 11.16 68.8 prof ## 2 general managers 12.26 25879 4.02 69.1 prof ## 3 accountants 12.77 9271 15.70 63.4 prof ## 4 purchasing officers 11.42 8865 9.11 56.8 prof ## 5 chemists 14.62 8403 11.68 73.5 prof Spend some time checking that you understand each row. Our challenge for this tutorial is to work out what predicts the prestige of occupations. 5.3 Interlude on methodology The objective of this tutorial is simply to understand linear regression; however, I think its worth commenting briefly on methodology. It is important to develop research questions and (if possible, directional) statistical hypotheses before obtaining data, based on prior evidence and theory. These days, analysis plans are often published before any data are obtained. This is to avoid data dredging (also known as p-hacking) which makes it likely to find results by capitalising on chance that have no hope of replicating or generalising beyond the sample. Looking at graphs of data counts as data dredging; you cant create hypotheses from hindsight. See this page from the Center for Open Science for more info. 5.4 Descriptives Ill include tidyverse for ggplot, pipes, etc., as they will come in handy later: library(tidyverse) The base R summary command is a quick way to obtain a summary of all variables; give it the data frame you wish to summarise as follows: summary(dat) ## occ education income women ## Length:102 Min. : 6.380 Min. : 611 Min. : 0.000 ## Class :character 1st Qu.: 8.445 1st Qu.: 4106 1st Qu.: 3.592 ## Mode :character Median :10.540 Median : 5930 Median :13.600 ## Mean :10.738 Mean : 6798 Mean :28.979 ## 3rd Qu.:12.648 3rd Qu.: 8187 3rd Qu.:52.203 ## Max. :15.970 Max. :25879 Max. :97.510 ## prestige type ## Min. :14.80 Length:102 ## 1st Qu.:35.23 Class :character ## Median :43.60 Mode :character ## Mean :46.83 ## 3rd Qu.:59.27 ## Max. :87.20 There are a variety of ways to create what is often known as Table 1, so called because it is usually the first table in quantitative journal articles. Here is one, in the tableone package: library(tableone) dat %&gt;% select(-occ) %&gt;% CreateTableOne(data = .) ## ## Overall ## n 102 ## education (mean (SD)) 10.74 (2.73) ## income (mean (SD)) 6797.90 (4245.92) ## women (mean (SD)) 28.98 (31.72) ## prestige (mean (SD)) 46.83 (17.20) ## type (%) ## bc 44 (44.9) ## prof 31 (31.6) ## wc 23 (23.5) The select line says to remove the occ (occupation) variable (can you see why I did that?). The data = . option is there because CreateTableOne doesnt understand %&gt;% plumbing. The . represents the output from the previous line so connects the information flow correctly. You will also usually want to create scatterplots of relationships between continuous variables, similar to what we did with the Gapminder dataset. Which leads onto the following activity. 5.4.1 Activity Create the following scatterplots: prestige against education prestige against income Describe the relationship you see. 5.4.2 Answer a. prestige against education dat %&gt;% ggplot(aes(x = education, y = prestige)) + geom_point() + labs(x = &quot;Average years of education&quot;, y = &quot;Pineo-Porter prestige&quot;) There appears to be a linear association between education and prestige; more education is associated with higher prestige. b. prestige against income dat %&gt;% ggplot(aes(x = income, y = prestige)) + geom_point() + labs(x = &quot;Average income (dollars)&quot;, y = &quot;Pineo-Porter prestige&quot;) Up to around $10000, there seems to be a linear correlation between income and prestige, after which the relationship flattens out, i.e., more income does not lead to higher prestige. (At least, thats what I see!) 5.5 Prep to understand the simplest regression model 5.5.1 Activity The simplest regression model we will explore in a moment just models the mean of the outcome variable. So that we can see how it works, first calculate the mean and SD of prestige. 5.5.2 Answer mean(dat$prestige) ## [1] 46.83333 sd(dat$prestige) ## [1] 17.20449 5.6 The simplest regression model: intercept-only model The command for fitting a regression model is called lm, which is short for linear model. It wants a formula, describing the model to be fitted, and the name of a data frame containing your data. Here is how to fit the intercept-only model: mod0 &lt;- lm(formula = prestige ~ 1, data = dat) The left-hand side of ~ (tilde) is the outcome or response variable we want to explain/predict. The right-hand side lists predictors. Here 1 denotes the intercept. Since the first two parameters of lm are formula and data (check ?lm), this can be abbreviated to: mod0 &lt;- lm(prestige ~ 1, dat) As ever, since we have saved the result in a variable, nothing has visibly happened. You could have a peek at the result with: mod0 ## ## Call: ## lm(formula = prestige ~ 1, data = dat) ## ## Coefficients: ## (Intercept) ## 46.83 There is a LOT more info in the object which you can see with this structure command, str: str(mod0) If you run this you will see why we usually prefer to use summary on the output; this pulls out useful info and presents it in a straightforward way: summary(mod0) ## ## Call: ## lm(formula = prestige ~ 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.033 -11.608 -3.233 12.442 40.367 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.833 1.703 27.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.2 on 101 degrees of freedom The estimate for the intercept is 46.833, the same as the mean of prestige we calculated above. There is other information: The standard error (SE), which provides information on the precision of the estimate: the smaller the SE, the more precise the estimate of the population mean. The t-value, which is equal to the estimate divided by the SE. This is used to test whether the estimate is different to zero, which isnt particularly informative for the intercept; its more so when we get to slopes in a moment. The p-value is calculated from the distribution of t under the assumption that the population estimate is 0 and helps us interpret the t. R presents it here as &lt;2e-16. This says that p is less than 0.0000000000000002, or \\(2 \\times 10^{-16}\\). So, for what its worth, and using the usual not-entirely-correct applied stats vernacular: the intercept, here mean prestige, is statistically significantly greater than zero. We will get to the residual standard error later, but for now observe that it is the same as the SD of prestige. sd(dat$prestige) ## [1] 17.20449 Okay, so you have calculated the mean and SD of a variable in an absurdly convolved way, alongside a statistical test that probably isnt any use. Why have I done this to you? Two reasons: It is useful to see what models do for the simplest possible specification  increasingly useful as the models become more complicated. The intercept-only model can be compared with more complex models, i.e., models with more predictors, to see if adding predictors actually explains more variance. 5.6.1 Activity Try fitting the intercept-only model again for the education variable. Compare the result with its mean and SD. 5.6.2 Answer The mean is: mean(dat$education) ## [1] 10.73804 The SD: sd(dat$education) ## [1] 2.728444 Fit the model: lm(education ~ 1, data = dat) %&gt;% summary() ## ## Call: ## lm(formula = education ~ 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.358 -2.293 -0.198 1.909 5.232 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.7380 0.2702 39.75 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.728 on 101 degrees of freedom (Here I have used a pipe  again do whatever makes most sense!) 5.7 Adding a slope to the regression model So that was a silly model. Next we are going to add years of education as a predictor to see if it explains any of the variation in prestige. Before doing so, here is a picture of the relationship from earlier. plot_ed_prestige &lt;- ggplot(dat, aes(x = education, y = prestige)) + geom_point() plot_ed_prestige I am confident that there is a positive relationship between education and prestige, using the interocular trauma test, but it is instructive to see the intercept-only model prediction overlaid on this graph. To extract the coefficients from a model, we use coef: coef(mod0) ## (Intercept) ## 46.83333 In this case, there only is one estimate, so this is equivalent to: coef(mod0)[1] ## (Intercept) ## 46.83333 But usually there will be more than one coefficient in a model. Since we saved the earlier plot, it is easy to add on a horizontal line for the intercept-only model: plot_ed_prestige + geom_hline(yintercept = coef(mod0)[1]) Now lets fit the surely better model: mod1 &lt;- lm(prestige ~ 1 + education, data = dat) summary(mod1) ## ## Call: ## lm(formula = prestige ~ 1 + education, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.0397 -6.5228 0.6611 6.7430 18.1636 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -10.732 3.677 -2.919 0.00434 ** ## education 5.361 0.332 16.148 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.103 on 100 degrees of freedom ## Multiple R-squared: 0.7228, Adjusted R-squared: 0.72 ## F-statistic: 260.8 on 1 and 100 DF, p-value: &lt; 2.2e-16 (Note: prestige ~ 1 + education is equivalent to prestige ~ education; R puts the intercept in by default. Try both to see!) We can plot the the predicted mean prestige for each year of education using the models coefficients coef(mod1) ## (Intercept) education ## -10.731982 5.360878  with geom_abline (pronounced A B line, named after a base R function abline where a is the intercept and b is the slope) like so: plot_ed_prestige + geom_abline(intercept = coef(mod1)[1], slope = coef(mod1)[2]) The intercept is -10.732. This is clearly no longer the mean of prestige (scroll up and you will see that the measure is defined as a number between 0-100); rather, the intercept is the predicted mean of the outcome variable where other predictors are zero so it depends on what other predictors are present in the model. Heres another picture, stretched to show where the intercept is: It is not uncommon for the intercept to be substantively nonsense, but statistically necessary to ensure accurate slopes. We will see later how to make the intercept more interpretable, without changing the slope estimates. The slope for education is 5.361. This means that for every unit increase in education, the prestige score increases by 5.361. The units for education are years, so we should say something like: every year increase in education is associated with 5.4 more prestige points. Is this a big effect do you reckon? As before, the ts and ps are present. Now the p-value for education is actually useful. The value is &lt; 2e-16, i.e., less than \\(2 \\times 10^{-16}\\). This means the sample-estimate of the slope we got would be very unlikely if the population slope were zero. Or, in the usual applied stats vernacular: there is a statistically significant relationship between education and prestige. (Keep a look out for how results are summarised in the literature you are exploring  again I urge you to look at journal articles in your field to get ideas for write-up aesthetics.) The \\(R^2\\) is also useful. The summary command presents \\(R^2\\) as a proportion; it represents the proportion of the variance in the outcome variable explained by the predictors. Generally we use the adjusted \\(R^2\\) because this adjusts for the number of predictors in the model and reduces bias in the estimate. If \\(R^2\\) were 0 then that would mean that the predictors dont explain any variance in the outcome. If it were 1, that would mean that the predictors explain all the variance  statistically, knowing the outcome variable does not add any further information. 5.7.1 Activity Use a regression model with prestige as the outcome variable and average income as a predictor. Describe the relationship according to the model and whether it is statistically significant. It will help to check above to see what units the income is measured in. 5.7.2 Answer mod_income &lt;- lm(prestige ~ income, data = dat) summary(mod_income) ## ## Call: ## lm(formula = prestige ~ income, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -33.007 -8.378 -2.378 8.432 32.084 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.714e+01 2.268e+00 11.97 &lt;2e-16 *** ## income 2.897e-03 2.833e-04 10.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.09 on 100 degrees of freedom ## Multiple R-squared: 0.5111, Adjusted R-squared: 0.5062 ## F-statistic: 104.5 on 1 and 100 DF, p-value: &lt; 2.2e-16 The slope for income is statistically significant at the traditional 5% level, with a very small \\(p\\). But what did you make of that slope estimate, 2.897e-03? This is equivalent to \\(2.897 \\times 10^{-3}\\) or about 0.0029. So for every unit increase in income, the prestige score goes up by 0.0029 on a scale from 0-100. The income units here are the Canadian dollar. We probably dont expect much of an increase in prestige for one extra dollar income! Heres the coefficient: coef(mod_income)[2] ## income ## 0.002896799 We might expect 1000 dollars more salary to have more of an impact: coef(mod_income)[2] * 1000 ## income ## 2.896799 So $1000 more salary is associated with about 2.9 more prestige points. Another way to do this is as follows. First transform the salary so it is in thousands of dollars rather than single dollars. Ill mutate the data frame to add a new variable called income_1000s: dat &lt;- dat %&gt;% mutate(income_1000s = income/1000) Now fit the model again using this new variable as the predictor. mod_income1000s &lt;- lm(prestige ~ income_1000s, data = dat) summary(mod_income1000s) ## ## Call: ## lm(formula = prestige ~ income_1000s, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -33.007 -8.378 -2.378 8.432 32.084 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.1412 2.2677 11.97 &lt;2e-16 *** ## income_1000s 2.8968 0.2833 10.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.09 on 100 degrees of freedom ## Multiple R-squared: 0.5111, Adjusted R-squared: 0.5062 ## F-statistic: 104.5 on 1 and 100 DF, p-value: &lt; 2.2e-16 Note how the t and p havent changed; however, now the estimate for income is easier to interpret. 5.8 Residuals Residuals are important for understanding regression fits. They are calculated for each data point and a given model as the difference between the actual value of the outcome variable and the model prediction. There are pictures coming up which will illustrate. R gives us model predictions using the predict command and also automatically calculates residuals using resid. Lets first do it for the intercept-only model. The code below says: select the variables prestige and education (this is to keep things neat and tidy) add variables predict0 and resid0 which consist of the predictions and residuals, respectively, for mod0 save it all in dat_for_resids dat_for_resids &lt;- dat %&gt;% select(prestige, education) %&gt;% mutate(predict0 = predict(mod0), residual0 = resid(mod0)) head(dat_for_resids, 10) ## prestige education predict0 residual0 ## 1 68.8 13.11 46.83333 21.966667 ## 2 69.1 12.26 46.83333 22.266667 ## 3 63.4 12.77 46.83333 16.566667 ## 4 56.8 11.42 46.83333 9.966667 ## 5 73.5 14.62 46.83333 26.666667 ## 6 77.6 15.64 46.83333 30.766667 ## 7 72.6 15.09 46.83333 25.766667 ## 8 78.1 15.44 46.83333 31.266667 ## 9 73.1 14.52 46.83333 26.266667 ## 10 68.8 14.64 46.83333 21.966667 Since its the intercept-only model, the prediction is always the mean; thats why predict0 says 46.83333, 46.83333, 46.83333, 46.83333, 46.83333 Look at the first residual, 21.97. That is calculated as the actual value of prestige minus the model-predicted value: \\(68.8 - 46.83\\). Here is a picture showing the residuals; hopefully this highlights that they just measure how far each data point is from the model prediction: dat_for_resids %&gt;% ggplot(aes(x = education, y = prestige)) + geom_segment(aes(xend = education, yend = predict0)) + geom_point() + geom_hline(yintercept = coef(mod0)) Now lets calculate predictions and residuals again for the more sensible model, mod1. dat_for_resids &lt;- dat_for_resids %&gt;% mutate(predict1 = predict(mod1), residual1 = resid(mod1)) dat_for_resids %&gt;% select(prestige, predict1, residual1, residual0) %&gt;% head(10) ## prestige predict1 residual1 residual0 ## 1 68.8 59.54913 9.250875 21.966667 ## 2 69.1 54.99238 14.107621 22.266667 ## 3 63.4 57.72643 5.673573 16.566667 ## 4 56.8 50.48924 6.310758 9.966667 ## 5 73.5 67.64405 5.855950 26.666667 ## 6 77.6 73.11215 4.487854 30.766667 ## 7 72.6 70.16366 2.436337 25.766667 ## 8 78.1 72.03997 6.060030 31.266667 ## 9 73.1 67.10796 5.992037 26.266667 ## 10 68.8 67.75127 1.048732 21.966667 Here is the plot: dat_for_resids %&gt;% ggplot(aes(x = education, y = prestige)) + geom_segment(aes(xend = education, yend = predict1)) + geom_point() + geom_abline(intercept = coef(mod1)[1], slope = coef(mod1)[2]) 5.8.1 Activity The second model clearly describes the data better than the intercept-only model, but can you explain why, solely in terms of the residuals? 5.8.2 Answer The residuals seem smaller. Intuitively, roughly, thats what is going on. Are all the residuals smaller? We can work out the length of the residual (i.e., the distance between model prediction and actual data), ignoring the sign, by using absolute function, abs, and then take the difference: dat_for_resids &lt;- dat_for_resids %&gt;% mutate(resid_diff = abs(residual1) - abs(residual0), intercept_better = resid_diff &gt;= 0) I have also added a variable called intercept_better if the intercept-only model has a smaller residual for that observation than the more complex model. Have a look: View(dat_for_resids) The answer is no, not all residuals are smaller for the complex model. dat_for_resids %&gt;% group_by(intercept_better) %&gt;% count() ## # A tibble: 2 x 2 ## # Groups: intercept_better [2] ## intercept_better n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 73 ## 2 TRUE 29 For 29 observations, the intercept-only model has smaller residuals than the model with a slope for education. This makes sense if we look again at the graph for the intercept-only model, this time with the lines coloured by whether the intercept-only model is better. I have also added a dashed line for the model with a slope for education. dat_for_resids %&gt;% ggplot(aes(x = education, y = prestige, colour = intercept_better)) + geom_segment(aes(xend = education, yend = predict0)) + geom_point() + geom_hline(yintercept = coef(mod0)) + labs(colour = &quot;Intercept-only better&quot;) + geom_abline(intercept = coef(mod1)[1], slope = coef(mod1)[2], linetype = &quot;dashed&quot;) So its not quite true that all the residuals are smaller in the more complex model. Most are, though. One way to combine all the residuals is to square them first (to remove the sign, i.e., whether the residual is positive or negative), and sum them together. This is called the residual sum of squares and, it turns out, is what the regression model minimises for any particular dataset. Heres the arithmetic for the intercept-only model: sum(dat_for_resids$residual0^2) ## [1] 29895.43 And heres the arithmetic for the model with a predictor for education: sum(dat_for_resids$residual1^2) ## [1] 8286.99 So the residual sum of squares are smaller for the latter model. 5.9 Comparing models Here is a reminder of two of the models we fitted: mod0 &lt;- lm(prestige ~ 1, data = dat) mod1 &lt;- lm(prestige ~ 1 + education, data = dat) To compare these, use anova, with the least complex model first: anova(mod0, mod1) ## Analysis of Variance Table ## ## Model 1: prestige ~ 1 ## Model 2: prestige ~ 1 + education ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 101 29895 ## 2 100 8287 1 21608 260.75 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This only works for nested models, where the smaller models predictors are also included in the larger model. This is true here, since the larger model is the same as the intercept-only model with the addition of a slope for education. The anova command has calculated the residual sum of squares (RSS) for us. There is also a p-value; since it is far smaller than 0.05, we can conclude that the more complex model explains statistically significantly more variance. We would usually write this as (something like): the model with education as a predictor explains prestige better than the intercept-only model, \\(F(1,100) = 260.8\\), \\(p &lt; .001\\). (Generally when the p-value is very small, we use the less-than in this fashion.) 5.10 Regression with two or more predictors Linear regression models can include arbitrary numbers of predictors  in social science, often a dozen or more. They become increasingly challenging to visualise in higher dimensions, so lets start with two predictors, education and income. When you run the code below, a window will pop up with an interactive 3D plot. library(car) library(rgl) scatter3d(prestige ~ education + income, data = dat, surface = FALSE) You must enable Javascript to view this page properly. Now, lets fit a model to those data. 5.10.1 Activity We have seen how to fit an intercept-only model and to add a predictor to that. Can you work out how to model prestige as an outcome variable with predictors of education and income (using the version scaled to thousands)? Dont worry about interpreting the slopes just yet. Is this model a better fit than the intercept-only model? 5.10.2 Answer a. Can you work out how to model prestige as an outcome variable with predictors of education and income (using the version scaled to thousands)? Hopefully that was straightforward; just use + again: mod_both &lt;- lm(prestige ~ education + income_1000s, data = dat) summary(mod_both) ## ## Call: ## lm(formula = prestige ~ education + income_1000s, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4040 -5.3308 0.0154 4.9803 17.6889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.8478 3.2190 -2.127 0.0359 * ## education 4.1374 0.3489 11.858 &lt; 2e-16 *** ## income_1000s 1.3612 0.2242 6.071 2.36e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.81 on 99 degrees of freedom ## Multiple R-squared: 0.798, Adjusted R-squared: 0.7939 ## F-statistic: 195.6 on 2 and 99 DF, p-value: &lt; 2.2e-16 Both predictors were statistically significant. Interpretation coming below b. Is this model a better fit than the intercept-only model? We already have the intercept-only model as mod0 so comparing models is done by: anova(mod0, mod_both) ## Analysis of Variance Table ## ## Model 1: prestige ~ 1 ## Model 2: prestige ~ education + income_1000s ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 101 29895.4 ## 2 99 6038.9 2 23857 195.55 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model with two predictors explains more variation than the intercept-only model, \\(F(2,99) = 195.6, p &lt; .001\\) 5.11 Interpreting regression models with two or more predictors Okay, so we have fitted a model now; Ive called it mod_both. We can easily visualise this using: scatter3d(prestige ~ education + income, data = dat, surface = TRUE) You must enable Javascript to view this page properly. Again, try dragging the graph around. (By default scatter3d fits the same model we did.) The dataset was a cloud of data points and lm has fitted a plane to it, minimising the RSS. Here are the coefficients, rounded to one decimal place; how do we interpret them? coef(mod_both) %&gt;% round(1) ## (Intercept) education income_1000s ## -6.8 4.1 1.4 Firstly, the intercept is useless again as it is outside the range of valid values for prestige. It represents the level of prestige for occupations with an average zero years of education and zero income. We will deal with it later, but its not uncommon just to ignore intercepts when interpreting model fits. The coefficient for education is 4.1. The interpretation for this is: every year extra education is associated with 4.1 more prestige points, whilst holding income constant. Similarly for income: an increase in income by $1000 is associated with 1.4 more prestige points, whilst holding education constant. This demonstrates one of the advantages of a multiple regression model; the predictors now indicate the unique contribution of a predictor whilst holding the other predictors constant, assuming that the model is correct. We also now have a formula for predicting the mean level of prestige given education and income, which we could apply to other occupations not present in the data (from the same year and country) to see how well the model works. The formula is: \\[ \\mathtt{prestige} = -6.8 + 4.1 \\times \\mathtt{education} + 1.4 \\times \\mathtt{income\\_1000s} \\] 5.12 Optional: that pesky negative intercept Ive complained a couple of times about the intercept. If you really want it to take on a meaningful value, one easy solution which provides the same slope estimates, is simply to shift the predictor variables so that zero represents a value actually present in the data. We can do that by mean-centring the predictors, that is, shifting them so that zero is now the mean. Theres a command called scale that does this for us, but it is easy to do it by hand too. Simply subtract the mean from each value. Im going to add two variables, education_c and income_1000s_c, which represent mean centred education and income, and then save the result in a new data frame called dat_centred. dat_centred &lt;- dat %&gt;% mutate(education_c = education - mean(education), income_1000s_c = income_1000s - mean(income_1000s)) Lets compare the centred and uncentred variables: dat_centred %&gt;% select(education, education_c, income_1000s, income_1000s_c) %&gt;% CreateTableOne(data = .) ## ## Overall ## n 102 ## education (mean (SD)) 10.74 (2.73) ## education_c (mean (SD)) 0.00 (2.73) ## income_1000s (mean (SD)) 6.80 (4.25) ## income_1000s_c (mean (SD)) 0.00 (4.25) As you can see, they have the same SDs. The centred variables both have mean zero, as desired. Heres a picture of all pairwise scatterplots: library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 dat_centred %&gt;% select(prestige, education, education_c, income_1000s, income_1000s_c) %&gt;% ggpairs(upper = &quot;blank&quot;, progress = FALSE) Spend some time with a cup of tea or two to persuade yourself that centring hasnt affected relationships between key variables, it has just shifted the centred ones so they have a mean of zero. 5.12.1 Activity Now fit the model again, using prestige as the outcome variable before but now the centred versions of education and income. What does the intercept represent now? 5.12.2 Answer mod_centred &lt;- lm(prestige ~ education_c + income_1000s_c, data = dat_centred) summary(mod_centred) ## ## Call: ## lm(formula = prestige ~ education_c + income_1000s_c, data = dat_centred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4040 -5.3308 0.0154 4.9803 17.6889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.8333 0.7733 60.561 &lt; 2e-16 *** ## education_c 4.1374 0.3489 11.858 &lt; 2e-16 *** ## income_1000s_c 1.3612 0.2242 6.071 2.36e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.81 on 99 degrees of freedom ## Multiple R-squared: 0.798, Adjusted R-squared: 0.7939 ## F-statistic: 195.6 on 2 and 99 DF, p-value: &lt; 2.2e-16 As the table below shows, the slope estimates are identical for the two models. However, now the intercept is interpretable and gives the mean prestige at mean education and mean income. bind_cols(Variable = names(coef(mod_both)), `Original coefficients` = coef(mod_both), `Centred coefs` = coef(mod_centred)) ## # A tibble: 3 x 3 ## Variable `Original coefficients` `Centred coefs` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -6.85 46.8 ## 2 education 4.14 4.14 ## 3 income_1000s 1.36 1.36 5.13 Finally: confidence intervals To obtain confidence intervals, just use the confint command on the model: confint(mod_centred) ## 2.5 % 97.5 % ## (Intercept) 45.2988979 48.367769 ## education_c 3.4451272 4.829762 ## income_1000s_c 0.9162805 1.806051 There is a version in the car package (car is short for Companion to Applied Regression  the textbook) which also includes the coefficient. library(car) Confint(mod_centred) ## Estimate 2.5 % 97.5 % ## (Intercept) 46.833333 45.2988979 48.367769 ## education_c 4.137444 3.4451272 4.829762 ## income_1000s_c 1.361166 0.9162805 1.806051 Another way to get the same, in case handy for other scenarios where the car package function is not available: cbind(Estimate = coef(mod_centred), confint(mod_centred)) ## Estimate 2.5 % 97.5 % ## (Intercept) 46.833333 45.2988979 48.367769 ## education_c 4.137444 3.4451272 4.829762 ## income_1000s_c 1.361166 0.9162805 1.806051 5.14 Very optional extras 5.14.1 Making functions This is how to make a function in R: add_one &lt;- function(x) { x + 1 } add_one(41) ## [1] 42 It is not very useful. Here is an actually useful example that does the following: Pivot the data frame longer, so that there are multiple rows per occupation  one for each variable  and only two columns: variable and value Group by variable pivot_all_longer &lt;- function(.data) { .data %&gt;% pivot_longer(cols = everything(), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% group_by(variable) } Here it is in action, applied only to numeric variables: dat %&gt;% select(where(is.numeric)) %&gt;% pivot_all_longer ## # A tibble: 510 x 2 ## # Groups: variable [5] ## variable value ## &lt;chr&gt; &lt;dbl&gt; ## 1 education 13.1 ## 2 income 12351 ## 3 women 11.2 ## 4 prestige 68.8 ## 5 income_1000s 12.4 ## 6 education 12.3 ## 7 income 25879 ## 8 women 4.02 ## 9 prestige 69.1 ## 10 income_1000s 25.9 ## # ... with 500 more rows So now there are four times as many rows and two columns. But why?! You can pipe this into the summarise command and easily summarise all numeric variables as follows: dat %&gt;% select(where(is.numeric)) %&gt;% pivot_all_longer() %&gt;% summarise(M = mean(value), Mdn = median(value), SD = sd(value), valid_n = sum(!is.na(value)), n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 6 ## variable M Mdn SD valid_n n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 education 10.7 10.5 2.73 102 102 ## 2 income 6798. 5930. 4246. 102 102 ## 3 income_1000s 6.80 5.93 4.25 102 102 ## 4 prestige 46.8 43.6 17.2 102 102 ## 5 women 29.0 13.6 31.7 102 102 So its another way to create a Table 1. 5.14.2 Another way to make scatterplots: GGally Recall that select is part of tidyverse and selects variables. Here I am using the base R function is.numeric which returns TRUE if a variable is numeric, e.g., not a categorical variable. The first parameter of ggpairs is a data frame, which the %&gt;% pipe gladly provides. library(GGally) dat %&gt;% select(where(is.numeric)) %&gt;% ggpairs(upper = &quot;blank&quot;, progress = FALSE) The wiggly curves on the diagonals are smoothed histograms, also known as density plots. "],["linear-regression-diagnostics.html", "Chapter 6 Linear regression diagnostics 6.1 Before we begin 6.2 The dataset 6.3 Fit a regression model 6.4 Checking for normally distributed residuals 6.5 Checking constant residual variance 6.6 Checking for relationships between residuals and predicted outcome or predictors 6.7 Checking linearity 6.8 Checking influence: leave-one-out analyses 6.9 Checking the variance inflation factors (VIFs) 6.10 The challenge", " Chapter 6 Linear regression diagnostics A cursory glance at Chapter 8 of Fox and Weisberg (2019) will reveal that there are many diagnostic checks for regression models. An oft-cited book on diagnostics by Belsey et al. (1980) runs to 300 pages. There is an element of subjectivity in deciding which checks to use and how to interpret them, and hence a chance that authors cherry pick diagnostics to make it appear that a model explains the data better than it actually does. So, as ever, it is important to create an analysis plan before seeing data, ideally registered in some way online. By the end of this chapter you will know how to carry out common diagnostic checks of regression models using R. Many of the ideas transfer with little effort to other models we will encounter later too. I recommend that you follow along with the examples, fiddling with the code as your curiosity guides you (dont simply copy and paste code and run it  play!). The tutorial ends with an activity, similar to the sorts of things that have been asked in exams. 6.1 Before we begin You could continue on the end of last weeks Markdown file or make a new one. We will be using the same prestige.csv dataset, so whatever you do please ensure the data is saved in the same folder as your Markdown file. Ensure these handy packages are installed and included: library(car) library(tidyverse) 6.2 The dataset So its easy to find, here are the variable names again. Each row describes an occupation and aggregated data about that occupation. Variable name Description occ Occupation education Average years of education for people in the job income Average income in dollars women Percentage of women in occupation prestige A Pineo-Porter prestige score for the occupation with a possible range of 0-100 (higher = more prestigious) type bc = blue collar wc = white collar prof = professional, managerial, or technical Read it in: dat &lt;- read.csv(&quot;prestige.csv&quot;) Im going to mutate this to add the income in $1000s again. dat &lt;- dat %&gt;% mutate(income_1000s = income/1000) 6.3 Fit a regression model We will spend some time exploring this simple two-predictor model, so ensure it exists in the environment: mod_both &lt;- lm(prestige ~ education + income_1000s, data = dat) summary(mod_both) ## ## Call: ## lm(formula = prestige ~ education + income_1000s, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4040 -5.3308 0.0154 4.9803 17.6889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.8478 3.2190 -2.127 0.0359 * ## education 4.1374 0.3489 11.858 &lt; 2e-16 *** ## income_1000s 1.3612 0.2242 6.071 2.36e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.81 on 99 degrees of freedom ## Multiple R-squared: 0.798, Adjusted R-squared: 0.7939 ## F-statistic: 195.6 on 2 and 99 DF, p-value: &lt; 2.2e-16 6.3.1 Activity How should the coefficients be interpreted? 6.3.2 Answer Both education and income are statistically significant predictors of prestige (both ps &lt; .001). Each year of education is associated with 4.1 more prestige points and $1000 more income is associated with 1.4 extra prestige points (whilst holding other predictors constant). The model explains 79% of the variance in prestige and, perhaps unsurprisingly with that high an \\(R^2\\), is statistically significantly better than the intercept-only model, \\(F(2,99) = 195.6\\), \\(p &lt; .001\\). Note how that model test is included in the last line of the model summary; we can also check it explicitly with: mod0 &lt;- lm(prestige ~ 1, data = dat) anova(mod0, mod_both) ## Analysis of Variance Table ## ## Model 1: prestige ~ 1 ## Model 2: prestige ~ education + income_1000s ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 101 29895.4 ## 2 99 6038.9 2 23857 195.55 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.4 Checking for normally distributed residuals To obtain the residuals for a model, use the command resid and save them somewhere. I will add them onto the dat data frame so that it is easy to cross reference each residual with the original data: dat$mod_both_resids &lt;- resid(mod_both) These should have a normal, also known as a Gaussian distribution, in linear regression models. How do we check? 6.4.1 Base R histogram One way is to have a look at a histogram using the base R hist function: hist(dat$mod_both_resids) Is that a normal distribution? Probably, yes; however, in general ¯\\_()_/¯, it is not always obvious. There are better ways to check  read on! 6.4.2 Quantile-comparison plot An easier way to check the distribution is using a quantile-comparison plot, also known as a quantile-quantile or Q-Q plot. The car package, associated with the Fox and Weisberg (2019) textbook, has a lovely function for creating them called qqPlot: qqPlot(dat$mod_both_resids, id = list(labels = dat$occ, n = 2)) ## newsboys farmers ## 53 67 By default it always labels the two most extreme values and tells you what rows they are on. I have added the n parameter explicitly with the default value of 2 so that you can play around with it. I also used the id parameter to tell the function the names of the occupations. (Leave it out to see what happens.) The residuals are on the y-axis and the theoretically expected quantiles on a standard (mean = 0, SD = 1) normal distribution are on the x-axis. If the data we provide are identical to the normal distribution, then the points should fall along the blue diagonal line. By default, a 95% confidence envelope is drawn on (the dashed curves); around 95% of the points should be within this envelope if the data we provide has the same distribution as the comparison distribution. Too many outside this suggests deviation from the reference distribution. The residuals do indeed seem normally distributed. Read on to see what a Q-Q plot looks like when data are not normally distributed 6.4.2.1 Illustration using simulated data Lets try a Q-Q plot for a skewed distribution which is definitely not normally distributed: set.seed(42) # this line means you get the same (pseudo)random numbers as me skewed &lt;- exp(rnorm(50)) hist(skewed) Here is the qqPlot: qqPlot(skewed) ## [1] 12 9 The points do not all lie on the diagonal; indeed there is marked curvature. Now, a sample we know to be normally distributed and large (2500 participants) so it will be easy to see its shape. set.seed(5) perfectly_normal &lt;- rnorm(2500, 1000, 200) hist(perfectly_normal) Here is the qqPlot: qqPlot(perfectly_normal) ## [1] 2112 230 Note how two extreme points are always labelled; this does not mean they are outliers. 6.4.3 Statistical test of normality You could also use a statistical test of whether the residuals have a normal distribution, but it would not be a good idea. If the sample size is relatively small, then the test has low power; in other words, it wont be very good at detecting non-normal data. If the sample size is large, then the test has high power to detect even minuscule deviations from a normal distribution  deviations so small that they wont have any noticeable impact on results. For completeness here is one such test, the Shapiro-Wilk normality test. Lets try it first on perfectly_normal: shapiro.test(perfectly_normal) ## ## Shapiro-Wilk normality test ## ## data: perfectly_normal ## W = 0.99935, p-value = 0.5612 The p-value is above 0.05 so this is not statistically significant. The test checks for deviation from a normal distribution, so we did not find evidence that the data is non-normal. Note my judicious use of negatives there; I do not commit myself to belief that the data are normal! Now try again for the skewed data: shapiro.test(skewed) ## ## Shapiro-Wilk normality test ## ## data: skewed ## W = 0.72855, p-value = 2.797e-08 As one might hope, given the shape of the histogram, the p-value is very small: \\(2.8 \\times 10^{-8}\\). We can be very confident from picture and this test that the data are not normally distributed. 6.4.3.1 Activity Try the Shapiro-Wilk normality test on the residuals of mod_both. 6.4.3.2 Answer shapiro.test(resid(mod_both)) ## ## Shapiro-Wilk normality test ## ## data: resid(mod_both) ## W = 0.99402, p-value = 0.9371 Since the p-value is far above 0.05 at 0.93, we did not find any evidence that the residuals are non-normal. 6.5 Checking constant residual variance Linear regression assumes that residual variance is constant for all values of the predicted outcomes and predictors. If it is constant, then the residuals are said to be homoscedastic; otherwise if the variance varies then they are heteroscedastic. Here is a picture of made up data to illustrate, focussing initially on the predicted outcomes. We want the residuals to be homoscedastic as shown in graph a on the left. 6.5.1 Activity Above, we have fitted a model called mod_both. We already have the residuals saved in dat$mod_both_resids. You can get the predicted values of a model (i.e., ask the model to tell you a predicted prestige, based on rows of data for education and income) using the predict function; save them in dat$mod_both_predicted. Plot the residuals against predicted outcomes and assess by visual inspection whether the residuals have constant variance. 6.5.2 Answer First save the predicted values: dat$mod_both_predicted &lt;- predict(mod_both) Now plot them against the residuals: ggplot(dat, aes(x = mod_both_predicted, y = mod_both_resids)) + geom_point() + labs(x = &quot;Predicted outcome&quot;, y = &quot;Residual&quot;) The variance looks fairly constant across levels of the prediction, maybe decreasing a little as the prediction increases. The next section provides an even faster way to check for constant variance for the predicted outcome and predictors. 6.6 Checking for relationships between residuals and predicted outcome or predictors There should be no relationship between the residuals and the predicted outcome (also known as fitted values) or between the residuals and any of the predictors. This includes the mean of the residuals as well as the variance introduced in the previous section. We can check both in the same plots with residualPlots. The blue curves below show a quadratic function fitted by regression and can help spot any patterns in the mean of the residuals. In these graphs, the blue curve should ideally be a horizontal straight line (i.e., it should not be a curve!) and the points should be randomly scattered around it residualPlots(mod_both, tests = FALSE) (Look up the help for residualPlots to see what happens if you set tests = TRUE; but you probably dont need that distraction now.) The graph for income is a particular worry; we can zoom in and have a closer look: residualPlots(mod_both, terms = ~ income_1000s, fitted = FALSE, tests = FALSE) (Look up the help for residualPlots to see what happens if you set fitted = TRUE  or give it a go!) The curviness suggests that the mean of the residuals vary as a function of income, which suggets that we may wish to transform the income variable (more on this later). Its tricky to see what is going on with the variance of the residuals. Fox and Weisberg (2019, pp. 415-417) introduce a formal statistical test of non-constant variance called the Breusch-Pagan test or non-constant variance score test. For example, you can use it to check whether the residual variance varies along the magnitude of the predictions: ncvTest(mod_both) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 0.1664679, Df = 1, p = 0.68327 This is not statistically significant, \\(\\chi^2(1) = 0.17\\), \\(p = .68\\). So there is no evidence that the residual variance varies by predicted outcome. You can also check predictors. Here is a test of whether the residual variance varies as a function of income: ncvTest(mod_both, ~ income_1000s) ## Non-constant Variance Score Test ## Variance formula: ~ income_1000s ## Chisquare = 2.019597, Df = 1, p = 0.15528 This is also not statistically significant, \\(\\chi^2(1) = 2.02\\), \\(p = .16\\). See Fox and Weisberg (2019, pp. 244-252) for advice on what to do if you do find non-constant residual variance. 6.7 Checking linearity 6.7.1 What should be linear in a linear model? Linear regression models explain relationships between outcome and predictors that can be expressed in the form: \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots \\beta_n x_n \\] Confusingly (perhaps?), this does not mean that linear regression can only model linear relationships, since we can transform y and the xs in arbitrary ways. Here is a made up dataset: set.seed(202) made_up &lt;- tibble(x = rnorm(200, 0, 1), y = x^2 + rnorm(length(x), 0, .5)) And a picture thereof: made_up_scatter &lt;- made_up %&gt;% ggplot(aes(x = x, y = y)) + geom_point() made_up_scatter If you try to model x and y as-is, then the coefficients will be incorrect. wrong_mod &lt;- lm(y ~ x, data = made_up) summary(wrong_mod) ## ## Call: ## lm(formula = y ~ x, data = made_up) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9242 -0.8842 -0.4939 0.4803 5.6926 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.97169 0.09793 9.922 &lt;2e-16 *** ## x 0.06607 0.09804 0.674 0.501 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.38 on 198 degrees of freedom ## Multiple R-squared: 0.002289, Adjusted R-squared: -0.00275 ## F-statistic: 0.4542 on 1 and 198 DF, p-value: 0.5011 Adding the regression line to the data shows why: made_up_scatter + geom_abline(intercept = coef(wrong_mod)[1], slope = coef(wrong_mod)[2], colour = &quot;purple&quot;) However, if you transform the x first, then the predictions will be fine. Below, to illustrate, I am squaring x, so we are trying to estimate \\(\\beta_0\\) and \\(\\beta_1\\) for the model: \\[ y = \\beta_0 + \\beta_1 x^2 \\] better_mod &lt;- lm(y ~ I(x^2), data = made_up) summary(better_mod) ## ## Call: ## lm(formula = y ~ I(x^2), data = made_up) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.12429 -0.39648 -0.01204 0.31134 1.30659 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01933 0.04573 0.423 0.673 ## I(x^2) 0.95993 0.02752 34.875 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.517 on 198 degrees of freedom ## Multiple R-squared: 0.86, Adjusted R-squared: 0.8593 ## F-statistic: 1216 on 1 and 198 DF, p-value: &lt; 2.2e-16 The I inhibits R from trying to interpret x^2 as anything other than arithmetic: \\(x^2\\). Now lets plot the model predictions: made_up$predicted &lt;- predict(better_mod) made_up %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = predicted), color = &quot;purple&quot;) That looks a lot better! To see why linear regression is able to handle a decidedly non-linear relationship, look at \\(y\\) plotted against \\(x^2\\): ggplot(made_up, aes(x^2, y)) + geom_point() This relationship is linear, so linear regression can describe it. Regression analysis doesnt care what you do with the predictors before asking it to fit a linear model. Chapter 3.4 of Fox and Weisberg (2019) introduces the art of data transformation. In social science, it is common to use log and polynomial transformations like squaring and cubing. In areas with developed theory, more complex relationships can be conjectured which mean something in the theory, rather than merely reacting to pattern in graphs. It is possible to do all kinds of things to data to squeeze them into particular models, also known as analysing the data to within an inch of its life. Where possible, any transformations should be added to an analysis plan before the data are seen. Unanticipated transformations should be clearly noted in write-ups  this is crucially important. 6.7.2 Checking for linearity One thing you can do is look at pairwise relationships between the predictors and outcome variables with scatterplots. We already saw in last weeks tutorial a hint that there is a nonlinear relationship between income and prestige: ggplot(dat, aes(income, prestige)) + geom_point() Sometimes nonlinear relationships can only be spotted after a model has been fitted and variance explained by other predictors. A fab way to see is via a component-plus-residual plot, also known as a partial-residual plot (see Fox and Weisberg, 2019, pp. 410-412). Its a one-liner, showing the model prediction for that predictor as a straight dashed blue line and a local regression curve in magenta to help visualise the shape of the data. The magenta and blue ideally overlap. crPlots(mod_both) This is equivalent to the following line, which explicitly names all the predictors: crPlots(mod_both, terms = ~ education + income_1000s) So you can select specific predictors by naming them, which can be helpful when you are struggling to squeeze all predictors onto the screen. crPlots(mod_both, terms = ~ income_1000s) Each plot shows the predictor on the x-axis and the partial residuals on the y-axis. Each partial residual in the plot above is calculated as \\[\\epsilon_i + \\beta_{\\mathtt{income\\_1000s}} \\mathtt{income\\_1000s}_{i}\\] In other words, the residual, \\(\\epsilon_i\\), plus the slope multiplied by income; i.e., its the prediction from this part of the model. These are also known as component + residual plots. 6.7.2.1 Activity What happens to the model if you use logged-income rather than income as a predictor? Calculate a new variable, with a name of your choice, which is equal to the log of income_1000s and add it to the data frame. To log data, using the log function. Fit the regression model again, using this logged variable and education as predictors. What impact does this have on the linearity of the predictor? How do you now interpret the relationship between (logged) income and prestige? 6.7.2.2 Answer a. Calculate a new variable, with a name of your choice, which is equal to the log of income_1000s and add it to the data frame. dat$income_1000s_logged &lt;- log(dat$income_1000s) b. Fit the regression model again, using this logged variable and education as predictors. mod_both_again &lt;- lm(prestige ~ education + income_1000s_logged, data = dat) summary(mod_both_again) ## ## Call: ## lm(formula = prestige ~ education + income_1000s_logged, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.0346 -4.5657 -0.1857 4.0577 18.1270 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -16.1868 2.9662 -5.457 3.58e-07 *** ## education 4.0020 0.3115 12.846 &lt; 2e-16 *** ## income_1000s_logged 11.4375 1.4371 7.959 2.94e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.145 on 99 degrees of freedom ## Multiple R-squared: 0.831, Adjusted R-squared: 0.8275 ## F-statistic: 243.3 on 2 and 99 DF, p-value: &lt; 2.2e-16 c. What impact does this have on the linearity of the predictor? crPlots(mod_both_again) This looks much better! d. How do you now interpret the relationship between (logged) income and prestige? Both predictors were statistically significant (see the summary above) so we can focus on the coefficients: coef(mod_both_again) %&gt;% round(1) ## (Intercept) education income_1000s_logged ## -16.2 4.0 11.4 Prestige increases by 11.4 points for every unit increase in logged income, whilst holding education constant. Prestige increases by 4 points for every year of education, whilst holding logged income constant. The logged predictor is challenging to interpret; when we get to logistic regression I will introduce another way to graph predictions which may make it easier to make sense of whether they have practical significance. 6.8 Checking influence: leave-one-out analyses Leave-one-out analyses check that results havent been unduly influenced by a small number of unusual data points. They do what the name suggests: Fit a model. For every row of data: Remove the row. Refit the model. Calculate a statistic comparing the model on all data with the model which has this one row removed. Replace the row and go onto the next one. Summarise the effect for every observation in the dataset. The end result will be as many leave-one-out statistics as there are rows in the data frame. Then it is up to you, the analyst, to decide what to do with any data points identified. We will consider a range of leave-one-out statistics below, but first a picture which I hope is helpful. Here is small made up dataset with a naughty data point up at the top right. Here is an animation showing what happens to the regression slope when each row of data is removed; note what happens to the regression line when the sixth is dropped out: There are many (many) ways to assess the impact. I will cover three below: 6.8.1 Residual outliers You can test whether there are any outlier residuals with a Bonferroni Outlier Test. This is equivalent to dropping each observation in turn and seeing whether it leads to a mean shift in model estimates. The Bonferroni part refers to an p-value adjustment that accounts for the number of tests carried out, equal to the number of rows in the dataset. Here is how to use it  simply provide the model to test: outlierTest(mod_both) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 53 -2.596087 0.010879 NA The unadjusted p-value is less than 0.05; however, that p-value is an underestimate, given the large number of tests carried out, The Bonferroni-adjusted p-value is not provided since it is very high (it would be 1). So we have no evidence of any outliers. 6.8.2 Cooks distance Cooks distance measures the combined impact on all model coefficients (i.e., the intercept and slopes) of leaving out a row of data. You can calculate it as follows (I will save the results onto dat): dat$mod_both_cooks &lt;- cooks.distance(mod_both) dat$mod_both_cooks %&gt;% round(2) ## [1] 0.00 0.28 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.03 ## [16] 0.01 0.00 0.00 0.03 0.06 0.02 0.00 0.00 0.07 0.05 0.04 0.03 0.00 0.05 0.00 ## [31] 0.03 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00 ## [46] 0.02 0.00 0.01 0.01 0.00 0.01 0.02 0.07 0.03 0.00 0.00 0.00 0.00 0.00 0.00 ## [61] 0.02 0.00 0.01 0.01 0.02 0.01 0.05 0.01 0.00 0.01 0.01 0.00 0.00 0.01 0.00 ## [76] 0.00 0.00 0.00 0.01 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 ## [91] 0.02 0.01 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.00 Some statisticians suggest that a value over 1 indicates trouble (we see another threshold shortly), so we can look at the maximum value: max(dat$mod_both_cooks) ## [1] 0.2796504 That is fine. Others suggest eyeing up the data and seeing if any values for Cooks distance looks visually large relative to the others: plot(dat$mod_both_cooks, ylab = &quot;Cook&#39;s distance&quot;) The Index ranges from 1 to the total number of values. Just one stands out  which is also the maximum value we just identified. We can use filter to have a look: dat %&gt;% filter(mod_both_cooks &gt; 0.25) ## occ education income women prestige type income_1000s ## 1 general managers 12.26 25879 4.02 69.1 prof 25.879 ## mod_both_resids mod_both_predicted income_1000s_logged mod_both_cooks ## 1 -10.0029 79.1029 3.253432 0.2796504 Note that I chose the 0.25 threshold by simply looking at the graph. 6.8.3 DFBETA and (close sibling) DFBETAS DFBETA values (my favourite) are calculated for the intercept and each slope. They simply denote the difference in the coefficients between models with versus without a particular row of data. If the coefficient without an observation is larger, then the DEBETA for that observation and predictor will be positive. Calculate them with: dfbeta(mod_both) %&gt;% round(3) ## (Intercept) education income_1000s ## 1 -0.076 0.003 0.013 ## 2 -0.408 0.152 -0.200 ## 3 -0.086 0.011 0.003 ## 4 0.015 -0.001 0.005 ## 5 -0.436 0.057 -0.014 ## 6 -0.299 0.033 -0.001 ## 7 -0.349 0.046 -0.012 ## 8 -0.094 0.007 0.005 ## 9 -0.187 0.019 0.004 ## 10 -0.003 0.000 0.000 ## 11 -0.189 0.039 -0.019 ## 12 -0.098 0.019 -0.006 ## 13 0.305 -0.041 0.008 ## 14 0.082 -0.011 0.003 ## 15 -0.610 0.086 -0.028 ## 16 0.274 -0.041 0.017 ## 17 0.111 -0.002 -0.017 ## 18 0.095 -0.015 0.006 ## 19 0.646 -0.078 0.011 ## 20 -0.797 0.127 -0.064 ## 21 -0.551 0.055 0.007 ## 22 -0.100 0.016 -0.008 ## 23 0.023 -0.003 0.001 ## 24 0.261 0.027 -0.092 ## 25 0.739 -0.060 -0.033 ## 26 0.307 0.000 -0.060 ## 27 -0.332 0.071 -0.043 ## 28 -0.049 0.000 0.004 ## 29 -0.674 0.114 -0.058 ## 30 0.056 -0.006 0.000 ## 31 -0.408 0.078 -0.042 ## 32 0.026 0.011 -0.007 ## 33 -0.033 0.005 -0.001 ## 34 0.004 0.001 0.002 ## 35 0.007 -0.002 0.002 ## 36 0.035 -0.013 0.011 ## 37 -0.020 0.011 -0.009 ## 38 0.002 0.006 -0.006 ## 39 -0.010 0.005 -0.004 ## 40 -0.208 0.011 0.003 ## 41 0.320 -0.083 0.062 ## 42 0.018 -0.015 0.014 ## 43 -0.087 0.006 0.000 ## 44 -0.037 -0.003 0.006 ## 45 -0.014 -0.006 0.008 ## 46 0.044 -0.041 0.034 ## 47 -0.004 0.011 -0.009 ## 48 0.038 -0.024 0.013 ## 49 0.008 -0.022 0.021 ## 50 -0.067 0.006 -0.003 ## 51 -0.089 0.008 -0.015 ## 52 -0.135 -0.021 0.036 ## 53 -0.286 -0.043 0.080 ## 54 -0.183 -0.024 0.044 ## 55 0.006 -0.004 -0.002 ## 56 -0.007 -0.001 0.000 ## 57 0.012 0.000 0.001 ## 58 -0.034 0.004 -0.003 ## 59 0.013 -0.001 0.002 ## 60 0.015 -0.001 0.000 ## 61 -0.545 0.032 0.010 ## 62 0.111 -0.008 0.007 ## 63 -0.121 -0.015 0.031 ## 64 -0.394 0.028 0.004 ## 65 -0.632 0.049 0.001 ## 66 -0.514 0.037 0.003 ## 67 1.209 -0.099 0.005 ## 68 -0.319 0.004 0.026 ## 69 -0.162 0.015 -0.006 ## 70 0.508 -0.040 0.002 ## 71 -0.380 0.033 -0.006 ## 72 0.177 -0.015 0.003 ## 73 -0.175 0.010 0.005 ## 74 0.469 -0.042 0.007 ## 75 0.210 -0.017 0.001 ## 76 -0.079 0.008 -0.005 ## 77 0.230 -0.021 0.007 ## 78 -0.046 0.004 -0.001 ## 79 0.400 -0.039 0.013 ## 80 -0.002 0.000 0.000 ## 81 0.222 -0.020 0.007 ## 82 0.579 -0.030 -0.014 ## 83 -0.084 -0.001 0.005 ## 84 0.356 -0.029 0.000 ## 85 0.184 -0.016 0.004 ## 86 0.110 -0.010 0.006 ## 87 0.012 -0.001 0.000 ## 88 -0.043 0.005 -0.003 ## 89 0.151 -0.012 0.006 ## 90 0.678 -0.079 0.043 ## 91 0.705 -0.067 0.016 ## 92 0.601 -0.061 0.019 ## 93 -0.094 0.008 -0.001 ## 94 0.297 -0.029 0.011 ## 95 -0.176 0.014 0.000 ## 96 0.005 -0.007 0.016 ## 97 0.455 -0.053 0.030 ## 98 0.230 -0.021 0.005 ## 99 -0.336 0.025 0.000 ## 100 -0.364 0.027 -0.001 ## 101 -0.024 0.001 0.000 ## 102 0.068 -0.004 -0.002 Visualise them with: dfbetaPlots(mod_both) To work out whether a DFBETA matters, you must think what the units of the predictors are. So for example there is an observation which reduces the slope for income in $1000s by about 0.2. Is that big enough to cause concern for a measure of prestige which ranges from 0 to 100? Maybe it is worth a look to see which occupation is responsible. There is also a standardised version, DFBETAS (the S is for standardised, not a plural; pronounce it as standardised D F beta), which divides the DFBETA by the standard error of the slope in the model with the row of data removed. dfbetas(mod_both) %&gt;% round(2) ## (Intercept) education income_1000s ## 1 -0.02 0.01 0.06 ## 2 -0.13 0.44 -0.90 ## 3 -0.03 0.03 0.01 ## 4 0.00 0.00 0.02 ## 5 -0.14 0.16 -0.06 ## 6 -0.09 0.09 0.00 ## 7 -0.11 0.13 -0.05 ## 8 -0.03 0.02 0.02 ## 9 -0.06 0.05 0.02 ## 10 0.00 0.00 0.00 ## 11 -0.06 0.11 -0.09 ## 12 -0.03 0.05 -0.03 ## 13 0.09 -0.12 0.03 ## 14 0.03 -0.03 0.01 ## 15 -0.19 0.25 -0.12 ## 16 0.08 -0.12 0.07 ## 17 0.03 -0.01 -0.07 ## 18 0.03 -0.04 0.03 ## 19 0.20 -0.22 0.05 ## 20 -0.25 0.37 -0.29 ## 21 -0.17 0.16 0.03 ## 22 -0.03 0.05 -0.03 ## 23 0.01 -0.01 0.00 ## 24 0.08 0.08 -0.41 ## 25 0.23 -0.17 -0.15 ## 26 0.10 0.00 -0.27 ## 27 -0.10 0.21 -0.20 ## 28 -0.02 0.00 0.02 ## 29 -0.21 0.33 -0.26 ## 30 0.02 -0.02 0.00 ## 31 -0.13 0.23 -0.19 ## 32 0.01 0.03 -0.03 ## 33 -0.01 0.02 -0.01 ## 34 0.00 0.00 0.01 ## 35 0.00 -0.01 0.01 ## 36 0.01 -0.04 0.05 ## 37 -0.01 0.03 -0.04 ## 38 0.00 0.02 -0.03 ## 39 0.00 0.01 -0.02 ## 40 -0.06 0.03 0.02 ## 41 0.10 -0.24 0.28 ## 42 0.01 -0.04 0.06 ## 43 -0.03 0.02 0.00 ## 44 -0.01 -0.01 0.02 ## 45 0.00 -0.02 0.04 ## 46 0.01 -0.12 0.16 ## 47 0.00 0.03 -0.04 ## 48 0.01 -0.07 0.06 ## 49 0.00 -0.06 0.10 ## 50 -0.02 0.02 -0.01 ## 51 -0.03 0.02 -0.07 ## 52 -0.04 -0.06 0.16 ## 53 -0.09 -0.13 0.37 ## 54 -0.06 -0.07 0.20 ## 55 0.00 -0.01 -0.01 ## 56 0.00 0.00 0.00 ## 57 0.00 0.00 0.00 ## 58 -0.01 0.01 -0.01 ## 59 0.00 0.00 0.01 ## 60 0.00 0.00 0.00 ## 61 -0.17 0.09 0.04 ## 62 0.03 -0.02 0.03 ## 63 -0.04 -0.04 0.14 ## 64 -0.12 0.08 0.02 ## 65 -0.20 0.14 0.00 ## 66 -0.16 0.11 0.01 ## 67 0.38 -0.29 0.02 ## 68 -0.10 0.01 0.12 ## 69 -0.05 0.04 -0.03 ## 70 0.16 -0.12 0.01 ## 71 -0.12 0.10 -0.03 ## 72 0.05 -0.04 0.01 ## 73 -0.05 0.03 0.02 ## 74 0.15 -0.12 0.03 ## 75 0.07 -0.05 0.00 ## 76 -0.02 0.02 -0.02 ## 77 0.07 -0.06 0.03 ## 78 -0.01 0.01 -0.01 ## 79 0.12 -0.11 0.06 ## 80 0.00 0.00 0.00 ## 81 0.07 -0.06 0.03 ## 82 0.18 -0.09 -0.07 ## 83 -0.03 0.00 0.02 ## 84 0.11 -0.08 0.00 ## 85 0.06 -0.05 0.02 ## 86 0.03 -0.03 0.03 ## 87 0.00 0.00 0.00 ## 88 -0.01 0.01 -0.01 ## 89 0.05 -0.03 0.02 ## 90 0.21 -0.23 0.20 ## 91 0.22 -0.19 0.07 ## 92 0.19 -0.17 0.08 ## 93 -0.03 0.02 0.00 ## 94 0.09 -0.08 0.05 ## 95 -0.05 0.04 0.00 ## 96 0.00 -0.02 0.07 ## 97 0.14 -0.15 0.13 ## 98 0.07 -0.06 0.02 ## 99 -0.10 0.07 0.00 ## 100 -0.11 0.08 0.00 ## 101 -0.01 0.00 0.00 ## 102 0.02 -0.01 -0.01 Visualise DFBETAS with: dfbetasPlots(mod_both) Note how the graphs look identical to those for DFBETA except that the y-axis scale has changed. Some statisticians argue that a DFBETAS (i.e., the standardised one) value over \\(1\\) or below \\(-1\\) is potentially troublesome, so should be inspected. 6.8.4 View them all You can obtain a HUGE data frame of leave-one-out-analyses by using the influence.measures command. This gives the following measures. Measure Thresholds used by R DFBETAS (for each model variable) \\(|\\mathtt{DFBETAS}| &gt; 1\\) DFFIT \\(|\\mathtt{DFFIT}| &gt; 3 \\sqrt{k/(n - k)}\\) covariance ratios (COVRATIO) \\(|1 - \\mathtt{COVRATIO}| &gt; 3k/(n - k)\\) Cooks distance Over the median of \\(F(k, n-k)\\) Diagonal elements of the hat matrix Over \\(3k/n\\) The R help for this command is spectacularly poor so I have added in the thresholds as per the current R code (its all open source), where \\(k\\) is the number of predictors (including the intercept) and \\(n\\) is the number of observations. The absolute value of \\(x\\), written \\(|x|\\), means remove any negative sign, so \\(|-42| = 42\\). Any values over the relevant threshold are marked with an asterisk. Bollen and Jackman (1985) provide alternative recommendations, e.g., \\(2/\\sqrt{n}\\) for DFBETAS. Others refuse to name a threshold and instead emphasise looking at the pattern of values and using subjective judgement to determine whether any are outlying. Lets have a look at all the influence measures for our model, mod_both. mod_both_influence &lt;- influence.measures(mod_both) mod_both_influence ## Influence measures of ## lm(formula = prestige ~ education + income_1000s, data = dat) : ## ## dfb.1_ dfb.edct dfb.i_10 dffit cov.r cook.d hat inf ## 1 -0.023428 0.008369 5.92e-02 0.098882 1.048 3.28e-03 0.02693 ## 2 -0.127604 0.439484 -9.00e-01 -0.921848 1.321 2.80e-01 0.27146 * ## 3 -0.026683 0.030933 1.15e-02 0.077727 1.035 2.03e-03 0.01564 ## 4 0.004730 -0.002131 2.34e-02 0.061706 1.034 1.28e-03 0.01217 ## 5 -0.135547 0.163813 -6.04e-02 0.201997 1.028 1.36e-02 0.03277 ## 6 -0.092549 0.093654 -3.14e-03 0.128644 1.063 5.55e-03 0.04179 ## 7 -0.108326 0.130767 -5.41e-02 0.153550 1.055 7.89e-03 0.03995 ## 8 -0.028984 0.021024 2.15e-02 0.052012 1.080 9.10e-04 0.04733 ## 9 -0.058001 0.053674 1.95e-02 0.099890 1.052 3.35e-03 0.02997 ## 10 -0.001008 0.000979 1.94e-04 0.001639 1.063 9.05e-07 0.03048 ## 11 -0.058743 0.110713 -8.53e-02 0.168031 1.002 9.36e-03 0.01810 ## 12 -0.030339 0.053875 -2.70e-02 0.097830 1.025 3.20e-03 0.01413 ## 13 0.094863 -0.117254 3.49e-02 -0.162101 1.022 8.75e-03 0.02361 ## 14 0.025302 -0.031457 1.30e-02 -0.038651 1.063 5.03e-04 0.03158 ## 15 -0.191087 0.248192 -1.24e-01 0.297268 0.986 2.90e-02 0.03303 ## 16 0.084795 -0.118001 7.46e-02 -0.138343 1.053 6.41e-03 0.03641 ## 17 0.034406 -0.005921 -7.45e-02 -0.100998 1.136 3.43e-03 0.09547 * ## 18 0.029378 -0.041536 2.73e-02 -0.048705 1.068 7.98e-04 0.03689 ## 19 0.201674 -0.223985 5.15e-02 -0.282897 1.007 2.64e-02 0.03777 ## 20 -0.250322 0.368606 -2.86e-01 0.419809 0.992 5.75e-02 0.05349 ## 21 -0.171334 0.157351 3.17e-02 0.244508 1.043 1.99e-02 0.04700 ## 22 -0.030898 0.046882 -3.41e-02 0.057072 1.062 1.10e-03 0.03238 ## 23 0.007169 -0.008766 3.87e-03 -0.010199 1.075 3.50e-05 0.04074 ## 24 0.080937 0.076089 -4.10e-01 -0.466055 1.261 7.25e-02 0.20340 * ## 25 0.231448 -0.172875 -1.48e-01 -0.385996 1.006 4.89e-02 0.05363 ## 26 0.095555 -0.000033 -2.67e-01 -0.352122 1.060 4.11e-02 0.07268 ## 27 -0.104151 0.206102 -1.95e-01 0.289319 0.959 2.73e-02 0.02522 ## 28 -0.015223 0.000708 1.68e-02 -0.034136 1.045 3.92e-04 0.01584 ## 29 -0.212773 0.332168 -2.61e-01 0.401687 0.940 5.20e-02 0.03606 ## 30 0.017326 -0.018125 1.44e-03 -0.024827 1.069 2.08e-04 0.03652 ## 31 -0.128317 0.226702 -1.90e-01 0.304204 0.950 3.01e-02 0.02528 ## 32 0.008028 0.032435 -3.32e-02 0.130383 0.993 5.63e-03 0.01066 ## 33 -0.010208 0.015309 -5.88e-03 0.025523 1.046 2.19e-04 0.01581 ## 34 0.001229 0.002525 7.03e-03 0.032990 1.039 3.66e-04 0.01096 ## 35 0.002016 -0.006535 7.89e-03 -0.011300 1.053 4.30e-05 0.02103 ## 36 0.010853 -0.037897 5.00e-02 -0.064902 1.053 1.42e-03 0.02598 ## 37 -0.006050 0.030210 -3.87e-02 0.060053 1.043 1.21e-03 0.01754 ## 38 0.000672 0.015874 -2.87e-02 0.036888 1.055 4.58e-04 0.02479 ## 39 -0.003224 0.014731 -1.86e-02 0.028660 1.048 2.76e-04 0.01787 ## 40 -0.064468 0.031375 1.56e-02 -0.100111 1.022 3.35e-03 0.01340 ## 41 0.100886 -0.240817 2.81e-01 -0.354356 0.954 4.07e-02 0.03281 ## 42 0.005578 -0.041646 6.38e-02 -0.083023 1.048 2.31e-03 0.02424 ## 43 -0.026796 0.016210 -7.76e-04 -0.039571 1.041 5.27e-04 0.01287 ## 44 -0.011527 -0.007312 2.47e-02 -0.043438 1.043 6.35e-04 0.01538 ## 45 -0.004469 -0.018502 3.64e-02 -0.051660 1.047 8.97e-04 0.01958 ## 46 0.013861 -0.119945 1.55e-01 -0.269456 0.909 2.33e-02 0.01512 * ## 47 -0.001148 0.030194 -3.91e-02 0.075892 1.032 1.93e-03 0.01364 ## 48 0.011822 -0.069139 5.78e-02 -0.186477 0.953 1.14e-02 0.01155 ## 49 0.002438 -0.063848 9.54e-02 -0.146894 1.010 7.18e-03 0.01711 ## 50 -0.020867 0.016934 -1.41e-02 -0.037097 1.041 4.63e-04 0.01271 ## 51 -0.027833 0.021911 -6.68e-02 -0.157596 0.982 8.20e-03 0.01220 ## 52 -0.042088 -0.060365 1.59e-01 -0.224638 0.981 1.66e-02 0.02103 ## 53 -0.091547 -0.125400 3.69e-01 -0.464748 0.872 6.81e-02 0.03105 * ## 54 -0.057555 -0.070047 1.99e-01 -0.278165 0.950 2.52e-02 0.02196 ## 55 0.001836 -0.010405 -1.02e-02 -0.066674 1.030 1.49e-03 0.01105 ## 56 -0.002038 -0.002340 6.57e-04 -0.018692 1.040 1.18e-04 0.00998 ## 57 0.003585 -0.001176 4.91e-03 0.019633 1.041 1.30e-04 0.01058 ## 58 -0.010555 0.011195 -1.14e-02 -0.017565 1.052 1.04e-04 0.02058 ## 59 0.004073 -0.003789 7.99e-03 0.016463 1.044 9.13e-05 0.01289 ## 60 0.004494 -0.002684 -1.04e-03 0.005532 1.055 1.03e-05 0.02256 ## 61 -0.171011 0.092960 4.36e-02 -0.231651 0.956 1.75e-02 0.01707 ## 62 0.034575 -0.023880 3.32e-02 0.099449 1.015 3.30e-03 0.01107 ## 63 -0.037589 -0.043482 1.38e-01 -0.173367 1.038 1.00e-02 0.03290 ## 64 -0.122230 0.079256 1.87e-02 -0.142306 1.034 6.77e-03 0.02570 ## 65 -0.196906 0.141189 2.46e-03 -0.218212 1.007 1.58e-02 0.02731 ## 66 -0.159958 0.107108 1.32e-02 -0.185839 1.010 1.15e-02 0.02319 ## 67 0.384108 -0.290933 2.39e-02 0.414395 0.902 5.47e-02 0.03011 * ## 68 -0.099258 0.012807 1.15e-01 -0.195151 1.009 1.26e-02 0.02443 ## 69 -0.050001 0.042853 -2.54e-02 -0.066229 1.040 1.47e-03 0.01687 ## 70 0.158012 -0.115778 9.18e-03 0.177763 1.014 1.05e-02 0.02347 ## 71 -0.117971 0.095052 -2.76e-02 -0.131785 1.033 5.81e-03 0.02360 ## 72 0.054762 -0.044123 1.28e-02 0.061175 1.051 1.26e-03 0.02360 ## 73 -0.054020 0.028264 2.34e-02 -0.070113 1.054 1.65e-03 0.02750 ## 74 0.145466 -0.120356 3.13e-02 0.154088 1.044 7.94e-03 0.03296 ## 75 0.065093 -0.049111 3.20e-03 0.070059 1.059 1.65e-03 0.03113 ## 76 -0.024506 0.021447 -2.27e-02 -0.049523 1.039 8.24e-04 0.01312 ## 77 0.071291 -0.060176 3.32e-02 0.092860 1.033 2.89e-03 0.01691 ## 78 -0.014151 0.012328 -6.57e-03 -0.017311 1.051 1.01e-04 0.01995 ## 79 0.124160 -0.111672 5.88e-02 0.144669 1.030 6.99e-03 0.02440 ## 80 -0.000579 0.000457 -1.64e-04 -0.000704 1.050 1.67e-07 0.01786 ## 81 0.068768 -0.057364 3.02e-02 0.088902 1.034 2.65e-03 0.01684 ## 82 0.182926 -0.087059 -6.57e-02 0.268211 0.917 2.32e-02 0.01597 ## 83 -0.026035 -0.001804 2.09e-02 -0.079813 1.024 2.13e-03 0.01081 ## 84 0.110300 -0.081606 -6.14e-04 0.118292 1.056 4.69e-03 0.03506 ## 85 0.056802 -0.046744 1.81e-02 0.066283 1.046 1.48e-03 0.02060 ## 86 0.034145 -0.027442 2.69e-02 0.069259 1.031 1.61e-03 0.01218 ## 87 0.003639 -0.003079 9.35e-04 0.003854 1.067 5.00e-06 0.03381 ## 88 -0.013165 0.013295 -1.15e-02 -0.019240 1.053 1.25e-04 0.02119 ## 89 0.046698 -0.033770 2.49e-02 0.086927 1.023 2.53e-03 0.01163 ## 90 0.211996 -0.229395 1.95e-01 0.287493 0.993 2.72e-02 0.03354 ## 91 0.219723 -0.191586 7.30e-02 0.236191 1.013 1.85e-02 0.03227 ## 92 0.186715 -0.173845 8.41e-02 0.201891 1.041 1.36e-02 0.03942 ## 93 -0.029161 0.021401 -2.52e-03 -0.033420 1.052 3.76e-04 0.02133 ## 94 0.091979 -0.083887 5.04e-02 0.113012 1.036 4.28e-03 0.02185 ## 95 -0.054561 0.038658 -5.22e-05 -0.061911 1.051 1.29e-03 0.02358 ## 96 0.001690 -0.021106 6.89e-02 0.083169 1.070 2.33e-03 0.04120 ## 97 0.141671 -0.152423 1.32e-01 0.197000 1.023 1.29e-02 0.03016 ## 98 0.071064 -0.060197 2.30e-02 0.079219 1.050 2.11e-03 0.02518 ## 99 -0.104242 0.071422 1.24e-03 -0.122924 1.029 5.05e-03 0.02029 ## 100 -0.112988 0.076393 -2.55e-03 -0.139650 1.014 6.49e-03 0.01727 ## 101 -0.007405 0.003961 -1.36e-03 -0.014894 1.041 7.47e-05 0.01062 ## 102 0.021023 -0.010209 -7.91e-03 0.029911 1.048 3.01e-04 0.01739 That is Too Much Information. The first three are the DFBETAS values, with abbreviated variable names: dfb.1_ (intercept) dfb.edct (education) dfb.i_10 (income in 1000s) Cooks distance is over to the right as cook.d. One way to deal with this mess is to look only at those rows where any of the measures cross their threshold, which we get with a handy summary command (hurrah!): summary(mod_both_influence) ## Potentially influential observations of ## lm(formula = prestige ~ education + income_1000s, data = dat) : ## ## dfb.1_ dfb.edct dfb.i_10 dffit cov.r cook.d hat ## 2 -0.13 0.44 -0.90 -0.92_* 1.32_* 0.28 0.27_* ## 17 0.03 -0.01 -0.07 -0.10 1.14_* 0.00 0.10_* ## 24 0.08 0.08 -0.41 -0.47 1.26_* 0.07 0.20_* ## 46 0.01 -0.12 0.16 -0.27 0.91_* 0.02 0.02 ## 53 -0.09 -0.13 0.37 -0.46 0.87_* 0.07 0.03 ## 67 0.38 -0.29 0.02 0.41 0.90_* 0.05 0.03 There are no problems for any DFBETAS or Cooks distance. Exercise to the interested reader to see if the values for DFFIT, covariance ratios, or diagonals of the hat matrix are a concern. 6.8.5 So, er, what should we do with potentially influential observations? Now we are back to the art of analysis. You could try removing all the potentially influence observations by using slice with a - like so: sliced_dat &lt;- dat %&gt;% slice(-c(2,17,24,46,53,67)) Then refit the model: mod_both_sliced &lt;- lm(prestige ~ education + income_1000s, data = sliced_dat) summary(mod_both_sliced) ## ## Call: ## lm(formula = prestige ~ education + income_1000s, data = sliced_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.9509 -4.3412 -0.1208 4.9519 16.3430 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.0248 3.0527 -2.629 0.01 * ## education 4.0955 0.3453 11.862 &lt; 2e-16 *** ## income_1000s 1.6756 0.3010 5.566 2.5e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.212 on 93 degrees of freedom ## Multiple R-squared: 0.8077, Adjusted R-squared: 0.8035 ## F-statistic: 195.3 on 2 and 93 DF, p-value: &lt; 2.2e-16 Both predictors are still statistically significant Have the coefficients changed? cbind(&quot;Original&quot; = coef(mod_both), &quot;Outliers out&quot; = coef(mod_both_sliced)) %&gt;% round(2) ## Original Outliers out ## (Intercept) -6.85 -8.02 ## education 4.14 4.10 ## income_1000s 1.36 1.68 The coefficient for education is about the same; the coefficient for income has increased a little. At the moment, I dont have any substantive reason to exclude these six values. Maybe if you have a developed theory of occupations then looking in more detail at the potentially outlying ones will help? Heres how to see them  Im using slice again, this time without the - which means to keep rather than exclude the listed rows. I have also used select to focus on variables which I thought might help work out what is going on. dat %&gt;% slice(c(2,17,24,46,53,67)) %&gt;% select(occ, education, income_1000s, prestige, mod_both_predicted) ## occ education income_1000s prestige mod_both_predicted ## 1 general managers 12.26 25.879 69.1 79.10290 ## 2 lawyers 15.77 19.263 82.3 84.61986 ## 3 physicians 15.96 25.308 87.2 93.63422 ## 4 collectors 11.20 4.741 29.4 45.94489 ## 5 newsboys 9.62 0.918 14.8 34.20399 ## 6 farmers 6.84 3.643 44.1 26.41107 Influential observations may indicate a more systemic problem with the model, e.g., non-linearity, especially if a large proportion of observations show up. They may also be qualitatively interesting and worthy of further research! 6.9 Checking the variance inflation factors (VIFs) Multicollinearity is a long word meaning that two or more predictors are highly linearly correlated with each other. This is a problem for interpreting coefficients since we interpret each one whilst holding the others constant. But if they are highly correlated, then holding other predictors constant is challenging! There is a command called vif in the car package which calculates variance inflation factors for us and can be used to check for multicolinearity: vif(mod_both) ## education income_1000s ## 1.500598 1.500598 We want the VIFs close to 1. If they are 4 then that is cause for mild worry and 9 probably signals that something has to be done, e.g., removing a predictor. We can interpret the meaning of VIFs by taking their square root: this says how many times wider the 95% confidence intervals are compared to what they would be with uncorrelated predictors: sqrt(vif(mod_both)) ## education income_1000s ## 1.224989 1.224989 So the trouble thresholds I provided above lead to confidence intervals that are twice (VIF = 4; \\(\\sqrt{4}=2\\)) or three times (VIF = 9; \\(\\sqrt{9}=3\\)) the width of those for uncorrelated predictors. 6.9.1 Activity Try fitting a model predicting prestige which has education, income (in 1000s) and logged income in thousands. Before looking, what do you suspect might happen to the VIFs? Check and interpret the answer. 6.9.2 Answer viffed_model &lt;- lm(prestige ~ education + income_1000s + income_1000s_logged, data = dat) sqrt(vif(viffed_model)) ## education income_1000s income_1000s_logged ## 1.230514 2.231250 2.177696 The width of the confidence intervals for both income variables is double what they would be if all predictors were uncorrelated, which suggests something is up The logging has meant that the predictors are not perfectly linearly correlated, but it is challenging to interpret income whilst holding logged income constant. 6.10 The challenge As promised, here is a modelling challenge for you: 6.10.1 Activity Fit a model predicting prestige from education, income (in $1000s, logged), and the percentage of women in the occupation Does this model fit better than the model without the percentage of women added? Is there multicollinearity? Are the relationships linear? Try adding the percentage of women squared as an additional predictor and checking for linearity again. Does the model you just fitted (in e) explain statistically significantly more variance than the model with only income and education as predictors? Check the DFBETAS values (again for the model fitted in e)  are any concerning? 6.10.2 Answer a. Fit a model predicting prestige from education, income (in $1000s, logged), and the percentage of women in the occupation challenge_mod &lt;- lm(prestige ~ education + income_1000s_logged + women, data = dat) b. Does this model fit better than the model without the percentage of women added? Ive lost track of what models I have fitted above, so here is the relevant simpler model again: without_gender_mod &lt;- lm(prestige ~ education + income_1000s_logged, data = dat) Compare the two models with an F-test, simpler first: anova(without_gender_mod, challenge_mod) ## Analysis of Variance Table ## ## Model 1: prestige ~ education + income_1000s_logged ## Model 2: prestige ~ education + income_1000s_logged + women ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 5053.6 ## 2 98 4929.9 1 123.75 2.4601 0.12 There is no statistically significant improvement in model fit, \\(F(1,98) = 2.5\\), \\(p = .12\\). c. Is there multicollinearity? vif(challenge_mod) ## education income_1000s_logged women ## 1.877097 2.572283 1.806431 We can interpret the VIFs by taking their square root: this says how many times wider the 95% confidence intervals would be compared with uncorrelated predictors: sqrt(vif(challenge_mod)) ## education income_1000s_logged women ## 1.370072 1.603834 1.344035 So the largest VIF is for income, and the correlations in predictors mean its confidence interval is about 1.6 times wider. Based on finger-in-the-wind subjective judgement, I am going to conclude that this doesnt matter. Though it may do if I wanted a particularly precise estimate. Confint(challenge_mod) %&gt;% round(2) ## Estimate 2.5 % 97.5 % ## (Intercept) -18.14 -24.48 -11.79 ## education 3.73 3.03 4.43 ## income_1000s_logged 13.44 9.64 17.24 ## women 0.05 -0.01 0.11 d. Are the relationships linear? crPlots(challenge_mod) It looks like a bit of a curve for the predictor of percentage women in the occupation, which might suggest adding a squared term  which coincidentally we try in the next question e. Try adding the percentage of women squared as an additional predictor and checking for linearity again challenge_mod2 &lt;- lm(prestige ~ education + income_1000s_logged + women + I(women^2), data = dat) crPlots(challenge_mod2) Now the relationships are much more linear. f. Does the model you just fitted (in e) explain statistically significantly more variance than the model with only income and education as predictors? anova(without_gender_mod, challenge_mod2) ## Analysis of Variance Table ## ## Model 1: prestige ~ education + income_1000s_logged ## Model 2: prestige ~ education + income_1000s_logged + women + I(women^2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 5053.6 ## 2 97 4679.8 2 373.88 3.8748 0.02405 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes it does, \\(F(2, 97) = 3.9\\), \\(p = .024\\). g. Check the DFBETAS values (again for the model fitted in e)  are any concerning? summary(influence.measures(challenge_mod2)) ## Potentially influential observations of ## lm(formula = prestige ~ education + income_1000s_logged + women + I(women^2), data = dat) : ## ## dfb.1_ dfb.edct dfb.i_10 dfb.womn dfb.I(^2 dffit cov.r cook.d hat ## 2 0.07 0.08 -0.15 0.01 -0.03 -0.17 1.16_* 0.01 0.10 ## 20 0.07 0.69 -0.63 -0.32 0.13 0.80_* 0.89 0.12 0.10 ## 53 -0.13 -0.15 0.23 0.06 0.00 -0.24 1.46_* 0.01 0.29_* ## 63 0.30 0.32 -0.53 -0.32 0.30 0.70_* 1.28_* 0.10 0.24_* ## 67 0.53 -0.16 -0.18 -0.20 0.11 0.58 0.74_* 0.06 0.04 ## 68 0.05 0.03 -0.07 0.02 -0.04 0.09 1.18_* 0.00 0.11 ## 84 0.03 -0.07 0.05 -0.01 0.05 0.12 1.18_* 0.00 0.11 No |DFBETAS| values are over 1, if that is your definition of concerning. "],["categorical-predictors-and-interactions.html", "Chapter 7 Categorical predictors and interactions 7.1 Before we begin 7.2 The dataset 7.3 Factors 7.4 Visualising the data 7.5 The punchline: occupation type does predict prestige 7.6 Understanding factors in regression models 7.7 Interpreting the coefficients 7.8 Checking all combinations 7.9 The intercept is not always the mean of the comparison group 7.10 Recap 7.11 Challenge 7.12 Brief introduction to interactions", " Chapter 7 Categorical predictors and interactions By the end of this chapter you will: Understand how to use R factors, which automatically deal with fiddly aspects of using categorical predictors in statistical models. Be able to relate R output to what is going on behind the scenes, i.e., coding of a category with \\(n\\)-levels in terms of \\(n-1\\) binary 0/1 predictors. See how R does interactions effects (the * and : operators). The lovely thing about the topics covered in this chapter is that they all transfer to every other model, e.g., logistic regression and other generalised linear models, multilevel models, and a huge number of models which you may encounter in future. 7.1 Before we begin We are going to persist with the prestige dataset another time. (Last chapter, promise.) You could continue on the end of last weeks Markdown file or make a new one. Ensure these handy packages are installed (you will only have to do that once) and included: library(car) library(tidyverse) 7.2 The dataset For ease of reference. Each row describes an occupation and aggregated data about that occupation (in 1970s Canada). Variable name Description occ Occupation education Average years of education for people in the job income Average income in dollars women Percentage of women in occupation prestige A Pineo-Porter prestige score for the occupation with a possible range of 0-100 (higher = more prestigious) type bc = blue collar wc = white collar prof = professional, managerial, or technical Read it in: dat &lt;- read.csv(&quot;prestige.csv&quot;) This week we will explore whether there are mean differences in prestige between the three types of profession: blue collar, white collar, and professional. 7.3 Factors R has a special kind of object called a factor for representing categorical variables in statistical models. Ensure that R knows that you want to treat type as a factor by doing the following: dat$type &lt;- factor(dat$type) You can check whether a variable is a factor by doing: is.factor(dat$type) ## [1] TRUE Heres a variable that isnt: is.factor(dat$prestige) ## [1] FALSE Its numeric is.numeric(dat$prestige) ## [1] TRUE You can also look up the class of a variable: class(dat$type) ## [1] &quot;factor&quot; You can check the levels of a factor by using the levels function: levels(dat$type) ## [1] &quot;bc&quot; &quot;prof&quot; &quot;wc&quot; The order is alphabetical by default and can easily be changed. The levels function only works for factors: levels(dat$prestige) ## NULL 7.4 Visualising the data (I go through this in some detail, for information. Feel free to skip over it on first reading.) We will be exploring whether there are differences in a continuous variable (prestige) between levels of a categorical variable (type). A traditional visualisation for this genre of comparison is the bar plot, with either standard error of the mean (SE mean) or standard deviation (SD) error bars. These days, the recommendation is to look at the raw data before reducing it to means, for instance with a jitter plot. set.seed(100) # Optional dat %&gt;% ggplot(aes(type, prestige, colour = type)) + geom_jitter(height = 0, width = .1, show.legend = FALSE) + ylim(0,100) We can see 4 observations in an NA category. Those are missing data, i.e., values in the dataset which have not been assigned a type. dat %&gt;% filter(is.na(type)) ## occ education income women prestige type ## 1 athletes 11.44 8206 8.13 54.1 &lt;NA&gt; ## 2 newsboys 9.62 918 7.00 14.8 &lt;NA&gt; ## 3 babysitters 9.46 611 96.53 25.9 &lt;NA&gt; ## 4 farmers 6.84 3643 3.60 44.1 &lt;NA&gt; Also note that I have fixed the y-axis scale so that it covers the theoretical range of the prestige measure (0-100) rather than only the values present in the data. Here is how to make the traditional bar plot. First calculate the means and standard error of the means for each group. The standard error is calculated as \\[ \\frac{\\mathit{SD}}{\\sqrt{N}} \\] where \\(N\\) is the number of observations. R has a function called sqrt for calculating the square root: sqrt(4) ## [1] 2 We use group_by as before to ask tidyverse to group observations by type of occupation, and then summarise the data. The n function simply counts the number of observations, which will also be calculated per group following the group_by. dat_means &lt;- dat %&gt;% group_by(type) %&gt;% summarise(mean = mean(prestige), SD = sd(prestige), SE = SD/sqrt(n()), n = n()) dat_means ## # A tibble: 4 x 5 ## type mean SD SE n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 bc 35.5 10.0 1.51 44 ## 2 prof 67.8 8.68 1.56 31 ## 3 wc 42.2 9.52 1.98 23 ## 4 &lt;NA&gt; 34.7 17.7 8.84 4 Here is how I went about plotting these values. I began by plotting the means: dat_means %&gt;% ggplot(aes(type, mean)) + geom_bar(stat = &quot;identity&quot;) Then I added error bars: dat_means %&gt;% ggplot(aes(type, mean)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - SE, ymax = mean + SE)) Following this, I fiddled around for fifteen minutes, wishing I had just looked up a bar plot I had made previously (The lesson here is, build a stash of example R code you can quickly find, adapt, and reuse.) dat_means %&gt;% filter(!is.na(type)) %&gt;% ggplot(aes(type, mean, fill = type)) + geom_bar(stat = &quot;identity&quot;, show.legend = F) + geom_errorbar(aes(ymin = mean - SE, ymax = mean + SE), width = 1/4) + xlab(&quot;Occupation type&quot;) + ylab(&quot;Mean prestige&quot;) + ylim(0, 100) + theme_classic() You might want to add standard deviation bars instead: dat_means %&gt;% filter(!is.na(type)) %&gt;% ggplot(aes(type, mean, fill = type)) + geom_bar(stat = &quot;identity&quot;, show.legend = F) + geom_errorbar(aes(ymin = mean - SD, ymax = mean + SD), width = 1/4) + xlab(&quot;Occupation type&quot;) + ylab(&quot;Mean prestige&quot;) + ylim(0, 100) + theme_classic() You might also want less vibrant colours, e.g., if you wish to include the graph in a publication: dat_means %&gt;% filter(!is.na(type)) %&gt;% ggplot(aes(type, mean)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;grey&quot;) + geom_errorbar(aes(ymin = mean - SD, ymax = mean + SD), width = 1/4) + xlab(&quot;Occupation type&quot;) + ylab(&quot;Mean prestige&quot;) + ylim(0,100) + theme_classic() These are the chunk options I used to change the dimensions of the figure, which might be handy to know about too (they go at the top of the chunk): {r fig.height=3, fig.width=3} 7.5 The punchline: occupation type does predict prestige This is where we are heading. First, select the variables you want to model and remove missing values: dat_no_NAs &lt;- dat %&gt;% select(occ, prestige, type) %&gt;% na.omit() We could just have removed missing variables from the whole dataset  it would have worked for the prestige dataset since there are only four missing values and they are all in one variable. However, for larger datasets doing so might easily wipe out everything! For more on handling missing data correctly, try the book by van Buuren (2018). Now, fit an intercept-only model and compare it with a model which has type as a predictor. R will automatically do something sensible with type since it knows that it is a factor. intercept_only &lt;- lm(prestige ~ 1, data = dat_no_NAs) type_mod &lt;- lm(prestige ~ type, data = dat_no_NAs) # (the 1 is implicit) anova(intercept_only, type_mod) ## Analysis of Variance Table ## ## Model 1: prestige ~ 1 ## Model 2: prestige ~ type ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 28346.9 ## 2 95 8571.3 2 19776 109.59 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Occupation type does indeed explain variation in prestige, \\(F(2,95) = 109.59\\), \\(p &lt; .001\\). An even easier way to do this is using cars Anova command (uppercase A). This does not require you to fit an intercept-only model yourself and deals with multiple predictors (handy later): type_mod &lt;- lm(prestige ~ type, data = dat_no_NAs) Anova(type_mod) ## Anova Table (Type II tests) ## ## Response: prestige ## Sum Sq Df F value Pr(&gt;F) ## type 19775.6 2 109.59 &lt; 2.2e-16 *** ## Residuals 8571.3 95 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Read on to see why this has worked and also to work out which of the three occupations differ from each other in prestige 7.6 Understanding factors in regression models 7.6.1 How are categorical variables encoded? Lets begin with an easier example with only two groups: blue collar and white collar occupations: just_two &lt;- dat %&gt;% filter(type %in% c(&quot;bc&quot;,&quot;wc&quot;)) %&gt;% select(occ, prestige, type) Here are the means of prestige for both groups: just_two %&gt;% group_by(type) %&gt;% summarise(mean_prestige = mean(prestige)) ## # A tibble: 2 x 2 ## type mean_prestige ## &lt;fct&gt; &lt;dbl&gt; ## 1 bc 35.5 ## 2 wc 42.2 Next, lets fit a regression model, predicting prestige from occupation type: easier &lt;- lm(prestige ~ type, data = just_two) summary(easier) ## ## Call: ## lm(formula = prestige ~ type, data = just_two) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.2273 -7.0273 -0.2273 6.8227 25.2565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.527 1.486 23.914 &lt;2e-16 *** ## typewc 6.716 2.536 2.649 0.0101 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.855 on 65 degrees of freedom ## Multiple R-squared: 0.09742, Adjusted R-squared: 0.08353 ## F-statistic: 7.016 on 1 and 65 DF, p-value: 0.01013 Note how we told lm that the predictor is type; however, the summary is displaying a slope for typewc. Curious 7.6.1.1 Activity Examine the estimates (the intercept and the slope for typewc) in this model and compare them with the means of the two groups. Can you see where the numbers come from? 7.6.1.2 Answer The intercept is the same as the mean prestige for bc (\\(35.527\\)) and the slope for typewc is equal to the mean difference in prestige between the two groups: white collar minus blue collar (\\(42.243 - 35.527 = 6.716\\)). 7.6.2 How are binary (two-level) categorical predictors encoded? First let me emphasise that you do not need to do this coding yourself; this section aims to demonstrate how the coding works, not how to carry out an analysis. When you have a binary categorical variable (one with two levels), one level is coded as 0 and the other as 1. By default, R chooses alphabetically. Here is something equivalent  a hand coded variable which is 1 for white collar occupations and 0 for blue collar: just_two$type_wc &lt;- as.numeric(just_two$type == &quot;wc&quot;) Have a look, comparing the type column with type_wc: just_two ## occ prestige type type_wc ## 1 nursing aides 34.9 bc 0 ## 2 medical technicians 67.5 wc 1 ## 3 radio tv announcers 57.6 wc 1 ## 4 secretaries 46.0 wc 1 ## 5 typists 41.9 wc 1 ## 6 bookkeepers 49.4 wc 1 ## 7 tellers cashiers 42.3 wc 1 ## 8 computer operators 47.7 wc 1 ## 9 shipping clerks 30.9 wc 1 ## 10 file clerks 32.7 wc 1 ## 11 receptionsts 38.7 wc 1 ## 12 mail carriers 36.1 wc 1 ## 13 postal clerks 37.2 wc 1 ## 14 telephone operators 38.1 wc 1 ## 15 collectors 29.4 wc 1 ## 16 claim adjustors 51.1 wc 1 ## 17 travel clerks 35.7 wc 1 ## 18 office clerks 35.6 wc 1 ## 19 sales supervisors 41.5 wc 1 ## 20 commercial travellers 40.2 wc 1 ## 21 sales clerks 26.5 wc 1 ## 22 service station attendant 23.3 bc 0 ## 23 insurance agents 47.3 wc 1 ## 24 real estate salesmen 47.1 wc 1 ## 25 buyers 51.1 wc 1 ## 26 firefighters 43.5 bc 0 ## 27 policemen 51.6 bc 0 ## 28 cooks 29.7 bc 0 ## 29 bartenders 20.2 bc 0 ## 30 funeral directors 54.9 bc 0 ## 31 launderers 20.8 bc 0 ## 32 janitors 17.3 bc 0 ## 33 elevator operators 20.1 bc 0 ## 34 farm workers 21.5 bc 0 ## 35 rotary well drillers 35.3 bc 0 ## 36 bakers 38.9 bc 0 ## 37 slaughterers 1 25.2 bc 0 ## 38 slaughterers 2 34.8 bc 0 ## 39 canners 23.2 bc 0 ## 40 textile weavers 33.3 bc 0 ## 41 textile labourers 28.8 bc 0 ## 42 tool die makers 42.5 bc 0 ## 43 machinists 44.2 bc 0 ## 44 sheet metal workers 35.9 bc 0 ## 45 welders 41.8 bc 0 ## 46 auto workers 35.9 bc 0 ## 47 aircraft workers 43.7 bc 0 ## 48 electronic workers 50.8 bc 0 ## 49 radio tv repairmen 37.2 bc 0 ## 50 sewing mach operators 28.2 bc 0 ## 51 auto repairmen 38.1 bc 0 ## 52 aircraft repairmen 50.3 bc 0 ## 53 railway sectionmen 27.3 bc 0 ## 54 electrical linemen 40.9 bc 0 ## 55 electricians 50.2 bc 0 ## 56 construction foremen 51.1 bc 0 ## 57 carpenters 38.9 bc 0 ## 58 masons 36.2 bc 0 ## 59 house painters 29.9 bc 0 ## 60 plumbers 42.9 bc 0 ## 61 construction labourers 26.5 bc 0 ## 62 train engineers 48.9 bc 0 ## 63 bus drivers 35.9 bc 0 ## 64 taxi drivers 25.1 bc 0 ## 65 longshoremen 26.1 bc 0 ## 66 typesetters 42.2 bc 0 ## 67 bookbinders 35.2 bc 0 Now lets fit a regression model with this binary 0/1 variable as a predictor: the_binary &lt;- lm(prestige ~ type_wc, data = just_two) summary(the_binary) ## ## Call: ## lm(formula = prestige ~ type_wc, data = just_two) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.2273 -7.0273 -0.2273 6.8227 25.2565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.527 1.486 23.914 &lt;2e-16 *** ## type_wc 6.716 2.536 2.649 0.0101 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.855 on 65 degrees of freedom ## Multiple R-squared: 0.09742, Adjusted R-squared: 0.08353 ## F-statistic: 7.016 on 1 and 65 DF, p-value: 0.01013 Again, the intercept provides the mean prestige for blue collar and the slope for type_wc gives the mean difference between blue collar and white collar. This model is mathematically identical to the one that R built using the factor variable. A (slightly jittered) picture might help you see why this is. set.seed(45) just_two %&gt;% ggplot(aes(type_wc,prestige)) + geom_jitter(width = .02, height = 0) + geom_abline(intercept = coef(the_binary)[1], slope = coef(the_binary)[2]) The values at the left hand side, above 0, are for blue collar occupations and at the right hand size, above 1, are for white collar occupations. We asked lm to estimate an intercept (\\(\\beta_0\\)) and slope for type_wc (\\(\\beta_1\\)). \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\times \\mathtt{type\\_wc} \\] This is what it found: \\[ \\mathtt{prestige} = 35.5 + 6.7 \\times \\mathtt{type\\_wc} \\] Interpret the regression model as usual: for every unit increase in type_wc, prestige increases by 6.7. Thats the slope in the picture above. But the type_wc variable only has two possible values, so this is the simplest example of unit increase. It can be 1, in which case we get the mean of white collar: \\[ \\begin{array}{rcl} \\mathtt{prestige} &amp; = &amp; 35.5 + 6.7 \\times \\mathtt{type\\_wc}\\\\ &amp; = &amp; 35.5 + 6.7 \\times 1 \\\\ &amp; = &amp; 35.5 + 6.7 \\\\ &amp; = &amp; 42.2 \\end{array} \\] Alternatively, the predictor can be zero, in which case we get the mean of blue collar: \\[ \\begin{array}{rcl} \\mathtt{prestige} &amp; = &amp; 35.5 + 6.7 \\times \\mathtt{type\\_wc}\\\\ &amp; = &amp; 35.5 + 6.7 \\times 0 \\\\ &amp; = &amp; 35.5 + 0 \\\\ &amp; = &amp; 35.5 \\end{array} \\] By the way, this is equivalent to a two-sample t-test assuming equal variance: t.test(prestige ~ type, data = just_two, var.equal = TRUE) ## ## Two Sample t-test ## ## data: prestige by type ## t = -2.6487, df = 65, p-value = 0.01013 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.780279 -1.652132 ## sample estimates: ## mean in group bc mean in group wc ## 35.52727 42.24348 7.6.3 Categorical predictors with 3 or more levels In general, if a categorical variable has \\(n\\) levels, then there will be \\(n-1\\) binary predictors added to the regression model. One level must be chosen as the comparison level. Slopes for the predictors represent differences from that comparison level. Here is a model, fitted using Rs automatic coding (and summarised with the aid of a pipe, just to add a little variation): lm(prestige ~ type, data = dat_no_NAs) %&gt;% summary() ## ## Call: ## lm(formula = prestige ~ type, data = dat_no_NAs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.2273 -7.1773 -0.0854 6.1174 25.2565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.527 1.432 24.810 &lt; 2e-16 *** ## typeprof 32.321 2.227 14.511 &lt; 2e-16 *** ## typewc 6.716 2.444 2.748 0.00718 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.499 on 95 degrees of freedom ## Multiple R-squared: 0.6976, Adjusted R-squared: 0.6913 ## F-statistic: 109.6 on 2 and 95 DF, p-value: &lt; 2.2e-16 Now lets do the same again by hand, to show the coding: dat_no_NAs$type_prof &lt;- as.numeric(dat_no_NAs$type == &quot;prof&quot;) dat_no_NAs$type_wc &lt;- as.numeric(dat_no_NAs$type == &quot;wc&quot;) There are now two predictors: type_prof is 1 when the occupation type is prof and 0 otherwise; type_wc is 1 when the occupation type is wc and 0 otherwise. Whenever the occupation type is bc, both of these predictors are zero. Have a look to see what this has done, comparing the type column with type_prof and type_wc: dat_no_NAs ## occ prestige type type_prof type_wc ## 1 gov administrators 68.8 prof 1 0 ## 2 general managers 69.1 prof 1 0 ## 3 accountants 63.4 prof 1 0 ## 4 purchasing officers 56.8 prof 1 0 ## 5 chemists 73.5 prof 1 0 ## 6 physicists 77.6 prof 1 0 ## 7 biologists 72.6 prof 1 0 ## 8 architects 78.1 prof 1 0 ## 9 civil engineers 73.1 prof 1 0 ## 10 mining engineers 68.8 prof 1 0 ## 11 surveyors 62.0 prof 1 0 ## 12 draughtsmen 60.0 prof 1 0 ## 13 computer programers 53.8 prof 1 0 ## 14 economists 62.2 prof 1 0 ## 15 psychologists 74.9 prof 1 0 ## 16 social workers 55.1 prof 1 0 ## 17 lawyers 82.3 prof 1 0 ## 18 librarians 58.1 prof 1 0 ## 19 vocational counsellors 58.3 prof 1 0 ## 20 ministers 72.8 prof 1 0 ## 21 university teachers 84.6 prof 1 0 ## 22 primary school teachers 59.6 prof 1 0 ## 23 secondary school teachers 66.1 prof 1 0 ## 24 physicians 87.2 prof 1 0 ## 25 veterinarians 66.7 prof 1 0 ## 26 osteopaths chiropractors 68.4 prof 1 0 ## 27 nurses 64.7 prof 1 0 ## 28 nursing aides 34.9 bc 0 0 ## 29 physio therapsts 72.1 prof 1 0 ## 30 pharmacists 69.3 prof 1 0 ## 31 medical technicians 67.5 wc 0 1 ## 32 commercial artists 57.2 prof 1 0 ## 33 radio tv announcers 57.6 wc 0 1 ## 35 secretaries 46.0 wc 0 1 ## 36 typists 41.9 wc 0 1 ## 37 bookkeepers 49.4 wc 0 1 ## 38 tellers cashiers 42.3 wc 0 1 ## 39 computer operators 47.7 wc 0 1 ## 40 shipping clerks 30.9 wc 0 1 ## 41 file clerks 32.7 wc 0 1 ## 42 receptionsts 38.7 wc 0 1 ## 43 mail carriers 36.1 wc 0 1 ## 44 postal clerks 37.2 wc 0 1 ## 45 telephone operators 38.1 wc 0 1 ## 46 collectors 29.4 wc 0 1 ## 47 claim adjustors 51.1 wc 0 1 ## 48 travel clerks 35.7 wc 0 1 ## 49 office clerks 35.6 wc 0 1 ## 50 sales supervisors 41.5 wc 0 1 ## 51 commercial travellers 40.2 wc 0 1 ## 52 sales clerks 26.5 wc 0 1 ## 54 service station attendant 23.3 bc 0 0 ## 55 insurance agents 47.3 wc 0 1 ## 56 real estate salesmen 47.1 wc 0 1 ## 57 buyers 51.1 wc 0 1 ## 58 firefighters 43.5 bc 0 0 ## 59 policemen 51.6 bc 0 0 ## 60 cooks 29.7 bc 0 0 ## 61 bartenders 20.2 bc 0 0 ## 62 funeral directors 54.9 bc 0 0 ## 64 launderers 20.8 bc 0 0 ## 65 janitors 17.3 bc 0 0 ## 66 elevator operators 20.1 bc 0 0 ## 68 farm workers 21.5 bc 0 0 ## 69 rotary well drillers 35.3 bc 0 0 ## 70 bakers 38.9 bc 0 0 ## 71 slaughterers 1 25.2 bc 0 0 ## 72 slaughterers 2 34.8 bc 0 0 ## 73 canners 23.2 bc 0 0 ## 74 textile weavers 33.3 bc 0 0 ## 75 textile labourers 28.8 bc 0 0 ## 76 tool die makers 42.5 bc 0 0 ## 77 machinists 44.2 bc 0 0 ## 78 sheet metal workers 35.9 bc 0 0 ## 79 welders 41.8 bc 0 0 ## 80 auto workers 35.9 bc 0 0 ## 81 aircraft workers 43.7 bc 0 0 ## 82 electronic workers 50.8 bc 0 0 ## 83 radio tv repairmen 37.2 bc 0 0 ## 84 sewing mach operators 28.2 bc 0 0 ## 85 auto repairmen 38.1 bc 0 0 ## 86 aircraft repairmen 50.3 bc 0 0 ## 87 railway sectionmen 27.3 bc 0 0 ## 88 electrical linemen 40.9 bc 0 0 ## 89 electricians 50.2 bc 0 0 ## 90 construction foremen 51.1 bc 0 0 ## 91 carpenters 38.9 bc 0 0 ## 92 masons 36.2 bc 0 0 ## 93 house painters 29.9 bc 0 0 ## 94 plumbers 42.9 bc 0 0 ## 95 construction labourers 26.5 bc 0 0 ## 96 pilots 66.1 prof 1 0 ## 97 train engineers 48.9 bc 0 0 ## 98 bus drivers 35.9 bc 0 0 ## 99 taxi drivers 25.1 bc 0 0 ## 100 longshoremen 26.1 bc 0 0 ## 101 typesetters 42.2 bc 0 0 ## 102 bookbinders 35.2 bc 0 0 If we fit a model with these two hand coded predictors, the results are identical to what R did automatically: lm(prestige ~ type_prof + type_wc, data = dat_no_NAs) %&gt;% summary() ## ## Call: ## lm(formula = prestige ~ type_prof + type_wc, data = dat_no_NAs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.2273 -7.1773 -0.0854 6.1174 25.2565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.527 1.432 24.810 &lt; 2e-16 *** ## type_prof 32.321 2.227 14.511 &lt; 2e-16 *** ## type_wc 6.716 2.444 2.748 0.00718 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.499 on 95 degrees of freedom ## Multiple R-squared: 0.6976, Adjusted R-squared: 0.6913 ## F-statistic: 109.6 on 2 and 95 DF, p-value: &lt; 2.2e-16 Now the formula is: \\[ \\begin{array}{rcl} \\mathtt{prestige} &amp; = &amp; 35.5 + 32.3 \\times \\mathtt{type\\_prof} + 6.7 \\times \\mathtt{type\\_wc} \\end{array} \\] There are three possibilities: type_prof and type_wc are both zero, so the model computes the mean prestige for blue collar workers. \\[ \\begin{array}{rcl} \\mathtt{prestige} &amp; = &amp; 35.5 + 32.3 \\times 0 + 6.7 \\times 0\\\\ &amp; = &amp; 35.5 \\end{array} \\] type_prof is 1 and type_wc is 0, so the model computes the mean prestige for professionals. \\[ \\begin{array}{rcl} \\mathtt{prestige} &amp; = &amp; 35.5 + 32.3 \\times 1 + 6.7 \\times 0\\\\ &amp; = &amp; 35.5 + 32.3\\\\ &amp; = &amp; 67.8 \\end{array} \\] type_prof is 0 and type_wc is 1, so the model computes the mean prestige for white collar workers. \\[ \\begin{array}{rcl} \\mathtt{prestige} &amp; = &amp; 35.5 + 32.3 \\times 0 + 6.7 \\times 1\\\\ &amp; = &amp; 35.5 + 6.7\\\\ &amp; = &amp; 42.2 \\end{array} \\] 7.7 Interpreting the coefficients Heres the model again: lm(prestige ~ type, data = dat_no_NAs) %&gt;% summary() ## ## Call: ## lm(formula = prestige ~ type, data = dat_no_NAs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.2273 -7.1773 -0.0854 6.1174 25.2565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.527 1.432 24.810 &lt; 2e-16 *** ## typeprof 32.321 2.227 14.511 &lt; 2e-16 *** ## typewc 6.716 2.444 2.748 0.00718 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.499 on 95 degrees of freedom ## Multiple R-squared: 0.6976, Adjusted R-squared: 0.6913 ## F-statistic: 109.6 on 2 and 95 DF, p-value: &lt; 2.2e-16 From this we can see that the prestige for professionals is 32.3 points more than for blue collar workers (\\(t = 14.5\\), \\(p &lt; .001\\)) and prestige for white collar workers is 6.7 points more than for blue collar (\\(t = 2.7\\), \\(p = .007\\)); both differences are statistically significant. We have a variable with three levels and this has ended up the regression model as two predictors, (1) typewc and (2) typeprof, the slopes of which represent the difference between (1) wc and bc and (2) prof and bc, respectively. But this does not currently show us all combinations. What is the difference between wc and prof? Here is relevel to the rescue. It relevels a factor so that whichever level you name comes first. You can add the command directly to a model specification like this: lm(prestige ~ relevel(type, &quot;prof&quot;), data = dat_no_NAs) %&gt;% summary() ## ## Call: ## lm(formula = prestige ~ relevel(type, &quot;prof&quot;), data = dat_no_NAs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.2273 -7.1773 -0.0854 6.1174 25.2565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.848 1.706 39.770 &lt; 2e-16 *** ## relevel(type, &quot;prof&quot;)bc -32.321 2.227 -14.511 &lt; 2e-16 *** ## relevel(type, &quot;prof&quot;)wc -25.605 2.614 -9.795 4.53e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.499 on 95 degrees of freedom ## Multiple R-squared: 0.6976, Adjusted R-squared: 0.6913 ## F-statistic: 109.6 on 2 and 95 DF, p-value: &lt; 2.2e-16 Or, do it a longer but clearer way, e.g., dat_no_NAs$type_vs_prof &lt;- relevel(dat_no_NAs$type, &quot;prof&quot;) myMod &lt;- lm(prestige ~ type_vs_prof, data = dat_no_NAs) summary(myMod) ## ## Call: ## lm(formula = prestige ~ type_vs_prof, data = dat_no_NAs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.2273 -7.1773 -0.0854 6.1174 25.2565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.848 1.706 39.770 &lt; 2e-16 *** ## type_vs_profbc -32.321 2.227 -14.511 &lt; 2e-16 *** ## type_vs_profwc -25.605 2.614 -9.795 4.53e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.499 on 95 degrees of freedom ## Multiple R-squared: 0.6976, Adjusted R-squared: 0.6913 ## F-statistic: 109.6 on 2 and 95 DF, p-value: &lt; 2.2e-16 The advantage of this is that we can have a look at the levels and check that prof comes first: levels(dat_no_NAs$type_vs_prof) ## [1] &quot;prof&quot; &quot;bc&quot; &quot;wc&quot; 7.7.1 Activity Interpret the coefficients in this myMod model. 7.7.2 Answer Prestige for professionals is 32.3 points greater than for blue collar workers (\\(t = 14.5\\), \\(p &lt; .001\\)) and 25.6 points greater than for white collar workers (\\(t = 9.8\\), \\(p &lt; .001\\)). 7.8 Checking all combinations It can quickly become a pain to use relevel like this and there are various alternatives. At this point, I wish I hadnt been so clever using a pipe and instead just saved the model. I shall do so now. das_Model &lt;- lm(prestige ~ type, data = dat_no_NAs) We will use the emmeans package. library(emmeans) The main function therein is emmeans. Fox and Weisberg (2019, p. 203) comment that the formula argument is idiosyncratic. Anyway, here is how to use it: emmeans(das_Model, pairwise ~ type) ## $emmeans ## type emmean SE df lower.CL upper.CL ## bc 35.5 1.43 95 32.7 38.4 ## prof 67.8 1.71 95 64.5 71.2 ## wc 42.2 1.98 95 38.3 46.2 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## bc - prof -32.32 2.23 95 -14.511 &lt;.0001 ## bc - wc -6.72 2.44 95 -2.748 0.0195 ## prof - wc 25.60 2.61 95 9.795 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates The first parameter is the name of the model, the second is a formula specifying which comparisons are required. The idiosyncracy in the model specification is that pairwise is not a variable in the data  its a type of comparison. Compare and contrast with the results obtained by using relevel above. The estimates are identical. One difference you will observe is that the p-values are adjusted for the number of comparisons. There has been an enormous quantity of ink spilled on the problem of multiple comparisons. You may recall a variety of corrections, often named after their creators, e.g., Bonferroni, Bonferroni-Holm (my favourite), and idák. Here, the p-values are adjusted by tukey method which is Tukeys Honest Significant Differences (HSD) adjustment. (Tukey is another statistician.) 7.9 The intercept is not always the mean of the comparison group The intercept always works the same way: it is the mean of the outcome variable when the predictors are zero. In the examples we have looked at, this was the mean of the comparison group  whatever happened to be the first level of the factor. Lets try another model: more_data &lt;- dat %&gt;% select(occ, prestige, type, income) %&gt;% na.omit() type_and_income &lt;- lm(prestige ~ I(income/1000) + type, data = more_data) summary(type_and_income) ## ## Call: ## lm(formula = prestige ~ I(income/1000) + type, data = more_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.2141 -6.7359 -0.1931 5.1725 25.0776 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.9971 1.8008 15.547 &lt; 2e-16 *** ## I(income/1000) 1.4012 0.2434 5.757 1.07e-07 *** ## typeprof 25.0555 2.3020 10.884 &lt; 2e-16 *** ## typewc 7.1672 2.1140 3.390 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.21 on 94 degrees of freedom ## Multiple R-squared: 0.7765, Adjusted R-squared: 0.7693 ## F-statistic: 108.8 on 3 and 94 DF, p-value: &lt; 2.2e-16 Now the intercept is not the mean prestige for blue collar workers, but rather it is predicted mean prestige for blue collar workers when income is zero. By the way, we can use emmeans again to compare occupation types, holding income constant: emmeans(type_and_income, pairwise ~ type) ## $emmeans ## type emmean SE df lower.CL upper.CL ## bc 37.7 1.30 94 35.1 40.3 ## prof 62.8 1.72 94 59.4 66.2 ## wc 44.9 1.77 94 41.4 48.4 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## bc - prof -25.06 2.30 94 -10.884 &lt;.0001 ## bc - wc -7.17 2.11 94 -3.390 0.0029 ## prof - wc 17.89 2.63 94 6.809 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates 7.10 Recap Okay, we have covered rather a lot of ground there so let me summarise a swift method for testing whether a categorical predictor explains variance in some outcome. Ill use an example where we also statistically adjust for another predictor (known as a covariate). First, do whatever you need to do to obtain and tidy the data: more_data &lt;- dat %&gt;% select(occ, prestige, type, income) %&gt;% na.omit() Fit the model of interest: my_model &lt;- lm(prestige ~ I(income/1000) + type, data = more_data) Use Anova to see which predictors are statistically significant Anova(my_model) ## Anova Table (Type II tests) ## ## Response: prestige ## Sum Sq Df F value Pr(&gt;F) ## I(income/1000) 2234.5 1 33.147 1.068e-07 *** ## type 7988.5 2 59.251 &lt; 2.2e-16 *** ## Residuals 6336.7 94 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Look at a summary: summary(my_model) ## ## Call: ## lm(formula = prestige ~ I(income/1000) + type, data = more_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.2141 -6.7359 -0.1931 5.1725 25.0776 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.9971 1.8008 15.547 &lt; 2e-16 *** ## I(income/1000) 1.4012 0.2434 5.757 1.07e-07 *** ## typeprof 25.0555 2.3020 10.884 &lt; 2e-16 *** ## typewc 7.1672 2.1140 3.390 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.21 on 94 degrees of freedom ## Multiple R-squared: 0.7765, Adjusted R-squared: 0.7693 ## F-statistic: 108.8 on 3 and 94 DF, p-value: &lt; 2.2e-16 Use emmeans to check for any comparisons of interest which were not included in the way the factor was initially setup: emmeans(my_model, pairwise ~ type) ## $emmeans ## type emmean SE df lower.CL upper.CL ## bc 37.7 1.30 94 35.1 40.3 ## prof 62.8 1.72 94 59.4 66.2 ## wc 44.9 1.77 94 41.4 48.4 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## bc - prof -25.06 2.30 94 -10.884 &lt;.0001 ## bc - wc -7.17 2.11 94 -3.390 0.0029 ## prof - wc 17.89 2.63 94 6.809 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates Dont forget to check model diagnostics 7.11 Challenge 7.11.1 Activity Fit a regression model with income as the outcome variable and occupation type as a predictor. Does type explain variance in outcome? Is there a statistically significant difference between blue collar and white collar salaries? Is there a statistically significant difference between white collar and professional salaries? Are the residuals normally distributed? Try logging income to see what impact it has on the residual distribution Now compare all differences between groups 7.11.2 Answers a. Fit a regression model with income as the outcome variable and occupation type as a predictor. Does type explain variance in outcome? income_mod &lt;- lm(income ~ type, data = dat) Anova(income_mod) ## Anova Table (Type II tests) ## ## Response: income ## Sum Sq Df F value Pr(&gt;F) ## type 595956156 2 24.872 2.057e-09 *** ## Residuals 1138123006 95 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes: \\(F(2,95) = 24.87\\), \\(p &lt; .001\\) b. Is there a statistically significant difference between blue collar and white collar salaries? First, lets have a look at the means, to help aid interpretation: dat %&gt;% group_by(type) %&gt;% summarise(mean_income = mean(income)) ## # A tibble: 4 x 2 ## type mean_income ## &lt;fct&gt; &lt;dbl&gt; ## 1 bc 5374. ## 2 prof 10559. ## 3 wc 5052. ## 4 &lt;NA&gt; 3344. The means for blue collar and white collar look very similar, about $300 difference. Lets test that difference: summary(income_mod) ## ## Call: ## lm(formula = income ~ type, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5945.5 -2003.3 -466.2 1536.9 15319.5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5374.1 521.8 10.299 &lt; 2e-16 *** ## typeprof 5185.3 811.6 6.389 6.16e-09 *** ## typewc -321.8 890.6 -0.361 0.719 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3461 on 95 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.3437, Adjusted R-squared: 0.3299 ## F-statistic: 24.87 on 2 and 95 DF, p-value: 2.057e-09 There is no statistically significant difference, \\(t = 0.361\\), \\(p = .71\\). c. Is there a statistically significant difference between white collar and professional salaries? We can refit the model using relevel like so: income_mod_wc &lt;- lm(income ~ relevel(type, &quot;wc&quot;), data = dat) summary(income_mod_wc) ## ## Call: ## lm(formula = income ~ relevel(type, &quot;wc&quot;), data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5945.5 -2003.3 -466.2 1536.9 15319.5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5052.3 721.7 7.000 3.6e-10 *** ## relevel(type, &quot;wc&quot;)bc 321.8 890.6 0.361 0.719 ## relevel(type, &quot;wc&quot;)prof 5507.1 952.5 5.782 9.4e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3461 on 95 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.3437, Adjusted R-squared: 0.3299 ## F-statistic: 24.87 on 2 and 95 DF, p-value: 2.057e-09 Yes, professional salaries are a mean $5507 more than white collar salaries (\\(t = 5.78\\), \\(p &lt; .001\\)). d. Are the residuals normally distributed? The order of levels does not affect the residuals, so either model would do: qqPlot(resid(income_mod_wc)) ## [1] 2 24 Hmmmm something fishy for residuals above 5000. Lets look at the histogram in case it is more obvious: hist(resid(income_mod_wc)) Looks like they are not normally distributed, though maybe driven by outliers? e. Try logging income to see what impact it has on the residual distribution dat$log_income &lt;- log(dat$income) log_income_mod &lt;- lm(log_income ~ type, data = dat) Now the residuals look much better: qqPlot(resid(log_income_mod)) ## 68 2 ## 64 2 f. Now compare all differences between groups Lets use emmeans: emmeans(log_income_mod, pairwise ~ type) ## $emmeans ## type emmean SE df lower.CL upper.CL ## bc 8.51 0.0638 95 8.39 8.64 ## prof 9.16 0.0760 95 9.01 9.31 ## wc 8.46 0.0882 95 8.28 8.63 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## bc - prof -0.6452 0.0992 95 -6.507 &lt;.0001 ## bc - wc 0.0562 0.1088 95 0.516 0.8636 ## prof - wc 0.7014 0.1164 95 6.027 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates Note that the estimates are on the log scale, e.g., the difference between professional and white collar salaries is 0.7 on the logged $1000s scale. 7.12 Brief introduction to interactions Fox and Weisberg (2019, pp. 207-224) say lots about interactions. In this tutorial I just want to mention briefly what they are and how to ask R to model them. 7.12.1 What is an interaction? An interaction between two variables simply means that the relationship between the outcome and one of the variables is changed (or moderated) by the other variable. For example, we found that occupation type explains variation in prestige. It is possible that the magnitude of differences between occupation types depend on mean income. Interactions can also be used to test for intersectional effects. Discrimination may, for instance, depend on sexism and racism  both operate independently. However, additionally the differences between men and women on some variable like salary might also depend on ethnicity and/or on whether you are cisgender or transgender. Arithmetically, an interaction between two variables, A and B is represented as a new variable which is A and B multiplied together. R does the coding for us. 7.12.2 How to test for interactions in R This shows how to test for an interaction between income and occupation type. Lets select some data again: dat_for_int &lt;- dat %&gt;% select(occ, prestige, type, income) %&gt;% na.omit() First, predict prestige from income (in thousands) and occupation type. This model has the so-called main effects, i.e., without any interaction. dat$income_1000s &lt;- dat$income/1000 main_effect_mod &lt;- lm(prestige ~ income_1000s + type, data = dat) summary(main_effect_mod) ## ## Call: ## lm(formula = prestige ~ income_1000s + type, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.2141 -6.7359 -0.1931 5.1725 25.0776 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.9971 1.8008 15.547 &lt; 2e-16 *** ## income_1000s 1.4012 0.2434 5.757 1.07e-07 *** ## typeprof 25.0555 2.3020 10.884 &lt; 2e-16 *** ## typewc 7.1672 2.1140 3.390 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.21 on 94 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.7765, Adjusted R-squared: 0.7693 ## F-statistic: 108.8 on 3 and 94 DF, p-value: &lt; 2.2e-16 We can then use Anova as before: Anova(main_effect_mod) ## Anova Table (Type II tests) ## ## Response: prestige ## Sum Sq Df F value Pr(&gt;F) ## income_1000s 2234.5 1 33.147 1.068e-07 *** ## type 7988.5 2 59.251 &lt; 2.2e-16 *** ## Residuals 6336.7 94 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are effects for both income (\\(F(1,94) = 33.1\\), \\(p &lt; .001\\)) and occupation type (\\(F(2,94) = 59.3\\), \\(p &lt; .001\\)). Now, do differences between occupation types depend on income? We can check as follows: interaction_mod &lt;- lm(prestige ~ income_1000s + type + income_1000s:type, data = dat) anova(main_effect_mod, interaction_mod) ## Analysis of Variance Table ## ## Model 1: prestige ~ income_1000s + type ## Model 2: prestige ~ income_1000s + type + income_1000s:type ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 94 6336.7 ## 2 92 4859.2 2 1477.5 13.987 4.969e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And the answer is, yes; \\(F(2,92) = 14.0\\), \\(p &lt; .001\\). It is also possible to check both main effects (without interactions) and the interaction effect in one go: Anova(interaction_mod) ## Anova Table (Type II tests) ## ## Response: prestige ## Sum Sq Df F value Pr(&gt;F) ## income_1000s 2234.5 1 42.306 3.999e-09 *** ## type 7988.5 2 75.623 &lt; 2.2e-16 *** ## income_1000s:type 1477.5 2 13.987 4.969e-06 *** ## Residuals 4859.2 92 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The line beginning income_1000s:type shows the test of the interaction. To find out more about the Anova command and the Type II sum of squares it calculates by default, see Fox and Weisberg (2019, p. 262-264). 7.12.3 Understanding interactions The most challenging aspect of interactions is that one can rapidly end up with enormously complex models which are difficult to interpret, even if everything is statistically significant. The secret to making sense of them is to plot model predictions. We will explore the ggeffects package  one easy way to do this  in more depth when examining logistic regression, but here is a preview of how it helps: library(ggeffects) ggpredict(interaction_mod, terms = c(&quot;income_1000s&quot;, &quot;type&quot;)) %&gt;% plot(add.data = TRUE, ci = FALSE) Hopefully this graph illustrates what it means that the relationship between income and prestige depends on occupation type  the slopes are different for each type. We can also see what the model predictions look like without the interaction term: library(ggeffects) ggpredict(main_effect_mod, terms = c(&quot;income_1000s&quot;, &quot;type&quot;)) %&gt;% plot(add.data = TRUE, ci = FALSE) 7.12.4 Further reading You might be interested in Johnson-Neyman intervals. See the interactions package. "],["logistic-regression.html", "Chapter 8 Logistic regression 8.1 Setup 8.2 The dataset 8.3 Warmup activity 8.4 The punchline 8.5 Intermezzo: parametric versus nonparametric 8.6 What is a generalised linear model? 8.7 What is the log function again? 8.8 Intercept-only models again 8.9 Odds and log odds 8.10 Back to that intercept 8.11 Interpreting model slopes 8.12 Diagnostics 8.13 A challenge", " Chapter 8 Logistic regression By the end of this chapter you will understand: How generalised linear models (GLMs) generalise regression to deal with a wider variety of distributions of data. How to fit a GLM to binary data (also known as logistic regression). How to interpret the results. Which diagnostics from linear models do and dont apply here. Much herein will be familiar from what we covered for linear regression. The main challenge is that the outcome variable is on the log-odds scale, which makes it trickier to interpret the model intercept and slopes. I provide four different ways to make sense of them, so you can choose which makes most sense to you. This short video might be helpful too. 8.1 Setup Lets load the tidyverse. library(tidyverse) Im also going to use knitr to make tables look prettier: library(knitr) 8.2 The dataset We will look at a different dataset in this chapter (HURRAH; download it here), as analysed by Mroz (1987) and spotted in Fox and Weisberg (2019). The data are from the US Panel Study of Income Dynamics (PSID) in 1975: 753 married white women aged between 30 and 60. The outcome variable we will examine is whether participants participated in the labour force. So it will be worth thinking through your (implicit or otherwise) theories of what processes might be operating in 1975. We would expect a more diverse sample these days. You might also ponder what extra information you would want to know and why. For what its worth, Im curious what both wife and husband thought about gender roles and what sort of childcare was available. Im not sure if its relevant, but I am also curious to know how many participants were bisexual (we cant tell, but it is often wrongly assumed that people in a heterosexual marriage are both heterosexual). Variable name Description lfp labour-force participation at some point during the year (no/yes). lfp_yes same as above, but with yes = 1, no = 0 k5 number of children 5 years old or younger. k618 number of children 6 to 18 years old. age in years. wc wifes college attendance (no/yes). hc husbands college attendance (no/yes). lwg log expected wage rate; for women in the labour force, the actual wage rate; for women not in the labour force, an imputed value based on the regression of lwg on the other variables. inc family income exclusive of wifes income (in $1000s). Read in the data: dat &lt;- read.csv(&quot;psid1975.csv&quot;) Here is how to view a random sample of rows. (The set.seed command is optional  I am just using it so that I keep getting the same random sample. If youre curious, Wikipedias article is currently fine.) set.seed(3) dat %&gt;% slice_sample(n = 10) ## lfp lfp_yes k5 k618 age wc hc lwg inc ## 1 no 0 0 2 43 no no 1.2282799 14.550 ## 2 no 0 1 1 30 no yes 0.9791328 22.000 ## 3 no 0 0 0 53 no no 0.8861951 21.574 ## 4 no 0 0 3 39 no no 0.8532125 28.363 ## 5 yes 1 0 2 35 yes no 1.6046872 5.120 ## 6 yes 1 0 2 36 yes yes 1.7037485 23.600 ## 7 yes 1 0 2 48 no no 1.5448993 9.000 ## 8 no 0 0 0 50 yes yes 1.7312460 63.200 ## 9 no 0 1 0 30 no yes 1.0167637 19.392 ## 10 yes 1 0 1 48 yes yes 1.6549002 70.750 The lfp_yes variable was calculated using the line below. This might come in handy for your analyses should you wish to dichotomise a variable. as.numeric(dat$lfp == &quot;yes&quot;) 8.3 Warmup activity 8.3.1 Activity What do you think might predict participation in the labour force? Have a think before looking at the descriptives and write down your thoughts. Set wc and hc to factors. Generate a table of descriptive statistics (Table 1) using a method of your choosing. 8.3.2 Answer a. What do you think might predict participation in the labour force? Have a think before looking at the descriptives. It was too late for me since I had already peeked and can only report my hindsight bias! b. Set wc and hc to factors. dat$wc &lt;- factor(dat$wc) dat$hc &lt;- factor(dat$hc) c. Generate a table of descriptive statistics (Table 1) using a method of your choosing. Here is one we havent used yet, in the table1 package. Here is how to use it with a pipe; recall that the dot in data = . is how to refer to the data frame that flowed in via the pipe from the select line above: library(table1) dat %&gt;% select(-lfp_yes) %&gt;% table1(~ ., data = .) Overall(N=753) lfp no 325 (43.2%) yes 428 (56.8%) k5 Mean (SD) 0.238 (0.524) Median [Min, Max] 0 [0, 3.00] k618 Mean (SD) 1.35 (1.32) Median [Min, Max] 1.00 [0, 8.00] age Mean (SD) 42.5 (8.07) Median [Min, Max] 43.0 [30.0, 60.0] wc no 541 (71.8%) yes 212 (28.2%) hc no 458 (60.8%) yes 295 (39.2%) lwg Mean (SD) 1.10 (0.588) Median [Min, Max] 1.07 [-2.05, 3.22] inc Mean (SD) 20.1 (11.6) Median [Min, Max] 17.7 [-0.0290, 96.0] It is straightforward to generate descriptives by group. Since we will be exploring predictors of labour-force participation, it makes sense to group the analyses by whether the participant has worked or not during the past year: table1(~ k5 + k618 + age + inc | lfp, data = dat) no(N=325) yes(N=428) Overall(N=753) k5 Mean (SD) 0.366 (0.637) 0.140 (0.392) 0.238 (0.524) Median [Min, Max] 0 [0, 3.00] 0 [0, 2.00] 0 [0, 3.00] k618 Mean (SD) 1.36 (1.33) 1.35 (1.32) 1.35 (1.32) Median [Min, Max] 1.00 [0, 7.00] 1.00 [0, 8.00] 1.00 [0, 8.00] age Mean (SD) 43.3 (8.47) 42.0 (7.72) 42.5 (8.07) Median [Min, Max] 44.0 [30.0, 60.0] 42.0 [30.0, 60.0] 43.0 [30.0, 60.0] inc Mean (SD) 21.7 (12.7) 18.9 (10.6) 20.1 (11.6) Median [Min, Max] 19.0 [1.50, 96.0] 17.1 [-0.0290, 91.0] 17.7 [-0.0290, 96.0] And now you have looked - you can no longer hypothesise what might be going on! Heres a picture that took too long, but shows whats possible. It was built up part-by-part, so to understand it you probably need to devote some time to disassembling it. The labels on each data point are the ns, so you can see that the pattern beyond 6 children is due to individual households. I also explicitly named the package dplyr for the select and recode commands because the same names are used for functions in the car package. dat %&gt;% dplyr::select(k5, k618, lfp_yes) %&gt;% pivot_longer(names_to = &quot;children_age&quot;, values_to = &quot;children_num&quot;, cols = !lfp_yes) %&gt;% mutate(children_age = dplyr::recode(children_age, k5 = &quot;0 to 5&quot;, k618 = &quot;6 to 18&quot;)) %&gt;% group_by(children_age, children_num) %&gt;% summarise(perc_lfp = mean(lfp_yes)*100, n = n()) %&gt;% ggplot(aes(x = children_num, y = perc_lfp, colour = children_age, label = n)) + geom_point() + geom_line(size = 1) + geom_text(colour = &quot;black&quot;, nudge_y = 1, nudge_x = 0.1, hjust = 0, size = 3) + xlab(&quot;Number of children&quot;) + ylab(&quot;% women in paid employment&quot;) + labs(colour = &quot;Children age&quot;) ## `summarise()` regrouping output by &#39;children_age&#39; (override with `.groups` argument) 8.4 The punchline We will spend some time going into the arithmetic of logistic regression model coefficients since it helps interpret what the models mean. So that you have a sense of where this is heading, here is how to fit a model predicting whether women were in paid employment from the number of children in the family aged 5 or younger (k5) and 6 to 18 (k618): mod_kids &lt;- glm(lfp_yes ~ k5 + k618, data = dat, family = binomial) This is the intercept-only model: mod_0 &lt;- glm(lfp_yes ~ 1, data = dat, family = binomial) And this command compares the two: anova(mod_0, mod_kids, test = &quot;Chi&quot;) To get a summary use: summary(mod_kids) Here are confidence intervals: Confint(mod_kids) The R code involved is almost identical to what we used for linear regression. The main challenge will be interpreting the slopes, but I will share some tips to make this relatively painless. 8.5 Intermezzo: parametric versus nonparametric The term nonparametric may have some historical significance and meaning for theoretical statisticians, but it only serves to confuse applied statisticians. G. E. Noether (1984) Many introductory social science statistics courses explain that if your data are normally distributed, then you use parametric statistics, otherwise you use nonparametric statistics. Or that the data can be parametric or nonparametric. Both claims are false. The parametric versus nonparametric distinction refers to statistical models, not the data. It simply refers to whether the distributions involved are assumed to have a known mathematical form. If a model assumes a particular distribution, then it is parametric (Wolfowitz, 1942). We have already seen that you can check (to some extent) whether model assumptions are satisfied. You can think of parameters as being like knobs on an old radio. Tuning the knobs changes the parameters. The normal distribution has two parameters, mean and standard deviation: We will be exploring a parametric approach to modelling binary data (whether or not someone was in paid employment) in this chapter, using a special case of the binomial distribution with size = 1. There is one parameter: the probability, and values are either 0 or 1. This distribution is somewhat easier to grasp than the curve of a normal distribution, though we will encounter a logistic-al irritation in using it in practice. 8.6 What is a generalised linear model? As the name suggests, generalised linear models generalise the linear model. (Not to be confused with a general linear model  different thing.) They are initialised as GLM, which is pronounced glim. GLMs have three components (see Fox and Weisberg, 2019, pp. 272-275 for more detail): A specification of the error distribution. For linear models, this is a normal distribution. A linear formula linking outcome to predictors and the \\(\\beta\\)s to be estimated: \\[ g(y) = \\beta_0 + \\beta_1 x_1 + \\beta_1x_2 + \\cdots + \\beta_n x_n \\] The right hand side is identical to linear models. The left hand side, \\(g(y)\\), is mildly different to the usual \\(y\\) and is there to ensure the maths works for different shapes of distribution. GLMs also have a link function  thats the \\(g\\). For linear models, \\(g(y) = y\\), what is known as the identity link  it just passes the input unchanged to the output. For logistic regression, \\(g\\) is log-odds, also known as the logit. The formula then is \\[ \\log\\frac{p}{1-p}= \\beta_0 + \\beta_1 x_1 + \\beta_1x_2 + \\cdots + \\beta_n x_n \\] where \\(p\\) is the probability of an event happening (e.g., being in work). So, a linear regression model is a generalised linear model with a normal error distribution and an identity link. Logistic regression is a generalised linear model with a binomial error distribution and a log-odds (logit) link. The error distribution and link are together called the family. By default, glm chooses a sensible link for you so in practice you just have to name the error distribution. 8.7 What is the log function again? To interpret logistic regression models we need to understand log-odds and to understand log-odds we need to grasp logs. Im aware that it may be some time since you last used them! There are two aspects to revise: the arithmetic and why logs are typically used. 8.7.1 The arithmetic The arithmetic is easy: the log is simply the inverse of to the power of or exponentiation. Here is an example of exponentiation: \\[ 10^n = \\underbrace{10 \\times \\dots \\times 10}_{\\mathrm{n\\ times}} \\] So \\(10^2 = 100\\) and \\(10^4 = 10000\\) and so on. More generally, with \\(b^n\\), the \\(b\\) is called the base and \\(n\\) is the exponent. Log just reverses this. Suppose with want to find out what to raise 10 to the power of to get 1000, then we just calculate \\[ \\log_{10}(100) \\] In R: log(100, base = 10) ## [1] 2 So \\({\\color{magenta} {10}}^{\\color{blue} 2} = 100\\) and \\(\\log_{\\color{magenta} {10}}(100) = \\color{blue} 2\\). Lets try another base, 2. Two to the power of five is 2^5 ## [1] 32  32. 8.7.1.1 Activity Two to the power of what is 1024? 8.7.1.2 Answer log(1024, base = 2) ## [1] 10 Its 10. Lets check. 2^10 ## [1] 1024 So much for the arithmetic; onto some intutions for why it is used 8.7.2 Why log? Sadly, these days we are very aware of exponential and log functions since they are used when communicating data on the Covid-19 pandemic. Suppose some quantity doubles over time; lets take as an example the number of cat GIFs in my saved images over time: doubles &lt;- tibble(month = 0:9, cat_GIFs = 2^month) doubles %&gt;% kable(col.names = c(&quot;Month&quot;, &quot;Number of cat GIFs&quot;)) Month Number of cat GIFs 0 1 1 2 2 4 3 8 4 16 5 32 6 64 7 128 8 256 9 512 I started off with one GIF. As each month passes, the number of GIFs doubles. To find the number of GIFs at a particular time, use \\(2^{\\mathit{month}}\\). Here is 5 months into my stash: 2^5 ## [1] 32 Here is a picture of GIF growth: doubles %&gt;% ggplot(aes(month, cat_GIFs)) + geom_point() + labs(x = &quot;Month&quot;, y = &quot;Number of cat GIFs&quot;) + scale_x_continuous(breaks = seq(0,9,2)) + geom_function(fun = function(x) 2^x, n = 400) As we saw above, we can also run \\(2^\\mathit{month}\\) in reverse. Given a particular number of cat GIFs, say 32, how many times did the original number double to get there? Simply calculate \\(\\log_2(32)\\): log(32, base = 2) ## [1] 5 Often it is easier to see exponential relationships and any changes in relationships over time if they are plotted on a straight line. A way to straighten out exponential growth is to log the y-axis whilst keeping the original values at each tick point on the axis: doubles %&gt;% ggplot(aes(month, cat_GIFs)) + geom_point() + coord_trans(y = &quot;log2&quot;) + # this line transforms the axis labs(x = &quot;Month&quot;, y = &quot;Number of cat GIFs&quot;) + scale_x_continuous(breaks = seq(0,9,2)) + scale_y_continuous(breaks = 2^seq(0,9,1)) + # this scale_y line says where to place the labels geom_function(fun = function(x) 2^x, n = 400) We can also log the values, so that the y-axis now says how many times the number of GIFs has doubled. doubles %&gt;% ggplot(aes(month, log(cat_GIFs,2))) + geom_point() + labs(x = &quot;Month&quot;, y = &quot;Logged number of cat GIFs&quot;) + scale_y_continuous(breaks = seq(0,10,2)) + scale_x_continuous(breaks = seq(0,9,2)) + geom_function(fun = function(x) x, n = 400) I hope that provides some intuitions for what the log function does. Roughly, it transforms a process that multiplies, and can be hard to understand on the original scale, into a process that is additive. 8.8 Intercept-only models again For linear regression, the intercept in an intercept-only model was the mean of the the outcome variable. Intercepts in GLMs work the same; however, now we have the mild complication of the link function which is part of the arithmetic that makes GLMs work. The mean of the binary variable, lfp_yes, is: mean(dat$lfp_yes) ## [1] 0.5683931 This is the proportion of women who were in paid employment (do you see why the mean is a proportion here?). table(dat$lfp) ## ## no yes ## 325 428 Now lets see what logistic regression gives us: mod_intercept &lt;- glm(lfp_yes ~ 1, data = dat, family = binomial) summary(mod_intercept) ## ## Call: ## glm(formula = lfp_yes ~ 1, family = binomial, data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.296 -1.296 1.063 1.063 1.063 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.27530 0.07358 3.742 0.000183 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.7 on 752 degrees of freedom ## Residual deviance: 1029.7 on 752 degrees of freedom ## AIC: 1031.7 ## ## Number of Fisher Scoring iterations: 4 Most of that will look very familiar  the coefficients, standard error, p-values (though now of a \\(z\\)-value rather than \\(t\\))  however, the intercept is not the mean of lfp_yes. coef(mod_intercept) ## (Intercept) ## 0.275298 This is actually the log-odds of being in paid work, which is arithmetically related to the probability. So onto the next bit of arithmetic 8.9 Odds and log odds Odds are alternatives to probabilities for quantifying uncertainty. The probability of getting a 20 on one roll of a fair 20-sided die are \\(\\frac{1}{20}\\) or 0.05. The odds are 1 to 19 or \\(\\frac{1}{19}\\). There are 20 possible outcomes, each equally likely. We are interested in the odds of one of those outcomes. The odds are the ratio of the probability that an event will happen to the probability that it will not. For the 20-sided die case, since every outcome is equally likely, we can just use the number of outcomes directly in the formula. More generally, if the probability of an event is \\(p\\), then the odds are \\[ \\mathit{odds} = \\frac{p}{1-p} \\] If the probability of an event happening is \\(p\\), then \\(1-p\\) is the probability that it will not happen, hence the bottom line of that fraction. If you do some arithmetic (or Google) you will see that it is possible to go in reverse: \\[ p = \\frac{\\mathit{odds}}{1+\\mathit{odds}} \\] The log odds are the log of the odds, usually the natural log, i.e., log base \\(e = 2.718282\\ldots\\), which is what R provides by default. This can be written \\(\\log_e\\) or \\(\\ln\\). So the log odds of getting a 20 on one throw of a 20-sided die are: log(1/19) ## [1] -2.944439 Personally, I dont find \\(-2.94\\) a particularly meaningful number! And actually, later I will share some tricks so that you dont have to even think about what it means. However, I think it is important to get a sense of some example landmark odds and log odds along a range of probabilities. Fifteen minutes spent staring at this table will come in handy (Inf is infinity). Probability Odds Log odds (logit) 0.0 0.00 -Inf 0.1 0.11 -2.20 0.2 0.25 -1.39 0.3 0.43 -0.85 0.4 0.67 -0.41 0.5 1.00 0.00 0.6 1.50 0.41 0.7 2.33 0.85 0.8 4.00 1.39 0.9 9.00 2.20 1.0 Inf Inf Here are clues for where to look: Look at what the odds and log odds are when the probability is 0.5. Look at probabilities 0 and 1. Look for symmetry in the log odds above and below probability 0.5. The odds are also symmetric around 0.5, though this is a (little) more difficult to see. Try 0.2 and 0.8 probability. The odds of a 0.2 probability are \\[ \\frac{0.2}{1 - 0.2} = \\frac{0.2}{0.8} = \\frac{2}{8} = \\frac{1}{4} = 0.25 \\] The odds of a 0.8 probability are \\[ \\frac{0.8}{1 - 0.8} = \\frac{0.8}{0.2} = \\frac{8}{2} = \\frac{4}{1} = 4 \\] So the fraction just turns upside down. Finally, here is a picture showing the relationship between log-odds and probability: ggplot() + xlim(-6, 6) + geom_hline(yintercept = 0.5, colour = &quot;darkgrey&quot;) + geom_function(fun = plogis, n = 200) + ylab(&quot;Probability&quot;) + xlab(&quot;Log-odds (logit)&quot;) Note how the relationship is mostly non-linear, except for a patch in the middle where it is linear. The log-odds stretch out to negative and plus infinity, whereas probabilities are bound between 0 and 1. The probability for a log-odds of 6 is 0.9975274. The probability for a log-odds of 15, far off the scale on this graph, is 0.9999939. It never quite reaches 1 or, in the opposite direction, 0. To summarise then, probability, odds, and log-odds are different and interchangeable ways to quantify how sure you are that something is going to happen. 8.10 Back to that intercept We know the proportion of women in paid work: mean(dat$lfp_yes) ## [1] 0.5683931 Assuming a random sample, this is also an estimate of the probability that someone chosen at random from the larger population will be in paid work. Lets then calculate the odds: the_odds &lt;- mean(dat$lfp_yes) / (1-mean(dat$lfp_yes)) the_odds ## [1] 1.316923 Now the log-odds: log(the_odds) ## [1] 0.275298 Back to the model coefficient: coef(mod_intercept) ## (Intercept) ## 0.275298 HURRAH! We could also go in reverse from the intercept. The function exp(x) calculates \\(e^x\\) which is the inverse of \\(\\log_e\\). This gives us the odds again: the_odds_again &lt;- exp(coef(mod_intercept)) the_odds_again ## (Intercept) ## 1.316923 Now to get the probability: the_odds_again / (1+the_odds_again) ## (Intercept) ## 0.5683931 \\[ \\mathit{HURRAH}^\\mathit{HURRAH}! \\] Now we have got the hang of intercept-only models, onto slopes 8.11 Interpreting model slopes Look again at the graph: Our hindsight hypothesis (having looked at the picture)  not a good way to do science, but fine for pedagogy  is that the number of children aged 5 or under will predict labour-force participation, but the number aged 6 or over will be irrelevant. mod_children &lt;- glm(lfp_yes ~ k5 + k618, data = dat, family = binomial) summary(mod_children) ## ## Call: ## glm(formula = lfp_yes ~ k5 + k618, family = binomial, data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4566 -1.3712 0.9634 0.9953 1.7617 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.44479 0.11106 4.005 6.20e-05 *** ## k5 -0.87926 0.15817 -5.559 2.71e-08 *** ## k618 0.02730 0.05767 0.473 0.636 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.75 on 752 degrees of freedom ## Residual deviance: 994.53 on 750 degrees of freedom ## AIC: 1000.5 ## ## Number of Fisher Scoring iterations: 4 And indeed that is what we get. But how should we interpret those coefficients? Here is a suite of options. Any one of these would and has been publishable (see if you can spot them in the literature), but they are not equally interpretable. 8.11.1 Interpret on the log-odds scale This summary is a linear model on the log-odds (i.e., logit) scale. So we can interpret as before, just always ensuring we remember that the outcome units are log-odds rather than probabilities. 8.11.1.1 Activity Have a go at interpreting the intercept and slopes, focussing on the direction of effects and whether they are statistically significant. 8.11.1.2 Answer The intercept is actually interpretable (for a change!). This gives the log-odds of being in work when a family has zero children: 0.44. For every extra child aged 0-5, the log odds of being in work decrease by 0.88, \\(z = -5.6\\), \\(p &lt; .001\\). There is no statistically significant effect for children aged 6 or over; slope = 0.03, \\(z = 0.5\\), \\(p = 0.64\\). 8.11.2 Interpret using the divide-by-4 approximation Two of my favourite statisticians, Andrew Gelman and Jennifer Hill (2007, p. 82), provide a handy approximation for transforming slopes on the log-odds scale to changes in probability: As a rule of convenience, we can take logistic regression coefficients (other than the constant term) and divide them by 4 to get an upper bound of the predictive difference corresponding to a unit difference in x. This upper bound is a reasonable approximation near the midpoint of the logistic curve, where probabilities are close to 0.5. (The constant term is another word for the intercept.) Here is the rule in a picture: Where the the dashed purple line overlaps the curve, the probability \\(p = 0.5 + \\frac{\\mathit{logit}}{4}\\). 8.11.2.1 Activity Can you decode that paragraph from Gelman and Hill and use it to interpret the coefficients? coef(mod_children) ## (Intercept) k5 k618 ## 0.44478849 -0.87925889 0.02729833 8.11.2.2 Answer We can use the coefficients: coef(mod_children) ## (Intercept) k5 k618 ## 0.44478849 -0.87925889 0.02729833 Drop the intercept (or you could just do so by not looking at it!): coef(mod_children)[-1] ## k5 k618 ## -0.87925889 0.02729833 Divide by 4 and round (again, you could do this in other ways): (coef(mod_children)[-1] / 4) %&gt;% round(2) ## k5 k618 ## -0.22 0.01 For every extra child aged 5 or under, the probability of being work in decreases by (at most) 0.22. The effect for children aged 6-18 was not statistically significant; moreover, the sample estimate is minuscule: every extra child adds 0.01 to the probability of being in work. 8.11.3 Interpret using odds There is an easy way to transform the predictors so that interpretation are on the odds scale, which are more interpretable than log-odds. Lets explore why it works  skip to the So What? section on a first read if you wish. 8.11.3.1 Some arithmetic Here is the model formula again: \\[ \\log_e\\frac{p}{1-p}= \\beta_0 + \\beta_1 x_1 + \\beta_1x_2 + \\cdots + \\beta_n x_n \\] The left hand side gives the log odds and the right hand side is the linear formula we know and love from linear regression. If we exponentiate both sides, this removes the log on the left and, er, complicates the right: \\[ \\frac{p}{1-p}= e^{\\beta_0 + \\beta_1 x_1 + \\beta_1x_2 + \\cdots + \\beta_n x_n} \\] The left hand size now gives the odds. We can simplify the right to: \\[ \\frac{p}{1-p}= e^{\\beta_0} \\times e^{\\beta_1 x_1} \\times e^{\\beta_1 x_2} \\times \\cdots \\times e^{\\beta_n x_n} \\] since \\(b^{x+y} = b^x b^y\\). Note how all the additions have become multiplications. Since \\(b^{xy} = (b^x)^y\\) we can separate out the exponentiated slopes, which is useful because it is very easy to get these from a glm model summary (by exping the slopes). \\[ \\frac{p}{1-p}= e^{\\beta_0} \\times (e^{\\beta_1})^{x_1} \\times (e^{\\beta_2})^{x_2} \\times \\cdots \\times (e^{\\beta_n})^{x_n} \\] This is much easier to read if we rewrite \\(e^x\\) as \\(\\exp({x})\\): \\[ \\frac{p}{1-p}= \\exp({\\beta_0}) \\times \\exp({\\beta_1})^{x_1} \\times \\exp({\\beta_1})^{x_2} \\times \\cdots \\times \\exp({\\beta_n})^{x_n} \\] 8.11.3.2 What does this mean? (Or: so what?) Here are the coefficients from our model predicting being in paid work from number of children: coef(mod_children) ## (Intercept) k5 k618 ## 0.44478849 -0.87925889 0.02729833 We can exponentiate (and round) them to: exp(coef(mod_children)) %&gt;% round(2) ## (Intercept) k5 k618 ## 1.56 0.42 1.03 The coefficients for the slopes are called odds ratios. Slot them back into the formula: \\[ \\frac{p}{1-p}= 1.56 \\times 0.42^{\\mathtt{k5}} \\times 1.03^{\\mathtt{k618}} \\] This gives us an interpretation of the (exponentiated) coefficients in terms of odds. For every increase in k5 (children aged 0 to 5) by one child, the odds of being in work multiply by 0.42; in other words they are \\(100(1 - 0.42) = 58\\%\\) lower. Lets try plugging some numbers in, first with no children: \\[ \\begin{array}{rcl} \\frac{p}{1-p} &amp; = &amp; 1.56 \\times 0.42^{0} \\times 1.03^{0}\\\\ &amp; = &amp; 1.56 \\times 1 \\times 1 \\\\ &amp; = &amp; 1.56 \\end{array} \\] So the odds of being in work if you have no children are 1.56  i.e., the same as the exponentiated intercept. Lets try one child aged 0 to 5 and no children aged 6 or over: \\[ \\begin{array}{rcl} \\frac{p}{1-p} &amp; = &amp; 1.56 \\times 0.42^{1} \\times 1.03^{0}\\\\ &amp; = &amp; 1.56 \\times 0.42 \\times 1 \\\\ &amp; = &amp; 0.66 \\end{array} \\] And two children aged 0 to 5, again with no children aged 6 or over: \\[ \\begin{array}{rcl} \\frac{p}{1-p} &amp; = &amp; 1.56 \\times 0.42^{2} \\times 1.03^{0}\\\\ &amp; = &amp; 1.56 \\times 0.18 \\times 1 \\\\ &amp; = &amp; 0.28 \\end{array} \\] 8.11.4 Interpret using predicted probabilities This is probably my favourite way to get to grips with how a model works and R does all the hard work for us. The idea is simply to ask the model for predicted probabilities for a range of inputs, chosen to help you and readers see how the outcome variable is related to the predictors. There are two ways to do it in R. The second is easier (and you would be forgiven for trying it first) but the first explains whats going on. 8.11.4.1 Using a customs predictions table The recipe is as follows. First, generate a data frame with values for predictors for which you would like predictions. This is a data frame with one column for each variable in the model. The expand.grid command is handy for this as it generates all combinations of the values you provide. predictions &lt;- expand.grid(k5 = 0:3, k618 = c(0,5)) predictions ## k5 k618 ## 1 0 0 ## 2 1 0 ## 3 2 0 ## 4 3 0 ## 5 0 5 ## 6 1 5 ## 7 2 5 ## 8 3 5 Recall that k5 was statistically significant but k618 not, so I have chosen a range of values for k5 which show the association and also two extremes for k618. Now use Rs predict command, passing this predictions table into its newdat parameter. We can get the predictions on the log-odds scale or (more useful) as probabilities. For the latter, use the option type = \"response\". I generally save the result onto the predictions data frame: predictions$lfp_logodds &lt;- predict(mod_children, # the model newdata = predictions) predictions$lfp_prob &lt;- predict(mod_children, newdata = predictions, type = &quot;response&quot;) I will also add on the odds ratios, for completeness: predictions$lfp_odds &lt;- exp(predictions$lfp_logodds) Have a look (Im piping them through kabel from the knitr package to change the number of decimal places): predictions %&gt;% kable(digits = c(0,0,2,2,2)) k5 k618 lfp_logodds lfp_prob lfp_odds 0 0 0.44 0.61 1.56 1 0 -0.43 0.39 0.65 2 0 -1.31 0.21 0.27 3 0 -2.19 0.10 0.11 0 5 0.58 0.64 1.79 1 5 -0.30 0.43 0.74 2 5 -1.18 0.24 0.31 3 5 -2.06 0.11 0.13 The challenge is to pick values for the predictors which illustrate how the model works. You could then plot these using ggplot. predictions %&gt;% ggplot(aes(x = k5, y = lfp_prob, colour = factor(k618))) + geom_point() + geom_line() + labs(x = &quot;Number of children aged 0 to 5&quot;, y = &quot;Probability of being in work&quot;, colour = &quot;Children 6-18&quot;) + ylim(0,1) By the way, predictions work for linear models fitted using lm too and can be particularly handy for making sense of interactions or other non-linear effects. 8.11.4.2 Use ggeffects Every few months someone creates a helpful R package for creating or understanding models. The ggeffects package is tear-jerkingly beautiful. See Lüdecke (2018) for more information. There are also helpful examples on the package webpage. library(ggeffects) The command for making predictions is called ggpredict and wants a model and at least one variable name. It can generate predictions for up to four variables whilst holding the others at some constant. Lets start with one, k5. ggpredict(mod_children, terms = &quot;k5&quot;) ## ## # Predicted probabilities of lfp_yes ## # x = k5 ## ## x | Predicted | SE | 95% CI ## ----------------------------------- ## 0 | 0.62 | 0.08 | [0.58, 0.65] ## 1 | 0.40 | 0.15 | [0.33, 0.47] ## 2 | 0.22 | 0.29 | [0.13, 0.33] ## 3 | 0.10 | 0.45 | [0.05, 0.22] ## ## Adjusted for: ## * k618 = 1.00 ## Standard errors are on the link-scale (untransformed). It tries to guess sensible values to hold other predictions at. Here it has held k618 at 1. The package also comes with a plot function which automatically gives you a ggplot object. ggpredict(mod_children, terms = c(&quot;k5&quot;)) %&gt;% plot() The grey area is a 95% confidence band. You can then modify this plot as you would any other ggplot object: ggpredict(mod_children, terms = c(&quot;k5&quot;)) %&gt;% plot() + geom_point() + xlab(&quot;Number of children aged 0 to 5&quot;) + ylab(&quot;Probability of being in the labour force&quot;) + labs(title = NULL) + ylim(0,1) Before publication (or module assignment), be sure to explain in your Figure captions what exactly a graph shows, i.e., in this case that the number of children ages 6 to 18 has been held at 1. 8.12 Diagnostics Good news: we will use the same diagnostic tools as for linear regression. Actually (slightly) fewer will now apply. Load the car package: library(car) 8.12.1 Check the residual distribution You can check for outlying residuals as before: outlierTest(mod_children) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 92 1.778802 0.075272 NA Do not check if the residuals are normally distributed  they will not be and that is fine in GLM land. 8.12.2 Check that the residual mean is constant The residualPlots works as before, except when you run it with default setting on this model and data it will complain that all data are on boundary of neighbourhood, make span bigger. The span refers to how much smoothing the smoother does! residualPlots(mod_children, tests = FALSE) By default the span of the smoother is 2/3. For this model and data, setting it to 1 works fine: residualPlots(mod_children, tests = FALSE, smooth = list(span = 1)) The patterns in the data points you see here are due to the outcome variable only taking on a 0 or a 1. This is to be expected. Focus on the magenta curves and ensure that they arent curves (if you see what I mean?): they should be straight lines along zero. And indeed here they are. The variance does not need to be constant for GLMs other than the normal/Gaussian model. 8.12.3 Linearity of predictors We can use crPlots again to obtain component-plus-residual plots. crPlots(mod_children, smooth = list(span = 1)) These too are fine  no obvious curves, though potentially something worth exploring towards the upper end of k618 where there isnt much data (i.e., few families over 6 children). Again I had to fiddle with the span parameter to stop the smoother complaining. 8.12.4 Influence Again everything we covered in the previous session applies here. Heres a quick way to look at potentially influential points: influence.measures(mod_children) %&gt;% summary() ## Potentially influential observations of ## glm(formula = lfp_yes ~ k5 + k618, family = binomial, data = dat) : ## ## dfb.1_ dfb.k5 dfb.k618 dffit cov.r cook.d hat ## 18 -0.03 -0.02 0.09 0.09 1.01_* 0.00 0.01_* ## 53 -0.08 -0.03 0.15 0.15 1.04_* 0.01 0.04_* ## 74 -0.08 0.17 0.08 0.20_* 1.01_* 0.02 0.02_* ## 79 -0.03 0.18 0.01 0.18 1.01 0.02 0.01_* ## 92 0.02 0.18 -0.07 0.20_* 1.01 0.02 0.02_* ## 111 0.02 0.18 -0.07 0.20_* 1.01 0.02 0.02_* ## 186 -0.07 0.06 0.11 0.14 1.01_* 0.01 0.01_* ## 217 -0.07 0.06 0.11 0.14 1.01_* 0.01 0.01_* ## 252 -0.07 0.06 0.11 0.14 1.01_* 0.01 0.01_* ## 327 -0.03 -0.02 0.09 0.09 1.01_* 0.00 0.01_* ## 352 0.00 0.18 -0.03 0.19 1.01 0.02 0.01_* ## 371 -0.03 -0.02 0.09 0.09 1.01_* 0.00 0.01_* ## 400 -0.03 0.18 0.01 0.18 1.01 0.02 0.01_* ## 423 0.02 0.18 -0.07 0.20_* 1.01 0.02 0.02_* ## 424 -0.03 -0.02 0.09 0.09 1.01_* 0.00 0.01_* ## 430 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 434 0.05 0.03 -0.13 -0.14 1.01 0.01 0.01_* ## 447 0.02 -0.07 -0.02 -0.08 1.02_* 0.00 0.02_* ## 450 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 459 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 462 0.01 -0.07 0.00 -0.08 1.02_* 0.00 0.01_* ## 463 0.02 -0.07 -0.02 -0.08 1.02_* 0.00 0.02_* ## 466 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 482 0.05 0.03 -0.13 -0.14 1.01 0.01 0.01_* ## 483 0.00 -0.07 0.01 -0.07 1.02_* 0.00 0.01_* ## 484 0.00 -0.06 0.01 -0.06 1.02_* 0.00 0.02_* ## 499 0.05 0.03 -0.13 -0.14 1.01 0.01 0.01_* ## 509 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 528 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 553 0.01 -0.07 0.00 -0.08 1.02_* 0.00 0.01_* ## 574 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 580 0.00 -0.07 0.01 -0.07 1.02_* 0.00 0.01_* ## 585 0.00 -0.07 0.01 -0.07 1.02_* 0.00 0.01_* ## 595 0.05 0.03 -0.13 -0.14 1.01 0.01 0.01_* ## 605 0.02 -0.06 -0.01 -0.06 1.02_* 0.00 0.02_* ## 617 0.00 -0.07 0.01 -0.07 1.02_* 0.00 0.01_* ## 641 0.01 -0.07 0.00 -0.08 1.02_* 0.00 0.01_* ## 678 0.05 0.03 -0.13 -0.14 1.01 0.01 0.01_* ## 715 0.00 -0.05 0.02 -0.06 1.02_* 0.00 0.02_* ## 720 0.10 0.04 -0.20 -0.21_* 1.02_* 0.02 0.03_* ## 732 -0.01 -0.07 0.03 -0.08 1.02_* 0.00 0.02_* ## 746 0.05 -0.07 -0.07 -0.11 1.03_* 0.00 0.03_* ## 750 0.02 -0.07 -0.02 -0.08 1.02_* 0.00 0.02_* We could also look at (my favourite) the DFBETA plots: dfbetaPlots(mod_children) The numbers are minuscule, relative to the model slopes, so nothing to worry about. I am curious about the largest DFBETA for k618, though. The raw descriptives showed one person with eight children aged 6 to 18 who was in work. Is the slope being ever so slightly dragged up by her data? dfbeta(mod_children) %&gt;% as.data.frame() %&gt;% filter(k618 &gt; .009) ## (Intercept) k5 k618 ## 53 -0.009795223 -0.005237558 0.009994706 Thats row 53, so lets slice the data to look: dat %&gt;% slice(53) ## lfp lfp_yes k5 k618 age wc hc lwg inc ## 1 yes 1 0 8 37 no no 0.1625193 16.258 Yes, this is the one person with 8 children! But it doesnt matter since the dataset is so large and the DFBETA minuscule. 8.12.5 Multicolinearity Yep, you can still check the variance inflation factors (VIFs) and interpret as before! vif(mod_children) ## k5 k618 ## 1.010858 1.010858 These are both close to 1 so all is good. We could try adding multicollinearity, just to be sure that the VIFs work Here Im adding a new variable called kids which is the sum of k5 and k618: dat &lt;- dat %&gt;% mutate(kids = k5 + k618) Heres a model with both k618 and kids as predictors: high_vif_maybe &lt;- glm(lfp_yes ~ k618 + kids, data = dat, family = binomial) And here are the VIFs: vif(high_vif_maybe) ## k618 kids ## 9.189766 9.189766 They are high as you might expect, given how the kids variable was created. 8.13 A challenge 8.13.1 Activity There are several other variables in the dataset which you can now explore. Does a model with the following predictors added predict better than one with only the number of children? Variable name Description age in years. wc wifes college attendance (no/yes). hc husbands college attendance (no/yes). inc family income exclusive of wifes income (in $1000s). Interpret the coefficients, using a method of your choice Try some diagnostics (this week you have some more creative freedom)  do you want to do anything as a result of what you find? 8.13.2 (An) Answer a. Does a model with the following predictors [list of predictors] added predict better than one with only the number of children? Heres another way to add predictors to a previously fitted model; however, copy and paste works fine too! mod_more &lt;- update(mod_children, . ~ . + age + wc + hc + inc) Does this more complex model explain the data better? anova(mod_children, mod_more, test = &quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model 1: lfp_yes ~ k5 + k618 ## Model 2: lfp_yes ~ k5 + k618 + age + wc + hc + inc ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 750 994.53 ## 2 746 922.27 4 72.259 7.568e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For what its worth (we havent checked any model diagnostics yet, so this could be nonsense), yes, the model fits better, \\(\\chi^2(4) = 72.3\\), \\(p &lt; .001\\). b. Interpret the coefficients, using a method of your choice Im going to use two approaches. First the divide-by-four approach, using the original model summary to work out the direction of effects and whether they are statistically significant: summary(mod_more) ## ## Call: ## glm(formula = lfp_yes ~ k5 + k618 + age + wc + hc + inc, family = binomial, ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0971 -1.1138 0.6442 0.9980 2.1293 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.764814 0.626022 6.014 1.81e-09 *** ## k5 -1.470687 0.195576 -7.520 5.49e-14 *** ## k618 -0.087870 0.067298 -1.306 0.192 ## age -0.062877 0.012655 -4.969 6.75e-07 *** ## wcyes 1.003779 0.222230 4.517 6.28e-06 *** ## hcyes 0.119029 0.204072 0.583 0.560 ## inc -0.032105 0.008057 -3.985 6.76e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.75 on 752 degrees of freedom ## Residual deviance: 922.27 on 746 degrees of freedom ## AIC: 936.27 ## ## Number of Fisher Scoring iterations: 3 Here are the coefficients, for ease of reading: (coef(mod_more)[-1] / 4) %&gt;% round(2) ## k5 k618 age wcyes hcyes inc ## -0.37 -0.02 -0.02 0.25 0.03 -0.01 All statistically significant effects have ps far below .05. As before, the number of children aged 0 to 5 is negatively associated with the probability of being in work; each extra child reduces the probability by at most 0.37. There was no statistically significant effect for number of children aged 6 to 18. The womans age was a statistically significant predictor; each extra year reduced the probability by 0.06. Whether the woman had a college education was a statistically significant predictor and increased the probability of being in work by at most 0.25. There was no statistically significant effect for the husbands college education. Finally, family income, excluding the womans income, had a significant effect too. Each extra $1000 reduces the probability of her being in work by 0.01. Here is a picture. ggpredict(mod_more, terms = c(&quot;k5&quot;, &quot;wc&quot;, &quot;age [30,45,60]&quot;)) %&gt;% plot() + xlab(&quot;Number of children aged 0 to 5&quot;) + ylab(&quot;Probability of being in work&quot;) + labs(title = NULL, colour = &quot;Woman attended college&quot;) + ylim(0,1) + theme_gray() + theme(legend.position=&quot;bottom&quot;) Adjusted for: number of children aged 6 to 18 was fixed at 1, husband fixed at no college education, and income at the mean, $2013. (Wasnt that fantastically easy? Most of the code was just fiddling with the layout.) How about income? ggpredict(mod_more, terms = c(&quot;inc&quot;, &quot;wc&quot;, &quot;k5&quot;)) %&gt;% plot() + xlab(&quot;Family income exc. wife (in $1000s)&quot;) + ylab(&quot;Probability of being in work&quot;) + labs(title = NULL, colour = &quot;Attended college&quot;) + ylim(0,1) + theme_gray() + theme(legend.position=&quot;bottom&quot;) The effect of family income was sensitive to the number of children  more marked for lower numbers of children  highlighting why it is worth plotting model predictions. c. Try some diagnostics (this week you have some more creative freedom)  do you want to do anything as a result of what you find? Im going to start with the VIFs. vif(mod_more) ## k5 k618 age wc hc inc ## 1.396006 1.249884 1.650316 1.446317 1.552617 1.232176 No obvious problems. Residual plots show no obvious problems either: residualPlots(mod_more, tests = F, smooth = list(span = 1)) The component + residual plots look beautifully linear: crPlots(mod_more,smooth = list(span = .977)) How about the DFBETA values? dfbetaPlots(mod_more) Maybe worth looking at the largest DFBETA values for inc? dfbeta(mod_more) %&gt;% as.data.frame() %&gt;% filter(inc &gt; 0.0015) ## (Intercept) k5 k618 age wcyes ## 104 -0.03229246 -0.002015986 -0.0005965456 2.067386e-05 0.007515251 ## 119 -0.06117716 0.019764440 0.0039864127 1.382225e-04 0.002447020 ## 386 0.01529085 -0.008335088 -0.0055467113 -1.037453e-03 0.005527216 ## 402 -0.02443821 0.002537611 -0.0010025296 -3.206300e-06 -0.011153507 ## hcyes inc ## 104 -0.007958268 0.001910912 ## 119 -0.017951258 0.002801355 ## 386 -0.011739797 0.002298775 ## 402 -0.020029836 0.002007410 dat %&gt;% slice(c(104,119,386,402)) ## lfp lfp_yes k5 k618 age wc hc lwg inc kids ## 1 yes 1 0 1 48 yes yes 1.654900 70.75 1 ## 2 yes 1 1 3 38 yes yes 1.299283 91.00 4 ## 3 yes 1 0 0 41 yes yes 1.948094 79.80 0 ## 4 yes 1 0 1 48 no no 1.341559 59.00 1 They all have high family incomes, compared to the distribution hist(dat$inc) Also all these participants are in work, so they are dragging the coefficient for income slightly upwards. We could try removing them and fitting the model again. sliced_dat &lt;- dat %&gt;% slice(-c(104,119,386,402)) mod_more_sliced &lt;- glm(lfp_yes ~ k5 + k618 + age + wc + hc + inc, family = binomial, data = sliced_dat) summary(mod_more_sliced) ## ## Call: ## glm(formula = lfp_yes ~ k5 + k618 + age + wc + hc + inc, family = binomial, ## data = sliced_dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0959 -1.1123 0.6371 0.9929 2.0528 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.913525 0.634438 6.168 6.89e-10 *** ## k5 -1.495603 0.198131 -7.549 4.40e-14 *** ## k618 -0.086207 0.067894 -1.270 0.204 ## age -0.062075 0.012762 -4.864 1.15e-06 *** ## wcyes 1.003235 0.224009 4.479 7.52e-06 *** ## hcyes 0.192563 0.206951 0.930 0.352 ## inc -0.043557 0.009046 -4.815 1.47e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1025.21 on 748 degrees of freedom ## Residual deviance: 909.38 on 742 degrees of freedom ## AIC: 923.38 ## ## Number of Fisher Scoring iterations: 4 The qualitative patterns are the same in terms of direction of effects and which are statistically significant. We could peek at the coefficients in more detail. cbind(coef(mod_more), coef(mod_more_sliced)) %&gt;% round(2) ## [,1] [,2] ## (Intercept) 3.76 3.91 ## k5 -1.47 -1.50 ## k618 -0.09 -0.09 ## age -0.06 -0.06 ## wcyes 1.00 1.00 ## hcyes 0.12 0.19 ## inc -0.03 -0.04 Not a huge amount of difference and we have no reason to believe they are actually outliers, just relatively rare in the dataset, so best leave them in. Whatever we had done, its important to show this sensitivity analysis so readers can decide for themselves which set of coefficients to use. "],["complex-surveys.html", "Chapter 9 Complex surveys 9.1 Readings 9.2 The dataset 9.3 The components of a survey design 9.4 Describing the data 9.5 Fitting a GLM 9.6 Slopes 9.7 Diagnostics 9.8 Another worked example: the European Social Survey", " Chapter 9 Complex surveys By the end of this chapter you will know how to: Setup a survey object using complex survey information such as sampling weight and stratification variables. Use a tidyverse-esq approach for descriptive statistics. Fit a GLM (logistic). We will use the survey package and a tidyverse-style wrapper called srvyr. library(survey) library(srvyr) 9.1 Readings These are handy: srvyr compared to the survey package explains a way to use survey data in the tidyverse. Fox and Weisbergs online appendix, Fitting Regression Models to Data From Complex Surveys. The main reference for the models implemented by survey is the (expensive) book by Lumley (2010). UCLA has extensive notes from a 2020 seminar on survey analysis. Analyzing international survey data with the pewmethods R package, by Kat Devlin, explains an alternative way to use weights for descriptive stats. 9.2 The dataset This chapters dataset is drawn from the 2011 Canadian National Election Study  taken from the carData package and described by the Fox and Weisberg appendix cited above. Download it here. There are 2231 observations on the following 9 variables: Variable name Description id Household ID number. province a factor with (alphabetical) levels AB, BC, MB, NB, NL, NS, ON, PE, QC, SK; the sample was stratified by province. population population of the respondents province, number over age 17. weight weight sample to size of population, taking into account unequal sampling probabilities by province and household size. gender a factor with levels Female, Male. abortion attitude toward abortion, a factor with levels No, Yes; answer to the question Should abortion be banned? importance importance of religion, a factor with (alphabetical) levels not, notvery, somewhat, very; answer to the question, In your life, would you say that religion is very important, somewhat important, not very important, or not important at all? education a factor with (alphabetical) levels bachelors (Bachelors degree), college (community college or technical school), higher (graduate degree), HS (high-school graduate), lessHS (less than high-school graduate), somePS (some post-secondary). urban place of residence, a factor with levels rural, urban. Read in the data. ces &lt;- read.csv(&quot;ces11.csv&quot;, stringsAsFactors = TRUE) Im setting stringsAsFactors to TRUE so that the variables which are obviously factors are setup accordingly (R used to this by default; sometimes it had irritating side-effects). 9.3 The components of a survey design The key parts of the dataset which describe the survey design are as follows: ces %&gt;% select(id, province, population, weight) %&gt;% head(6) ## id province population weight ## 1 2851 BC 3267345 4287.85 ## 2 521 QC 5996930 9230.78 ## 3 2118 QC 5996930 6153.85 ## 4 1815 NL 406455 3430.00 ## 5 1799 ON 9439960 8977.61 ## 6 1103 ON 9439960 8977.61 id is a unique identifier for each individual, which is particularly important when there is more than one data point per person, e.g., for multilevel modelling (not in this dataset). province the data were stratified by province  random sampling by landline numbers was done within province. population provides the population by province. weight is the sampling weight, in this dataset calculated based on differences in province population, the study sample size therein, and household size. Here is how to setup a survey object using srvyr: ces_s &lt;- ces %&gt;% as_survey(ids = id, strata = province, fpc = population, weights = weight) 9.4 Describing the data We will sometimes want to compare weighted and unweighted analyses. Here is a warmup activity to show how using the tidyverse. 9.4.1 Activity use the ces data frame and tidyverse to calculate the number of people who think abortion should be banned do the same again, but this time use the ces_s survey object created above  what do you notice? compare the proportions saying yes by group Hint: you will want to use group_by. Another hint: to count, use the function n. The version for survey objects is called survey_total. 9.4.2 Answer a. use the ces data frame and tidyverse to calculate the number of people who think abortion should be banned ces %&gt;% group_by(abortion) %&gt;% summarise(n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## abortion n ## &lt;fct&gt; &lt;int&gt; ## 1 No 1818 ## 2 Yes 413 b. do the same again, but this time use the ces_s survey object created above  what do you notice? ces_s %&gt;% group_by(abortion) %&gt;% summarise(n = survey_total()) ## # A tibble: 2 x 3 ## abortion n n_se ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 13059520. 196984. ## 2 Yes 2964018. 162360. The counts are much bigger than the number of rows in the dataset due to the sampling weights. c. compare the proportions saying yes by group One way to do this is by copy and paste! Unweighted: prop_unweighted &lt;- 413 / (413 + 1818) prop_unweighted ## [1] 0.1851188 Weighted: prop_weighted &lt;- 2964018 / (2964018 + 13059520) prop_weighted ## [1] 0.184979 The unweighted proportion of yes is only a little different in this case: 0.1851188 (unweighted) v 0.184979 (weighted). Heres how to answer the questions in one go: ces_s %&gt;% group_by(abortion) %&gt;% summarise(n_raw = unweighted(n()), n_weighted = survey_total()) %&gt;% mutate(prop_raw = n_raw / sum(n_raw), prop_weighted = n_weighted / sum(n_weighted)) %&gt;% select(-n_weighted_se) ## # A tibble: 2 x 5 ## abortion n_raw n_weighted prop_raw prop_weighted ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 1818 13059520. 0.815 0.815 ## 2 Yes 413 2964018. 0.185 0.185 9.5 Fitting a GLM The survey package makes this very easy. There is a command called svyglm which is identical to glm except it has parameter called design instead of data. See ?svyglm 9.5.1 Activity mutate the survey object to add a binary variable called againstAbortion which is 1 if the participant is against abortion and 0 if not. fit an intercept-only logistic regression model without using weights (you can use as_tibble to get the raw data frame hidden within the survey object). Do the same again, this time using the survey structure. compare the predicted proportions with the raw proportions we calculated earlier 9.5.2 Answer a. mutate the survey object to add a binary variable called againstAbortion which is 1 if the participant is against abortion and 0 if not. ces_s &lt;- ces_s %&gt;% mutate(againstAbortion = as.numeric(ces$abortion == &quot;Yes&quot;)) b. fit an intercept-only logistic regression model without using weights (you can use as_tibble to get the raw data frame hidden within the survey object). m0 &lt;- glm(againstAbortion ~ 1, data = as_tibble(ces_s), family = binomial) summary(m0) ## ## Call: ## glm(formula = againstAbortion ~ 1, family = binomial, data = as_tibble(ces_s)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6399 -0.6399 -0.6399 -0.6399 1.8367 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.48204 0.05451 -27.19 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2137.6 on 2230 degrees of freedom ## Residual deviance: 2137.6 on 2230 degrees of freedom ## AIC: 2139.6 ## ## Number of Fisher Scoring iterations: 4 c. Do the same again, this time using the survey structure. sm0 &lt;- svyglm(againstAbortion ~ 1, design = ces_s, family = binomial) ## Warning in eval(family$initialize): non-integer #successes in a binomial glm! summary(sm0) ## ## Call: ## svyglm(formula = againstAbortion ~ 1, design = ces_s, family = binomial) ## ## Survey design: ## Called via srvyr ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.48297 0.06534 -22.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1.000448) ## ## Number of Fisher Scoring iterations: 4 d. compare the predicted proportions with the raw proportions we calculated earlier This undoes the log-odds (logit) transform: exp(coef(m0)) / (1+exp(coef(m0))) ## (Intercept) ## 0.1851188 exp(coef(sm0)) / (1+exp(coef(sm0))) ## (Intercept) ## 0.184979 The answer is the same as for the simple unweighted and weighted proportions, respectively. 9.6 Slopes Now, having completed the traditional step of fitting an intercept-only model, we can give the slopes a go. The Anova command in the car package works for svyglm models as before. 9.6.1 Activity Regress againstAbortion on importance, education, and gender, and interpret what you find. 9.6.2 Answer sm1 &lt;- svyglm(againstAbortion ~ importance + education + gender, design = ces_s, family = binomial) ## Warning in eval(family$initialize): non-integer #successes in a binomial glm! summary(sm1) ## ## Call: ## svyglm(formula = againstAbortion ~ importance + education + gender, ## design = ces_s, family = binomial) ## ## Survey design: ## Called via srvyr ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.8204 0.2962 -12.897 &lt; 2e-16 *** ## importancenotvery 0.4606 0.3481 1.323 0.1858 ## importancesomewhat 1.3287 0.2715 4.894 1.06e-06 *** ## importancevery 3.1405 0.2619 11.993 &lt; 2e-16 *** ## educationcollege 0.4452 0.2278 1.954 0.0508 . ## educationhigher 0.3301 0.3046 1.084 0.2786 ## educationHS 0.5692 0.2269 2.508 0.0122 * ## educationlessHS 1.0307 0.2468 4.177 3.07e-05 *** ## educationsomePS 0.1439 0.2806 0.513 0.6081 ## genderMale 0.3299 0.1482 2.225 0.0261 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 0.973961) ## ## Number of Fisher Scoring iterations: 5 To interpret those categorical predictors, it will help to check what the levels are: levels(ces$importance) ## [1] &quot;not&quot; &quot;notvery&quot; &quot;somewhat&quot; &quot;very&quot; So not is the comparison level. levels(ces$education) ## [1] &quot;bachelors&quot; &quot;college&quot; &quot;higher&quot; &quot;HS&quot; &quot;lessHS&quot; &quot;somePS&quot; bachelors is the comparison level. Gender is just a binary. Example interpretations: Men were more likely to be against abortion (log-odds 0.33 more) People for whom religion was very important were more likely than those who said not important at all to be against abortion (log-odds 3.14) You could get the odds ratios like so: exp(coef(sm1)) %&gt;% round(2) ## (Intercept) importancenotvery importancesomewhat importancevery ## 0.02 1.59 3.78 23.12 ## educationcollege educationhigher educationHS educationlessHS ## 1.56 1.39 1.77 2.80 ## educationsomePS genderMale ## 1.15 1.39 So the odds ratio of very versus not important is 23.1. The ggeffects package also works with survey models (hurrah): library(ggeffects) ggeffect(sm1, terms = c(&quot;education&quot;, &quot;importance&quot;)) %&gt;% plot() + ylim(0,1) 9.7 Diagnostics Many of the diagnostic checks we previously encountered work here too. Here are the VIFs: vif(sm1) ## GVIF Df GVIF^(1/(2*Df)) ## importance 1.052799 3 1.008612 ## education 1.066741 5 1.006482 ## gender 1.039299 1 1.019460 My favorite, the DFBETA plots: dfbetaPlots(sm1) Try also: influence.measures(sm1) 9.8 Another worked example: the European Social Survey NatCen recently published analyses of views on fairness and justice in Britain (Curtice, Hudson, &amp; Montagu, 2020), using data from the ninth wave of the 2019 European Social Survey (European Social Survey Round 9 Data, 2018). Lets see if we can replicate the results. Firstly, you will have to download the data from the ESS website. Go for the SPSS version. (You will have to register but it is a very quick process.) It will arrive as a zip file (ESS9e02.spss.zip). Simply extract the SPSS file therein (ESS9e02.sav) and move it to the same folder as your Markdown file. library(haven) ess9 &lt;- read_sav(&quot;ESS9e02.sav&quot;) Try having a look: View(ess9) Okay, if that has worked then the next problem is finding the variables we want to analyse! The report opens with a spotlight figure: Only 20% of the British public think that differences in wealth in Britain are fair, whilst a majority (59%) think that wealth differences in Britain are unfairly large and a further 16% think that differences in wealth are unfairly small. The question asked was: In your opinion, are differences in wealth in Britain unfairly small, fair, or unfairly large? Its a good exercise to spend some time wading through the documentation on the ESS website to find the variable name, before looking at the answer below We will also need the variable for country (easier to spot) and any information required for setting up the survey object. The ESS website helpfully advises: In general, you must weight tables before quoting percentages from them. The Design weights (DWEIGHT) adjust for different selection probabilities, while the Post-stratification weights (PSPWGHT) adjust for sampling error and non-response bias as well as different selection probabilities. Either DWEIGHT or PSPWGHT must always be used. In addition, the Population size weights (PWEIGHT) should be applied if you are looking at aggregates or averages for two or more countries combined. See the guide Weighting European Social Survey Data for fuller details about which weights to use. This also links to a guide to weighting the data. Its also worth printing all the variable names  if only to spot that they have ended up in lower case. 9.8.1 Set up the survey object First, Im going to fix the country variable. It currently looks like: table(ess9$cntry) ## ## AT BE BG CH CY CZ DE EE ES FI FR GB HR HU IE IT ## 2499 1767 2198 1542 781 2398 2358 1904 1668 1755 2010 2204 1810 1661 2216 2745 ## LT LV ME NL NO PL PT RS SE SI SK ## 1835 918 1200 1673 1406 1500 1055 2043 1539 1318 1083 But the dataset also has nicer labels included, which we can get like this using the as_factor function (note the underscore). This function is in the haven package. ess9$cntry &lt;- as_factor(ess9$cntry, levels = &quot;labels&quot;) table(ess9$cntry) ## ## United Kingdom Belgium Germany Estonia Ireland ## 2204 1767 2358 1904 2216 ## Montenegro Sweden Bulgaria Switzerland Finland ## 1200 1539 2198 1542 1755 ## Slovenia Slovakia Netherlands Poland Norway ## 1318 1083 1673 1500 1406 ## France Croatia Spain Serbia Austria ## 2010 1810 1668 2043 2499 ## Italy Lithuania Portugal Hungary Latvia ## 2745 1835 1055 1661 918 ## Cyprus Czechia ## 781 2398 Now lets setup the survey object: ess9_survey &lt;- ess9 %&gt;% as_survey_design(ids = idno, strata = cntry, nest = TRUE, weights = pspwght) The nest option takes account of the ids being nested within strata: in other words the same ID is used more than once across the dataset but only once in a country. 9.8.2 Try the analysis The country variable is cntry and the wealth variable is wltdffr, which I spotted with the help of the code book. The first thing you will spot is that the original variable is coded from -4 to 4: Code Meaning -4 Small, extremely unfair -3 Small, very unfair -2 Small, somewhat unfair -1 Small, slightly unfair 0 Fair 1 Large, slightly unfair 2 Large, somewhat unfair 3 Large, very unfair 4 Large, extremely unfair So lets create another variable which is grouped as per the NatCen report: ess9_survey &lt;- ess9_survey %&gt;% mutate(wltdffr_group = case_when( wltdffr &gt;= -4 &amp; wltdffr &lt;= -1 ~ &quot;Unfairly small&quot;, wltdffr == 0 ~ &quot;Fair&quot;, wltdffr &gt;= 1 &amp; wltdffr &lt;= 4 ~ &quot;Unfairly large&quot; ), wltdffr_group = factor(wltdffr_group, levels = c(&quot;Unfairly small&quot;, &quot;Fair&quot;, &quot;Unfairly large&quot;)) ) gb_wealth &lt;- ess9_survey %&gt;% filter(cntry == &quot;United Kingdom&quot;) %&gt;% group_by(wltdffr_group) %&gt;% summarise(prop = survey_mean(vartype = &quot;ci&quot;)) gb_wealth ## # A tibble: 4 x 4 ## wltdffr_group prop prop_low prop_upp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Unfairly small 0.164 0.146 0.181 ## 2 Fair 0.196 0.176 0.216 ## 3 Unfairly large 0.589 0.565 0.613 ## 4 &lt;NA&gt; 0.0508 0.0401 0.0615 The results are the same as per the report. Lets round to show this more clearly: gb_wealth %&gt;% mutate(perc = (prop*100) %&gt;% round(0)) %&gt;% select(wltdffr_group, perc) ## # A tibble: 4 x 2 ## wltdffr_group perc ## &lt;fct&gt; &lt;dbl&gt; ## 1 Unfairly small 16 ## 2 Fair 20 ## 3 Unfairly large 59 ## 4 &lt;NA&gt; 5 We can also plot the results: gb_wealth %&gt;% filter(!is.na(wltdffr_group)) %&gt;% ggplot(aes(x = wltdffr_group, y = prop*100)) + geom_col(fill = &quot;#B053A1&quot;) + geom_errorbar(aes(ymin = prop_low*100, ymax = prop_upp*100), width = 0.2) + ylim(0,100) + labs(y = &quot;%&quot;, x = NULL, title = &quot;In your opinion, are differences in wealth in Britain\\nunfairly small, fair, or unfairly large?&quot;) Lets do it again for a selection of countries. First, make a function which carries out the analysis for one country: gimme_country_results &lt;- function(the_cntry) { ess9_survey %&gt;% filter(cntry == the_cntry) %&gt;% group_by(wltdffr_group) %&gt;% summarise(prop = survey_mean(vartype = &quot;ci&quot;)) %&gt;% mutate(cntry = the_cntry) } Check it works for the UK: gimme_country_results(&quot;United Kingdom&quot;) ## # A tibble: 4 x 5 ## wltdffr_group prop prop_low prop_upp cntry ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Unfairly small 0.164 0.146 0.181 United Kingdom ## 2 Fair 0.196 0.176 0.216 United Kingdom ## 3 Unfairly large 0.589 0.565 0.613 United Kingdom ## 4 &lt;NA&gt; 0.0508 0.0401 0.0615 United Kingdom Run it for all the countries of interest: conts &lt;- c(&quot;Germany&quot;, &quot;Spain&quot;, &quot;France&quot;, &quot;United Kingdom&quot;, &quot;Italy&quot;) euro_wealth &lt;- map_dfr(conts, gimme_country_results) head(euro_wealth) ## # A tibble: 6 x 5 ## wltdffr_group prop prop_low prop_upp cntry ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Unfairly small 0.261 0.241 0.281 Germany ## 2 Fair 0.121 0.106 0.135 Germany ## 3 Unfairly large 0.557 0.535 0.579 Germany ## 4 &lt;NA&gt; 0.0619 0.0508 0.0730 Germany ## 5 Unfairly small 0.212 0.192 0.232 Spain ## 6 Fair 0.0698 0.0575 0.0822 Spain Next, try a plot: euro_wealth %&gt;% filter(!is.na(wltdffr_group)) %&gt;% ggplot(aes(x = cntry, y = prop*100, ymin = prop_low*100, ymax = prop_upp*100, fill = wltdffr_group)) + geom_col(position = position_dodge(width = .8), width = 0.6) + geom_errorbar(position=position_dodge(width = .8), colour=&quot;black&quot;, width = 0.2) + ylim(0,100) + labs(y = &quot;%&quot;, x = NULL, title = &quot;In your opinion, are differences in wealth\\nunfairly small, fair, or unfairly large?&quot;, fill = NULL) "],["references.html", "Chapter 10 References", " Chapter 10 References Belsey, D. A., Kuh, E., &amp; Welsch, R. E. (1980). Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Hoboken, NJ: John Wiley &amp; Sons, Inc. Bollen, K. A., &amp; Jackman, R. W. (1985). Regression Diagnostics: An Expository Treatment of Outliers and Influential Cases. Sociological Methods &amp; Research, 13(4), 510542. Chang, W. (2020). R Graphics Cookbook (2nd ed.). Sebastopol, CA: OReilly. Colquhoun, D. (2014). An investigation of the false discovery rate and the misinterpretation of p-values. Royal Society Open Science, 1, 140216. doi: 10.1098/rsos.140216 Curtice, J., Hudson, N., and Montagu, I. (eds.) (2020) British Social Attitudes: The 37th Report. London: The National Centre for Social Research. European Social Survey Round 9 Data (2018). Data file edition 2.0. NSD - Norwegian Centre for Research Data, Norway  Data Archive and distributor of ESS data for ESS ERIC. doi:10.21338/NSD-ESS9-2018. Fox, J. &amp; Weisberg, S. (2019). An R Companion to Applied Regression, Third Edition, Sage. Gelman, A., &amp; Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge: Cambridge University Press. Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., &amp; Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. European Journal of Epidemiology, 31, 337350. doi: 10.1007/s10654-016-0149-3 Healy, K. (2019). Data Visualization: A Practical Introduction. Princeton University Press. Lumley, T. (2010). Complex Surveys: A Guide to Analysis Using R. Hoboken, NJ: John Wiley &amp; Sons, Inc. Lüdecke D (2018). ggeffects: Tidy Data Frames of Marginal Effects from Regression Models. Journal of Open Source Software, 3(26), 772. doi: 10.21105/joss.00772 Mroz, T. A. (1987). The sensitivity of an empirical model of married womens hours of work to economic and statistical assumptions. Econometrica, 55, 765799. Noether, G. E. (1984). Nonparametrics: The Early Years-Impressions and Recollections. American Statistician, 38, 173-178] Rafi, Z., &amp; Greenland, S. (2020). Semantic and cognitive tools to aid statistical science: replace confidence and significance by compatibility and surprise. BMC Medical Research Methodology, 20(1), 244. doi: 10.1186/s12874-020-01105-9 Tingley, D., Yamamoto, T., Hirose, K., Keele, L., &amp; Imai, K. (2014). mediation: R Package for Causal Mediation Analysis. Journal of Statistical Software, 59(5), 138. doi: 10.18637/jss.v059.i05 Van Buuren, S. (2018). Flexible Imputation of Missing Data (2nd Edition). Chapman &amp; Hall/CRC. Boca Raton, FL. Wickham, H., &amp; Grolemund, G. (2017). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Sebastopol, CA: OReilly. Wolfowitz, J. (1942). Additive Partition Functions and a Class of Statistical Hypotheses. Annals of Mathematical Statistics, 13, 247-279 "]]

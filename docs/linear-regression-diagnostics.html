<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear regression diagnostics | Using R for social research</title>
  <meta name="description" content="These are some notes on using R" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear regression diagnostics | Using R for social research" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are some notes on using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear regression diagnostics | Using R for social research" />
  
  <meta name="twitter:description" content="These are some notes on using R" />
  

<meta name="author" content="Andi Fugard  (almost@gmail.com, @InductiveStep)" />


<meta name="date" content="2020-11-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="categorical-predictors-and-interactions.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<link href="libs/table1-1.0/table1_defaults.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="starting-rstudio.html"><a href="starting-rstudio.html"><i class="fa fa-check"></i><b>1</b> Starting RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="starting-rstudio.html"><a href="starting-rstudio.html#download-install-and-run"><i class="fa fa-check"></i><b>1.1</b> Download, install, and run</a></li>
<li class="chapter" data-level="1.2" data-path="starting-rstudio.html"><a href="starting-rstudio.html#make-a-new-r-notebook-file"><i class="fa fa-check"></i><b>1.2</b> Make a new R Notebook file</a></li>
<li class="chapter" data-level="1.3" data-path="starting-rstudio.html"><a href="starting-rstudio.html#save-the-file"><i class="fa fa-check"></i><b>1.3</b> Save the file</a></li>
<li class="chapter" data-level="1.4" data-path="starting-rstudio.html"><a href="starting-rstudio.html#run-the-example-r-command"><i class="fa fa-check"></i><b>1.4</b> Run the example R command</a></li>
<li class="chapter" data-level="1.5" data-path="starting-rstudio.html"><a href="starting-rstudio.html#clear-it-and-try-a-sum"><i class="fa fa-check"></i><b>1.5</b> Clear it and try a sum</a></li>
<li class="chapter" data-level="1.6" data-path="starting-rstudio.html"><a href="starting-rstudio.html#did-that-help"><i class="fa fa-check"></i><b>1.6</b> Did that help?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting-r.html"><a href="starting-r.html"><i class="fa fa-check"></i><b>2</b> Starting R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="starting-r.html"><a href="starting-r.html#arithmetic"><i class="fa fa-check"></i><b>2.1</b> Arithmetic</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="starting-r.html"><a href="starting-r.html#activities"><i class="fa fa-check"></i><b>2.1.1</b> Activities</a></li>
<li class="chapter" data-level="2.1.2" data-path="starting-r.html"><a href="starting-r.html#answers"><i class="fa fa-check"></i><b>2.1.2</b> Answers</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="starting-r.html"><a href="starting-r.html#variables"><i class="fa fa-check"></i><b>2.2</b> Variables</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="starting-r.html"><a href="starting-r.html#activity"><i class="fa fa-check"></i><b>2.2.1</b> Activity</a></li>
<li class="chapter" data-level="2.2.2" data-path="starting-r.html"><a href="starting-r.html#answer"><i class="fa fa-check"></i><b>2.2.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="starting-r.html"><a href="starting-r.html#a-note-on-variable-names"><i class="fa fa-check"></i><b>2.3</b> A note on variable names</a></li>
<li class="chapter" data-level="2.4" data-path="starting-r.html"><a href="starting-r.html#vectors"><i class="fa fa-check"></i><b>2.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="starting-r.html"><a href="starting-r.html#activity-1"><i class="fa fa-check"></i><b>2.4.1</b> Activity</a></li>
<li class="chapter" data-level="2.4.2" data-path="starting-r.html"><a href="starting-r.html#answer-1"><i class="fa fa-check"></i><b>2.4.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="starting-r.html"><a href="starting-r.html#functions"><i class="fa fa-check"></i><b>2.5</b> Functions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="starting-r.html"><a href="starting-r.html#activity-2"><i class="fa fa-check"></i><b>2.5.1</b> Activity</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting-r.html"><a href="starting-r.html#answer-2"><i class="fa fa-check"></i><b>2.5.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting-r.html"><a href="starting-r.html#more-functions"><i class="fa fa-check"></i><b>2.6</b> More functions</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="starting-r.html"><a href="starting-r.html#activity-3"><i class="fa fa-check"></i><b>2.6.1</b> Activity</a></li>
<li class="chapter" data-level="2.6.2" data-path="starting-r.html"><a href="starting-r.html#answer-3"><i class="fa fa-check"></i><b>2.6.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="starting-r.html"><a href="starting-r.html#data-frames"><i class="fa fa-check"></i><b>2.7</b> Data frames</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="starting-r.html"><a href="starting-r.html#activity-4"><i class="fa fa-check"></i><b>2.7.1</b> Activity</a></li>
<li class="chapter" data-level="2.7.2" data-path="starting-r.html"><a href="starting-r.html#answer-4"><i class="fa fa-check"></i><b>2.7.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="starting-r.html"><a href="starting-r.html#loading-data-frames-from-a-file"><i class="fa fa-check"></i><b>2.8</b> Loading data frames from a file</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="starting-r.html"><a href="starting-r.html#activity-5"><i class="fa fa-check"></i><b>2.8.1</b> Activity</a></li>
<li class="chapter" data-level="2.8.2" data-path="starting-r.html"><a href="starting-r.html#answer-5"><i class="fa fa-check"></i><b>2.8.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="starting-r.html"><a href="starting-r.html#packages"><i class="fa fa-check"></i><b>2.9</b> Packages</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="starting-r.html"><a href="starting-r.html#activity-6"><i class="fa fa-check"></i><b>2.9.1</b> Activity</a></li>
<li class="chapter" data-level="2.9.2" data-path="starting-r.html"><a href="starting-r.html#answer-6"><i class="fa fa-check"></i><b>2.9.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="starting-r.html"><a href="starting-r.html#the-end"><i class="fa fa-check"></i><b>2.10</b> The end!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html"><i class="fa fa-check"></i><b>3</b> Visualising data in the tidyverse</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#getting-setup"><i class="fa fa-check"></i><b>3.1</b> Getting setup</a></li>
<li class="chapter" data-level="3.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#an-interlude-on-functions"><i class="fa fa-check"></i><b>3.2</b> An interlude on functions</a></li>
<li class="chapter" data-level="3.3" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#a-scatterplot-in-ggplot"><i class="fa fa-check"></i><b>3.3</b> A scatterplot in ggplot</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#warm-up-activity"><i class="fa fa-check"></i><b>3.3.1</b> Warm-up activity</a></li>
<li class="chapter" data-level="3.3.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#answer-7"><i class="fa fa-check"></i><b>3.3.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#another-aesthetic-colour"><i class="fa fa-check"></i><b>3.4</b> Another aesthetic: colour</a></li>
<li class="chapter" data-level="3.5" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#another-geom-jitter"><i class="fa fa-check"></i><b>3.5</b> Another geom: jitter</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#activity-to-develop-your-help-searching-skill"><i class="fa fa-check"></i><b>3.5.1</b> Activity to develop your help-searching skill!</a></li>
<li class="chapter" data-level="3.5.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#answer-8"><i class="fa fa-check"></i><b>3.5.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#aggregatingsummarising-data-by-group"><i class="fa fa-check"></i><b>3.6</b> Aggregating/summarising data by group</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#activity-7"><i class="fa fa-check"></i><b>3.6.1</b> Activity</a></li>
<li class="chapter" data-level="3.6.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#answer-9"><i class="fa fa-check"></i><b>3.6.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#pipes"><i class="fa fa-check"></i><b>3.7</b> Pipes</a></li>
<li class="chapter" data-level="3.8" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#plot-the-mean-life-expectancy-by-continent"><i class="fa fa-check"></i><b>3.8</b> Plot the mean life expectancy by continent</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#actvity"><i class="fa fa-check"></i><b>3.8.1</b> Actvity</a></li>
<li class="chapter" data-level="3.8.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#answer-10"><i class="fa fa-check"></i><b>3.8.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#yet-another-geom-line"><i class="fa fa-check"></i><b>3.9</b> Yet another geom: line</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#activity-8"><i class="fa fa-check"></i><b>3.9.1</b> Activity</a></li>
<li class="chapter" data-level="3.9.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#answer-11"><i class="fa fa-check"></i><b>3.9.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#filtering-data-along-the-pipeline"><i class="fa fa-check"></i><b>3.10</b> Filtering data along the pipeline</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#activity-9"><i class="fa fa-check"></i><b>3.10.1</b> Activity</a></li>
<li class="chapter" data-level="3.10.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#answer-12"><i class="fa fa-check"></i><b>3.10.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#other-handy-tools-select-slice-bind-and-arrange"><i class="fa fa-check"></i><b>3.11</b> Other handy tools: select, slice, bind, and arrange</a></li>
<li class="chapter" data-level="3.12" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#filtering-for-members-of-a-vector"><i class="fa fa-check"></i><b>3.12</b> Filtering for members of a vector</a></li>
<li class="chapter" data-level="3.13" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#final-challenge"><i class="fa fa-check"></i><b>3.13</b> Final challenge</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#activity-10"><i class="fa fa-check"></i><b>3.13.1</b> Activity</a></li>
<li class="chapter" data-level="3.13.2" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#answer-13"><i class="fa fa-check"></i><b>3.13.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="visualising-data-in-the-tidyverse.html"><a href="visualising-data-in-the-tidyverse.html#more-ideas-for-visualisations"><i class="fa fa-check"></i><b>3.14</b> More ideas for visualisations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html"><i class="fa fa-check"></i><b>4</b> P-values and confidence intervals</a>
<ul>
<li class="chapter" data-level="4.1" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#correlation-recap"><i class="fa fa-check"></i><b>4.1</b> Correlation recap</a></li>
<li class="chapter" data-level="4.2" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#testing-null-hypotheses"><i class="fa fa-check"></i><b>4.2</b> Testing null-hypotheses</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#what-can-samples-look-like-when-the-true-correlation-is-0"><i class="fa fa-check"></i><b>4.2.1</b> What can samples look like when the true correlation is 0?</a></li>
<li class="chapter" data-level="4.2.2" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#understanding-actual-data-in-relation-to-these-simulations"><i class="fa fa-check"></i><b>4.2.2</b> Understanding actual data in relation to these simulations</a></li>
<li class="chapter" data-level="4.2.3" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#so-what-is-a-p-value"><i class="fa fa-check"></i><b>4.2.3</b> So, what is a p-value?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#confidence-intervals"><i class="fa fa-check"></i><b>4.3</b> Confidence intervals</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#simulating-confidence"><i class="fa fa-check"></i><b>4.3.1</b> Simulating confidence</a></li>
<li class="chapter" data-level="4.3.2" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#what-is-a-confidence-interval-then"><i class="fa fa-check"></i><b>4.3.2</b> What is a confidence interval, then?</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="p-values-and-confidence-intervals.html"><a href="p-values-and-confidence-intervals.html#further-reading"><i class="fa fa-check"></i><b>4.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html#before-we-begin"><i class="fa fa-check"></i><b>5.1</b> Before we begin</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression.html"><a href="linear-regression.html#the-dataset"><i class="fa fa-check"></i><b>5.2</b> The dataset</a></li>
<li class="chapter" data-level="5.3" data-path="linear-regression.html"><a href="linear-regression.html#interlude-on-methodology"><i class="fa fa-check"></i><b>5.3</b> Interlude on methodology</a></li>
<li class="chapter" data-level="5.4" data-path="linear-regression.html"><a href="linear-regression.html#descriptives"><i class="fa fa-check"></i><b>5.4</b> Descriptives</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression.html"><a href="linear-regression.html#activity-11"><i class="fa fa-check"></i><b>5.4.1</b> Activity</a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression.html"><a href="linear-regression.html#answer-14"><i class="fa fa-check"></i><b>5.4.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression.html"><a href="linear-regression.html#prep-to-understand-the-simplest-regression-model"><i class="fa fa-check"></i><b>5.5</b> Prep to understand the simplest regression model</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-regression.html"><a href="linear-regression.html#activity-12"><i class="fa fa-check"></i><b>5.5.1</b> Activity</a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-regression.html"><a href="linear-regression.html#answer-15"><i class="fa fa-check"></i><b>5.5.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-regression.html"><a href="linear-regression.html#the-simplest-regression-model-intercept-only-model"><i class="fa fa-check"></i><b>5.6</b> The simplest regression model: intercept-only model</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="linear-regression.html"><a href="linear-regression.html#activity-13"><i class="fa fa-check"></i><b>5.6.1</b> Activity</a></li>
<li class="chapter" data-level="5.6.2" data-path="linear-regression.html"><a href="linear-regression.html#answer-16"><i class="fa fa-check"></i><b>5.6.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="linear-regression.html"><a href="linear-regression.html#adding-a-slope-to-the-regression-model"><i class="fa fa-check"></i><b>5.7</b> Adding a slope to the regression model</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="linear-regression.html"><a href="linear-regression.html#activity-14"><i class="fa fa-check"></i><b>5.7.1</b> Activity</a></li>
<li class="chapter" data-level="5.7.2" data-path="linear-regression.html"><a href="linear-regression.html#answer-17"><i class="fa fa-check"></i><b>5.7.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="linear-regression.html"><a href="linear-regression.html#residuals"><i class="fa fa-check"></i><b>5.8</b> Residuals</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="linear-regression.html"><a href="linear-regression.html#activity-15"><i class="fa fa-check"></i><b>5.8.1</b> Activity</a></li>
<li class="chapter" data-level="5.8.2" data-path="linear-regression.html"><a href="linear-regression.html#answer-18"><i class="fa fa-check"></i><b>5.8.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="linear-regression.html"><a href="linear-regression.html#comparing-models"><i class="fa fa-check"></i><b>5.9</b> Comparing models</a></li>
<li class="chapter" data-level="5.10" data-path="linear-regression.html"><a href="linear-regression.html#regression-with-two-or-more-predictors"><i class="fa fa-check"></i><b>5.10</b> Regression with two or more predictors</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="linear-regression.html"><a href="linear-regression.html#activity-16"><i class="fa fa-check"></i><b>5.10.1</b> Activity</a></li>
<li class="chapter" data-level="5.10.2" data-path="linear-regression.html"><a href="linear-regression.html#answer-19"><i class="fa fa-check"></i><b>5.10.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="linear-regression.html"><a href="linear-regression.html#interpreting-regression-models-with-two-or-more-predictors"><i class="fa fa-check"></i><b>5.11</b> Interpreting regression models with two or more predictors</a></li>
<li class="chapter" data-level="5.12" data-path="linear-regression.html"><a href="linear-regression.html#optional-that-pesky-negative-intercept"><i class="fa fa-check"></i><b>5.12</b> Optional: that pesky negative intercept</a>
<ul>
<li class="chapter" data-level="5.12.1" data-path="linear-regression.html"><a href="linear-regression.html#activity-17"><i class="fa fa-check"></i><b>5.12.1</b> Activity</a></li>
<li class="chapter" data-level="5.12.2" data-path="linear-regression.html"><a href="linear-regression.html#answer-20"><i class="fa fa-check"></i><b>5.12.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="linear-regression.html"><a href="linear-regression.html#finally-confidence-intervals"><i class="fa fa-check"></i><b>5.13</b> Finally: confidence intervals</a></li>
<li class="chapter" data-level="5.14" data-path="linear-regression.html"><a href="linear-regression.html#very-optional-extras"><i class="fa fa-check"></i><b>5.14</b> Very optional extras</a>
<ul>
<li class="chapter" data-level="5.14.1" data-path="linear-regression.html"><a href="linear-regression.html#making-functions"><i class="fa fa-check"></i><b>5.14.1</b> Making functions</a></li>
<li class="chapter" data-level="5.14.2" data-path="linear-regression.html"><a href="linear-regression.html#another-way-to-make-scatterplots-ggally"><i class="fa fa-check"></i><b>5.14.2</b> Another way to make scatterplots: GGally</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html"><i class="fa fa-check"></i><b>6</b> Linear regression diagnostics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#before-we-begin-1"><i class="fa fa-check"></i><b>6.1</b> Before we begin</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#the-dataset-1"><i class="fa fa-check"></i><b>6.2</b> The dataset</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#fit-a-regression-model"><i class="fa fa-check"></i><b>6.3</b> Fit a regression model</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#activity-18"><i class="fa fa-check"></i><b>6.3.1</b> Activity</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#answer-21"><i class="fa fa-check"></i><b>6.3.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#checking-for-normally-distributed-residuals"><i class="fa fa-check"></i><b>6.4</b> Checking for normally distributed residuals</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#base-r-histogram"><i class="fa fa-check"></i><b>6.4.1</b> Base R histogram</a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#quantile-comparison-plot"><i class="fa fa-check"></i><b>6.4.2</b> Quantile-comparison plot</a></li>
<li class="chapter" data-level="6.4.3" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#statistical-test-of-normality"><i class="fa fa-check"></i><b>6.4.3</b> Statistical test of normality</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#checking-constant-residual-variance"><i class="fa fa-check"></i><b>6.5</b> Checking constant residual variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#activity-20"><i class="fa fa-check"></i><b>6.5.1</b> Activity</a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#answer-23"><i class="fa fa-check"></i><b>6.5.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#checking-for-relationships-between-residuals-and-predicted-outcome-or-predictors"><i class="fa fa-check"></i><b>6.6</b> Checking for relationships between residuals and predicted outcome or predictors</a></li>
<li class="chapter" data-level="6.7" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#checking-linearity"><i class="fa fa-check"></i><b>6.7</b> Checking linearity</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#what-should-be-linear-in-a-linear-model"><i class="fa fa-check"></i><b>6.7.1</b> What should be linear in a linear model?</a></li>
<li class="chapter" data-level="6.7.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#checking-for-linearity"><i class="fa fa-check"></i><b>6.7.2</b> Checking for linearity</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#checking-influence-leave-one-out-analyses"><i class="fa fa-check"></i><b>6.8</b> Checking influence: leave-one-out analyses</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#residual-outliers"><i class="fa fa-check"></i><b>6.8.1</b> Residual outliers</a></li>
<li class="chapter" data-level="6.8.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#cooks-distance"><i class="fa fa-check"></i><b>6.8.2</b> Cook’s distance</a></li>
<li class="chapter" data-level="6.8.3" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#dfbeta-and-close-sibling-dfbetas"><i class="fa fa-check"></i><b>6.8.3</b> DFBETA and (close sibling) DFBETAS</a></li>
<li class="chapter" data-level="6.8.4" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#view-them-all"><i class="fa fa-check"></i><b>6.8.4</b> View them all</a></li>
<li class="chapter" data-level="6.8.5" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#so-er-what-should-we-do-with-potentially-influential-observations"><i class="fa fa-check"></i><b>6.8.5</b> So, er, what should we do with “potentially influential” observations…?</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#checking-the-variance-inflation-factors-vifs"><i class="fa fa-check"></i><b>6.9</b> Checking the variance inflation factors (VIFs)</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#activity-22"><i class="fa fa-check"></i><b>6.9.1</b> Activity</a></li>
<li class="chapter" data-level="6.9.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#answer-25"><i class="fa fa-check"></i><b>6.9.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#the-challenge"><i class="fa fa-check"></i><b>6.10</b> The challenge</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#activity-23"><i class="fa fa-check"></i><b>6.10.1</b> Activity</a></li>
<li class="chapter" data-level="6.10.2" data-path="linear-regression-diagnostics.html"><a href="linear-regression-diagnostics.html#answer-26"><i class="fa fa-check"></i><b>6.10.2</b> Answer</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html"><i class="fa fa-check"></i><b>7</b> Categorical predictors and interactions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#before-we-begin-2"><i class="fa fa-check"></i><b>7.1</b> Before we begin</a></li>
<li class="chapter" data-level="7.2" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#the-dataset-2"><i class="fa fa-check"></i><b>7.2</b> The dataset</a></li>
<li class="chapter" data-level="7.3" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#factors"><i class="fa fa-check"></i><b>7.3</b> Factors</a></li>
<li class="chapter" data-level="7.4" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#visualising-the-data"><i class="fa fa-check"></i><b>7.4</b> Visualising the data</a></li>
<li class="chapter" data-level="7.5" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#the-punchline-occupation-type-does-predict-prestige"><i class="fa fa-check"></i><b>7.5</b> The punchline: occupation type does predict prestige</a></li>
<li class="chapter" data-level="7.6" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#understanding-factors-in-regression-models"><i class="fa fa-check"></i><b>7.6</b> Understanding factors in regression models</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#how-are-categorical-variables-encoded"><i class="fa fa-check"></i><b>7.6.1</b> How are categorical variables encoded?</a></li>
<li class="chapter" data-level="7.6.2" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#how-are-binary-two-level-categorical-predictors-encoded"><i class="fa fa-check"></i><b>7.6.2</b> How are binary (two-level) categorical predictors encoded?</a></li>
<li class="chapter" data-level="7.6.3" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#categorical-predictors-with-3-or-more-levels"><i class="fa fa-check"></i><b>7.6.3</b> Categorical predictors with 3 or more levels</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>7.7</b> Interpreting the coefficients</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#activity-25"><i class="fa fa-check"></i><b>7.7.1</b> Activity</a></li>
<li class="chapter" data-level="7.7.2" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#answer-28"><i class="fa fa-check"></i><b>7.7.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#checking-all-combinations"><i class="fa fa-check"></i><b>7.8</b> Checking all combinations</a></li>
<li class="chapter" data-level="7.9" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#the-intercept-is-not-always-the-mean-of-the-comparison-group"><i class="fa fa-check"></i><b>7.9</b> The intercept is not always the mean of the comparison group</a></li>
<li class="chapter" data-level="7.10" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#recap"><i class="fa fa-check"></i><b>7.10</b> Recap</a></li>
<li class="chapter" data-level="7.11" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#challenge"><i class="fa fa-check"></i><b>7.11</b> Challenge</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#activity-26"><i class="fa fa-check"></i><b>7.11.1</b> Activity</a></li>
<li class="chapter" data-level="7.11.2" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#answers-1"><i class="fa fa-check"></i><b>7.11.2</b> Answers</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#brief-introduction-to-interactions"><i class="fa fa-check"></i><b>7.12</b> Brief introduction to interactions</a>
<ul>
<li class="chapter" data-level="7.12.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#what-is-an-interaction"><i class="fa fa-check"></i><b>7.12.1</b> What is an interaction?</a></li>
<li class="chapter" data-level="7.12.2" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#how-to-test-for-interactions-in-r"><i class="fa fa-check"></i><b>7.12.2</b> How to test for interactions in R</a></li>
<li class="chapter" data-level="7.12.3" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#understanding-interactions"><i class="fa fa-check"></i><b>7.12.3</b> Understanding interactions</a></li>
<li class="chapter" data-level="7.12.4" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#further-reading-1"><i class="fa fa-check"></i><b>7.12.4</b> Further reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#setup"><i class="fa fa-check"></i><b>8.1</b> Setup</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#the-dataset-3"><i class="fa fa-check"></i><b>8.2</b> The dataset</a></li>
<li class="chapter" data-level="8.3" data-path="logistic-regression.html"><a href="logistic-regression.html#warmup-activity"><i class="fa fa-check"></i><b>8.3</b> Warmup activity</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#activity-27"><i class="fa fa-check"></i><b>8.3.1</b> Activity</a></li>
<li class="chapter" data-level="8.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#answer-29"><i class="fa fa-check"></i><b>8.3.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="logistic-regression.html"><a href="logistic-regression.html#the-punchline"><i class="fa fa-check"></i><b>8.4</b> The punchline</a></li>
<li class="chapter" data-level="8.5" data-path="logistic-regression.html"><a href="logistic-regression.html#intermezzo-parametric-versus-nonparametric"><i class="fa fa-check"></i><b>8.5</b> Intermezzo: parametric versus nonparametric</a></li>
<li class="chapter" data-level="8.6" data-path="logistic-regression.html"><a href="logistic-regression.html#glms"><i class="fa fa-check"></i><b>8.6</b> What is a generalised linear model?</a></li>
<li class="chapter" data-level="8.7" data-path="logistic-regression.html"><a href="logistic-regression.html#what-is-the-log-function-again"><i class="fa fa-check"></i><b>8.7</b> What is the log function again…?</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="logistic-regression.html"><a href="logistic-regression.html#log-arithmetic"><i class="fa fa-check"></i><b>8.7.1</b> The arithmetic</a></li>
<li class="chapter" data-level="8.7.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-log"><i class="fa fa-check"></i><b>8.7.2</b> Why log?</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="logistic-regression.html"><a href="logistic-regression.html#intercept-only-models-again"><i class="fa fa-check"></i><b>8.8</b> Intercept-only models again</a></li>
<li class="chapter" data-level="8.9" data-path="logistic-regression.html"><a href="logistic-regression.html#odds-and-log-odds"><i class="fa fa-check"></i><b>8.9</b> Odds and log odds</a></li>
<li class="chapter" data-level="8.10" data-path="logistic-regression.html"><a href="logistic-regression.html#back-to-that-intercept"><i class="fa fa-check"></i><b>8.10</b> Back to that intercept</a></li>
<li class="chapter" data-level="8.11" data-path="logistic-regression.html"><a href="logistic-regression.html#interpret-slopes"><i class="fa fa-check"></i><b>8.11</b> Interpreting model slopes</a>
<ul>
<li class="chapter" data-level="8.11.1" data-path="logistic-regression.html"><a href="logistic-regression.html#interpret-on-the-log-odds-scale"><i class="fa fa-check"></i><b>8.11.1</b> Interpret on the log-odds scale</a></li>
<li class="chapter" data-level="8.11.2" data-path="logistic-regression.html"><a href="logistic-regression.html#interpret-using-the-divide-by-4-approximation"><i class="fa fa-check"></i><b>8.11.2</b> Interpret using the “divide-by-4” approximation</a></li>
<li class="chapter" data-level="8.11.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpret-using-odds"><i class="fa fa-check"></i><b>8.11.3</b> Interpret using odds</a></li>
<li class="chapter" data-level="8.11.4" data-path="logistic-regression.html"><a href="logistic-regression.html#interpret-using-predicted-probabilities"><i class="fa fa-check"></i><b>8.11.4</b> Interpret using predicted probabilities</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="logistic-regression.html"><a href="logistic-regression.html#diagnostics"><i class="fa fa-check"></i><b>8.12</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="8.12.1" data-path="logistic-regression.html"><a href="logistic-regression.html#check-the-residual-distribution"><i class="fa fa-check"></i><b>8.12.1</b> Check the residual distribution</a></li>
<li class="chapter" data-level="8.12.2" data-path="logistic-regression.html"><a href="logistic-regression.html#check-that-the-residual-mean-is-constant"><i class="fa fa-check"></i><b>8.12.2</b> Check that the residual mean is constant</a></li>
<li class="chapter" data-level="8.12.3" data-path="logistic-regression.html"><a href="logistic-regression.html#linearity-of-predictors"><i class="fa fa-check"></i><b>8.12.3</b> Linearity of predictors</a></li>
<li class="chapter" data-level="8.12.4" data-path="logistic-regression.html"><a href="logistic-regression.html#influence"><i class="fa fa-check"></i><b>8.12.4</b> Influence</a></li>
<li class="chapter" data-level="8.12.5" data-path="logistic-regression.html"><a href="logistic-regression.html#multicolinearity"><i class="fa fa-check"></i><b>8.12.5</b> Multicolinearity</a></li>
</ul></li>
<li class="chapter" data-level="8.13" data-path="logistic-regression.html"><a href="logistic-regression.html#a-challenge"><i class="fa fa-check"></i><b>8.13</b> A challenge</a>
<ul>
<li class="chapter" data-level="8.13.1" data-path="logistic-regression.html"><a href="logistic-regression.html#activity-31"><i class="fa fa-check"></i><b>8.13.1</b> Activity</a></li>
<li class="chapter" data-level="8.13.2" data-path="logistic-regression.html"><a href="logistic-regression.html#an-answer"><i class="fa fa-check"></i><b>8.13.2</b> (An) Answer</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="complex-surveys.html"><a href="complex-surveys.html"><i class="fa fa-check"></i><b>9</b> Complex surveys</a>
<ul>
<li class="chapter" data-level="9.1" data-path="complex-surveys.html"><a href="complex-surveys.html#readings"><i class="fa fa-check"></i><b>9.1</b> Readings</a></li>
<li class="chapter" data-level="9.2" data-path="complex-surveys.html"><a href="complex-surveys.html#the-dataset-4"><i class="fa fa-check"></i><b>9.2</b> The dataset</a></li>
<li class="chapter" data-level="9.3" data-path="complex-surveys.html"><a href="complex-surveys.html#the-components-of-a-survey-design"><i class="fa fa-check"></i><b>9.3</b> The components of a survey design</a></li>
<li class="chapter" data-level="9.4" data-path="complex-surveys.html"><a href="complex-surveys.html#describing-the-data"><i class="fa fa-check"></i><b>9.4</b> Describing the data</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="complex-surveys.html"><a href="complex-surveys.html#activity-32"><i class="fa fa-check"></i><b>9.4.1</b> Activity</a></li>
<li class="chapter" data-level="9.4.2" data-path="complex-surveys.html"><a href="complex-surveys.html#answer-33"><i class="fa fa-check"></i><b>9.4.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="complex-surveys.html"><a href="complex-surveys.html#fitting-a-glm"><i class="fa fa-check"></i><b>9.5</b> Fitting a GLM</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="complex-surveys.html"><a href="complex-surveys.html#activity-33"><i class="fa fa-check"></i><b>9.5.1</b> Activity</a></li>
<li class="chapter" data-level="9.5.2" data-path="complex-surveys.html"><a href="complex-surveys.html#answer-34"><i class="fa fa-check"></i><b>9.5.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="complex-surveys.html"><a href="complex-surveys.html#slopes"><i class="fa fa-check"></i><b>9.6</b> Slopes</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="complex-surveys.html"><a href="complex-surveys.html#activity-34"><i class="fa fa-check"></i><b>9.6.1</b> Activity</a></li>
<li class="chapter" data-level="9.6.2" data-path="complex-surveys.html"><a href="complex-surveys.html#answer-35"><i class="fa fa-check"></i><b>9.6.2</b> Answer</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="complex-surveys.html"><a href="complex-surveys.html#diagnostics-1"><i class="fa fa-check"></i><b>9.7</b> Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="mediation-analysis.html"><a href="mediation-analysis.html"><i class="fa fa-check"></i><b>10</b> Mediation analysis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#simulated-example"><i class="fa fa-check"></i><b>10.1</b> Simulated Example</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#make-up-some-data"><i class="fa fa-check"></i><b>10.1.1</b> Make up some data</a></li>
<li class="chapter" data-level="10.1.2" data-path="mediation-analysis.html"><a href="mediation-analysis.html#analyse-it"><i class="fa fa-check"></i><b>10.1.2</b> Analyse it</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>11</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for social research</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-diagnostics" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Linear regression diagnostics</h1>
<p>A cursory glance at Chapter 8 of Fox and Weisberg (2019) will reveal that there are many diagnostic checks for regression models. An oft-cited book on diagnostics by Belsey et al. (1980) runs to 300 pages.</p>
<p>There is an element of subjectivity in deciding which checks to use and how to interpret them, and hence a chance that authors cherry pick diagnostics to make it appear that a model explains the data better than it actually does. So, as ever, it is important to create an analysis plan before seeing data, ideally registered in some way online.</p>
<p>By the end of this chapter you will know how to carry out common diagnostic checks of regression models using R. Many of the ideas transfer with little effort to other models we will encounter later too.</p>
<p>I recommend that you follow along with the examples, fiddling with the code as your curiosity guides you (don’t simply copy and paste code and run it – play!). The tutorial ends with an activity, similar to the sorts of things that have been asked in exams.</p>
<div id="before-we-begin-1" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Before we begin</h2>
<p>You could continue on the end of last week’s Markdown file or make a new one. We will be using the same <code>prestige.csv</code> dataset, so whatever you do please ensure the data is saved in the same folder as your Markdown file.</p>
<p>Ensure these handy packages are installed and included:</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="linear-regression-diagnostics.html#cb244-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb244-2"><a href="linear-regression-diagnostics.html#cb244-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
</div>
<div id="the-dataset-1" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> The dataset</h2>
<p>So it’s easy to find, here are the variable names again. Each row describes an occupation and aggregated data about that occupation.</p>
<table>
<colgroup>
<col width="53%" />
<col width="46%" />
</colgroup>
<thead>
<tr class="header">
<th>Variable name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>occ</td>
<td>Occupation</td>
</tr>
<tr class="even">
<td>education</td>
<td>Average years of education for people in the job</td>
</tr>
<tr class="odd">
<td>income</td>
<td>Average income in dollars</td>
</tr>
<tr class="even">
<td>women</td>
<td>Percentage of women in occupation</td>
</tr>
<tr class="odd">
<td>prestige</td>
<td>A Pineo-Porter prestige score for the occupation with a possible range of 0-100 (higher = more prestigious)</td>
</tr>
<tr class="even">
<td>type</td>
<td>“bc” = blue collar <br> “wc” = white collar <br> “prof” = professional, managerial, or technical</td>
</tr>
</tbody>
</table>
<p>Read it in:</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="linear-regression-diagnostics.html#cb245-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;prestige.csv&quot;</span>)</span></code></pre></div>
<p>I’m going to <code>mutate</code> this to add the income in $1000s again.</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="linear-regression-diagnostics.html#cb246-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> dat <span class="sc">%&gt;%</span></span>
<span id="cb246-2"><a href="linear-regression-diagnostics.html#cb246-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">income_1000s =</span> income<span class="sc">/</span><span class="dv">1000</span>)</span></code></pre></div>
</div>
<div id="fit-a-regression-model" class="section level2 tabset" number="6.3">
<h2><span class="header-section-number">6.3</span> Fit a regression model</h2>
<p>We will spend some time exploring this simple two-predictor model, so ensure it exists in the environment:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="linear-regression-diagnostics.html#cb247-1" aria-hidden="true" tabindex="-1"></a>mod_both <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> education <span class="sc">+</span> income_1000s,</span>
<span id="cb247-2"><a href="linear-regression-diagnostics.html#cb247-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> dat)</span>
<span id="cb247-3"><a href="linear-regression-diagnostics.html#cb247-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_both)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = prestige ~ education + income_1000s, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.4040  -5.3308   0.0154   4.9803  17.6889 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -6.8478     3.2190  -2.127   0.0359 *  
## education      4.1374     0.3489  11.858  &lt; 2e-16 ***
## income_1000s   1.3612     0.2242   6.071 2.36e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.81 on 99 degrees of freedom
## Multiple R-squared:  0.798,  Adjusted R-squared:  0.7939 
## F-statistic: 195.6 on 2 and 99 DF,  p-value: &lt; 2.2e-16</code></pre>
<div id="activity-18" class="section level3" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Activity</h3>
<p>How should the coefficients be interpreted?</p>
</div>
<div id="answer-21" class="section level3" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Answer</h3>
<p>Both education and income are statistically significant predictors of prestige (both <em>p</em>’s &lt; .001). Each year of education is associated with 4.1 more prestige points and $1000 more income is associated with 1.4 extra prestige points (whilst holding other predictors constant). The model explains 79% of the variance in prestige and, perhaps unsurprisingly with that high an <span class="math inline">\(R^2\)</span>, is statistically significantly better than the intercept-only model, <span class="math inline">\(F(2,99) = 195.6\)</span>, <span class="math inline">\(p &lt; .001\)</span>.</p>
<p>Note how that model test is included in the last line of the model summary; we can also check it explicitly with:</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="linear-regression-diagnostics.html#cb249-1" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> dat)</span>
<span id="cb249-2"><a href="linear-regression-diagnostics.html#cb249-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod0, mod_both)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: prestige ~ 1
## Model 2: prestige ~ education + income_1000s
##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    101 29895.4                                  
## 2     99  6038.9  2     23857 195.55 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
<div id="checking-for-normally-distributed-residuals" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Checking for normally distributed residuals</h2>
<p>To obtain the residuals for a model, use the command <code>resid</code> and save them somewhere. I will add them onto the <code>dat</code> data frame so that it is easy to cross reference each residual with the original data:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="linear-regression-diagnostics.html#cb251-1" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>mod_both_resids <span class="ot">&lt;-</span> <span class="fu">resid</span>(mod_both)</span></code></pre></div>
<p>These should have a normal, also known as a Gaussian distribution, in linear regression models. How do we check…?</p>
<div id="base-r-histogram" class="section level3" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Base R histogram</h3>
<p>One way is to have a look at a histogram using the base R <code>hist</code> function:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="linear-regression-diagnostics.html#cb252-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(dat<span class="sc">$</span>mod_both_resids)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-175-1.png" width="672" /></p>
<p>Is that a normal distribution? Probably, yes; however, in general <code>¯\_(ツ)_/¯</code>, it is not always obvious. There are better ways to check – read on!</p>
</div>
<div id="quantile-comparison-plot" class="section level3" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Quantile-comparison plot</h3>
<p>An easier way to check the distribution is using a quantile-comparison plot, also known as a quantile-quantile or Q-Q plot. The <code>car</code> package, associated with the Fox and Weisberg (2019) textbook, has a lovely function for creating them called <code>qqPlot</code>:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="linear-regression-diagnostics.html#cb253-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqPlot</span>(dat<span class="sc">$</span>mod_both_resids, <span class="at">id =</span> <span class="fu">list</span>(<span class="at">labels =</span> dat<span class="sc">$</span>occ, <span class="at">n =</span> <span class="dv">2</span>))</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-176-1.png" width="672" /></p>
<pre><code>## newsboys  farmers 
##       53       67</code></pre>
<p>By default it always labels the two most extreme values and tells you what rows they are on. I have added the <code>n</code> parameter explicitly with the default value of 2 so that you can play around with it. I also used the <code>id</code> parameter to tell the function the names of the occupations. (Leave it out to see what happens.)</p>
<p>The residuals are on the y-axis and the theoretically expected quantiles on a standard (mean = 0, SD = 1) normal distribution are on the x-axis. <strong>If the data we provide are identical to the normal distribution, then the points should fall along the blue diagonal line.</strong> By default, a 95% confidence envelope is drawn on (the dashed curves); around 95% of the points should be within this envelope if the data we provide has the same distribution as the comparison distribution. Too many outside this suggests deviation from the reference distribution.</p>
<p>The residuals do indeed seem normally distributed. Read on to see what a Q-Q plot looks like when data are not normally distributed…</p>
<div id="illustration-using-simulated-data" class="section level4" number="6.4.2.1">
<h4><span class="header-section-number">6.4.2.1</span> Illustration using simulated data</h4>
<p>Let’s try a Q-Q plot for a skewed distribution which is definitely not normally distributed:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="linear-regression-diagnostics.html#cb255-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>) <span class="co"># this line means you get the same (pseudo)random numbers as me</span></span>
<span id="cb255-2"><a href="linear-regression-diagnostics.html#cb255-2" aria-hidden="true" tabindex="-1"></a>skewed <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="fu">rnorm</span>(<span class="dv">50</span>))</span>
<span id="cb255-3"><a href="linear-regression-diagnostics.html#cb255-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(skewed)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-177-1.png" width="672" /></p>
<p>Here is the <code>qqPlot</code>:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="linear-regression-diagnostics.html#cb256-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqPlot</span>(skewed)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-178-1.png" width="672" /></p>
<pre><code>## [1] 12  9</code></pre>
<p>The points do not all lie on the diagonal; indeed there is marked curvature.</p>
<p>Now, a sample we know to be normally distributed and large (2500 “participants”) so it will be easy to see its shape.</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="linear-regression-diagnostics.html#cb258-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">5</span>)</span>
<span id="cb258-2"><a href="linear-regression-diagnostics.html#cb258-2" aria-hidden="true" tabindex="-1"></a>perfectly_normal <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">2500</span>, <span class="dv">1000</span>, <span class="dv">200</span>)</span>
<span id="cb258-3"><a href="linear-regression-diagnostics.html#cb258-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(perfectly_normal)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-179-1.png" width="672" /></p>
<p>Here is the <code>qqPlot</code>:</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="linear-regression-diagnostics.html#cb259-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqPlot</span>(perfectly_normal)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-180-1.png" width="672" /></p>
<pre><code>## [1] 2112  230</code></pre>
<p>Note how two extreme points are always labelled; this does not mean they are outliers.</p>
</div>
</div>
<div id="statistical-test-of-normality" class="section level3 tabset" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> Statistical test of normality</h3>
<p>You could also use a statistical test of whether the residuals have a normal distribution, but it would not be a good idea. If the sample size is relatively small, then the test has low power; in other words, it won’t be very good at detecting non-normal data. If the sample size is large, then the test has high power to detect even minuscule deviations from a normal distribution – deviations so small that they won’t have any noticeable impact on results.</p>
<p>For completeness here is one such test, the Shapiro-Wilk normality test.</p>
<p>Let’s try it first on <code>perfectly_normal</code>:</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="linear-regression-diagnostics.html#cb261-1" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(perfectly_normal)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  perfectly_normal
## W = 0.99935, p-value = 0.5612</code></pre>
<p>The p-value is above 0.05 so this is “not statistically significant.” The test checks for <em>deviation</em> from a normal distribution, so we did not find evidence that the data is non-normal. Note my judicious use of negatives there; I do not commit myself to belief that the data are normal!</p>
<p>Now try again for the <code>skewed</code> data:</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="linear-regression-diagnostics.html#cb263-1" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(skewed)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  skewed
## W = 0.72855, p-value = 2.797e-08</code></pre>
<p>As one might hope, given the shape of the histogram, the p-value is very small: <span class="math inline">\(2.8 \times 10^{-8}\)</span>. We can be very confident from picture and this test that the data are <em>not</em> normally distributed.</p>
<div id="activity-19" class="section level4" number="6.4.3.1">
<h4><span class="header-section-number">6.4.3.1</span> Activity</h4>
<p>Try the Shapiro-Wilk normality test on the residuals of <code>mod_both</code>.</p>
</div>
<div id="answer-22" class="section level4" number="6.4.3.2">
<h4><span class="header-section-number">6.4.3.2</span> Answer</h4>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="linear-regression-diagnostics.html#cb265-1" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="fu">resid</span>(mod_both))</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(mod_both)
## W = 0.99402, p-value = 0.9371</code></pre>
<p>Since the p-value is far above 0.05 at 0.93, we did not find any evidence that the residuals are non-normal.</p>
</div>
</div>
</div>
<div id="checking-constant-residual-variance" class="section level2 tabset" number="6.5">
<h2><span class="header-section-number">6.5</span> Checking constant residual variance</h2>
<p>Linear regression assumes that residual variance is constant for all values of the predicted outcomes and predictors. If it is constant, then the residuals are said to be <em>homoscedastic</em>; otherwise if the variance varies then they are <em>heteroscedastic</em>.</p>
<p>Here is a picture of made up data to illustrate, focussing initially on the predicted outcomes. We want the residuals to be homoscedastic as shown in graph <em>a</em> on the left.</p>
<p><img src="R-notes_files/figure-html/unnamed-chunk-184-1.png" width="768" /></p>
<div id="activity-20" class="section level3" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Activity</h3>
<p>Above, we have fitted a model called <code>mod_both</code>. We already have the residuals saved in <code>dat$mod_both_resids</code>. You can get the predicted values of a model (i.e., ask the model to tell you a predicted prestige, based on rows of data for education and income) using the <code>predict</code> function; save them in <code>dat$mod_both_predicted</code>.</p>
<p>Plot the residuals against predicted outcomes and assess by visual inspection whether the residuals have constant variance.</p>
</div>
<div id="answer-23" class="section level3" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Answer</h3>
<p>First save the predicted values:</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="linear-regression-diagnostics.html#cb267-1" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>mod_both_predicted <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_both)</span></code></pre></div>
<p>Now plot them against the residuals:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="linear-regression-diagnostics.html#cb268-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> mod_both_predicted, <span class="at">y =</span> mod_both_resids)) <span class="sc">+</span></span>
<span id="cb268-2"><a href="linear-regression-diagnostics.html#cb268-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb268-3"><a href="linear-regression-diagnostics.html#cb268-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Predicted outcome&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residual&quot;</span>)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-186-1.png" width="672" /></p>
<p>The variance looks fairly constant across levels of the prediction, maybe decreasing a little as the prediction increases.</p>
<p>The next section provides an even faster way to check for constant variance for the predicted outcome and predictors.</p>
</div>
</div>
<div id="checking-for-relationships-between-residuals-and-predicted-outcome-or-predictors" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> Checking for relationships between residuals and predicted outcome or predictors</h2>
<p>There should be no relationship between</p>
<ol style="list-style-type: decimal">
<li>the residuals and the predicted outcome (also known as fitted values)</li>
<li>or between the residuals and any of the predictors.</li>
</ol>
<p>This includes the <em>mean</em> of the residuals as well as the <em>variance</em> introduced in the previous section. We can check both in the same plots with <code>residualPlots</code>.</p>
<p>The blue curves below show a quadratic function fitted by regression and can help spot any patterns in the mean of the residuals. In these graphs, the blue curve should ideally be a horizontal straight line (i.e., it should not be a curve!) and the points should be randomly scattered around it…</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="linear-regression-diagnostics.html#cb269-1" aria-hidden="true" tabindex="-1"></a><span class="fu">residualPlots</span>(mod_both, <span class="at">tests =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-187-1.png" width="576" /></p>
<p>(Look up the help for <code>residualPlots</code> to see what happens if you set <code>tests = TRUE</code>; but you probably don’t need that distraction now.)</p>
<p>The graph for income is a particular worry; we can zoom in and have a closer look:</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="linear-regression-diagnostics.html#cb270-1" aria-hidden="true" tabindex="-1"></a><span class="fu">residualPlots</span>(mod_both, <span class="at">terms =</span> <span class="sc">~</span> income_1000s,</span>
<span id="cb270-2"><a href="linear-regression-diagnostics.html#cb270-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">fitted =</span> <span class="cn">FALSE</span>,</span>
<span id="cb270-3"><a href="linear-regression-diagnostics.html#cb270-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">tests =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-188-1.png" width="672" /></p>
<p>(Look up the help for <code>residualPlots</code> to see what happens if you set <code>fitted = TRUE</code> – or give it a go!)</p>
<p>The curviness suggests that the mean of the residuals vary as a function of income, which suggets that we may wish to transform the income variable (more on this later). It’s tricky to see what is going on with the <em>variance</em> of the residuals.</p>
<p>Fox and Weisberg (2019, pp. 415-417) introduce a formal statistical test of non-constant variance called the <em>Breusch-Pagan test</em> or <em>non-constant variance score test</em>. For example, you can use it to check whether the residual variance varies along the magnitude of the predictions:</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="linear-regression-diagnostics.html#cb271-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ncvTest</span>(mod_both)</span></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 0.1664679, Df = 1, p = 0.68327</code></pre>
<p>This is not statistically significant, <span class="math inline">\(\chi^2(1) = 0.17\)</span>, <span class="math inline">\(p = .68\)</span>. So there is no evidence that the residual variance varies by predicted outcome.</p>
<p>You can also check predictors. Here is a test of whether the residual variance varies as a function of income:</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="linear-regression-diagnostics.html#cb273-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ncvTest</span>(mod_both, <span class="sc">~</span> income_1000s)</span></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ income_1000s 
## Chisquare = 2.019597, Df = 1, p = 0.15528</code></pre>
<p>This is also not statistically significant, <span class="math inline">\(\chi^2(1) = 2.02\)</span>, <span class="math inline">\(p = .16\)</span>.</p>
<p>See Fox and Weisberg (2019, pp. 244-252) for advice on what to do if you do find non-constant residual variance.</p>
</div>
<div id="checking-linearity" class="section level2" number="6.7">
<h2><span class="header-section-number">6.7</span> Checking linearity</h2>
<div id="what-should-be-linear-in-a-linear-model" class="section level3" number="6.7.1">
<h3><span class="header-section-number">6.7.1</span> What should be linear in a linear model?</h3>
<p>Linear regression models explain relationships between outcome and predictors that can be expressed in the form:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots \beta_n x_n
\]</span></p>
<p>Confusingly (perhaps?), this does not mean that linear regression can only model linear relationships, since we can transform <em>y</em> and the <em>x</em>’s in arbitrary ways.</p>
<p>Here is a made up dataset:</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="linear-regression-diagnostics.html#cb275-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">202</span>)</span>
<span id="cb275-2"><a href="linear-regression-diagnostics.html#cb275-2" aria-hidden="true" tabindex="-1"></a>made_up <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb275-3"><a href="linear-regression-diagnostics.html#cb275-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">y =</span> x<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x), <span class="dv">0</span>, .<span class="dv">5</span>))</span></code></pre></div>
<p>And a picture thereof:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="linear-regression-diagnostics.html#cb276-1" aria-hidden="true" tabindex="-1"></a>made_up_scatter <span class="ot">&lt;-</span> made_up <span class="sc">%&gt;%</span></span>
<span id="cb276-2"><a href="linear-regression-diagnostics.html#cb276-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb276-3"><a href="linear-regression-diagnostics.html#cb276-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb276-4"><a href="linear-regression-diagnostics.html#cb276-4" aria-hidden="true" tabindex="-1"></a>made_up_scatter</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-192-1.png" width="672" /></p>
<p>If you try to model <em>x</em> and <em>y</em> as-is, then the coefficients will be incorrect.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="linear-regression-diagnostics.html#cb277-1" aria-hidden="true" tabindex="-1"></a>wrong_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> made_up)</span>
<span id="cb277-2"><a href="linear-regression-diagnostics.html#cb277-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(wrong_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = made_up)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9242 -0.8842 -0.4939  0.4803  5.6926 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.97169    0.09793   9.922   &lt;2e-16 ***
## x            0.06607    0.09804   0.674    0.501    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.38 on 198 degrees of freedom
## Multiple R-squared:  0.002289,   Adjusted R-squared:  -0.00275 
## F-statistic: 0.4542 on 1 and 198 DF,  p-value: 0.5011</code></pre>
<p>Adding the regression line to the data shows why:</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="linear-regression-diagnostics.html#cb279-1" aria-hidden="true" tabindex="-1"></a>made_up_scatter <span class="sc">+</span></span>
<span id="cb279-2"><a href="linear-regression-diagnostics.html#cb279-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="fu">coef</span>(wrong_mod)[<span class="dv">1</span>],</span>
<span id="cb279-3"><a href="linear-regression-diagnostics.html#cb279-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">slope =</span> <span class="fu">coef</span>(wrong_mod)[<span class="dv">2</span>],</span>
<span id="cb279-4"><a href="linear-regression-diagnostics.html#cb279-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">colour =</span> <span class="st">&quot;purple&quot;</span>)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-194-1.png" width="672" /></p>
<p>However, if you transform the <em>x</em> first, then the predictions will be fine. Below, to illustrate, I am squaring <em>x</em>, so we are trying to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> for the model:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x^2
\]</span></p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="linear-regression-diagnostics.html#cb280-1" aria-hidden="true" tabindex="-1"></a>better_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> made_up)</span>
<span id="cb280-2"><a href="linear-regression-diagnostics.html#cb280-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(better_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ I(x^2), data = made_up)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.12429 -0.39648 -0.01204  0.31134  1.30659 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.01933    0.04573   0.423    0.673    
## I(x^2)       0.95993    0.02752  34.875   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.517 on 198 degrees of freedom
## Multiple R-squared:   0.86,  Adjusted R-squared:  0.8593 
## F-statistic:  1216 on 1 and 198 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The <code>I</code> inhibits R from trying to interpret <code>x^2</code> as anything other than arithmetic: <span class="math inline">\(x^2\)</span>.</p>
<p>Now let’s plot the model predictions:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="linear-regression-diagnostics.html#cb282-1" aria-hidden="true" tabindex="-1"></a>made_up<span class="sc">$</span>predicted <span class="ot">&lt;-</span> <span class="fu">predict</span>(better_mod)</span>
<span id="cb282-2"><a href="linear-regression-diagnostics.html#cb282-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb282-3"><a href="linear-regression-diagnostics.html#cb282-3" aria-hidden="true" tabindex="-1"></a>made_up <span class="sc">%&gt;%</span></span>
<span id="cb282-4"><a href="linear-regression-diagnostics.html#cb282-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb282-5"><a href="linear-regression-diagnostics.html#cb282-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb282-6"><a href="linear-regression-diagnostics.html#cb282-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x,</span>
<span id="cb282-7"><a href="linear-regression-diagnostics.html#cb282-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">y =</span> predicted),</span>
<span id="cb282-8"><a href="linear-regression-diagnostics.html#cb282-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">color =</span> <span class="st">&quot;purple&quot;</span>)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-196-1.png" width="672" /></p>
<p>That looks a lot better!</p>
<p>To see why “linear” regression is able to handle a decidedly non-linear relationship, look at <span class="math inline">\(y\)</span> plotted against <span class="math inline">\(x^2\)</span>:</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="linear-regression-diagnostics.html#cb283-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(made_up, <span class="fu">aes</span>(x<span class="sc">^</span><span class="dv">2</span>, y)) <span class="sc">+</span> </span>
<span id="cb283-2"><a href="linear-regression-diagnostics.html#cb283-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-197-1.png" width="672" /></p>
<p>This relationship <em>is</em> linear, so linear regression can describe it. Regression analysis doesn’t care what you do with the predictors before asking it to fit a linear model.</p>
<p>Chapter 3.4 of Fox and Weisberg (2019) introduces the art of data transformation. In social science, it is common to use log and polynomial transformations like squaring and cubing. In areas with developed theory, more complex relationships can be conjectured which mean something in the theory, rather than merely reacting to pattern in graphs.</p>
<p>It is possible to do all kinds of things to data to squeeze them into particular models, also known as “analysing the data to within an inch of its life.” Where possible, any transformations should be added to an analysis plan before the data are seen. Unanticipated transformations should be clearly noted in write-ups – this is crucially important.</p>
</div>
<div id="checking-for-linearity" class="section level3 tabset" number="6.7.2">
<h3><span class="header-section-number">6.7.2</span> Checking for linearity</h3>
<p>One thing you can do is look at pairwise relationships between the predictors and outcome variables with scatterplots. We already saw in last week’s tutorial a hint that there is a nonlinear relationship between income and prestige:</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="linear-regression-diagnostics.html#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(income, prestige)) <span class="sc">+</span></span>
<span id="cb284-2"><a href="linear-regression-diagnostics.html#cb284-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-198-1.png" width="672" /></p>
<p>Sometimes nonlinear relationships can only be spotted after a model has been fitted and variance explained by other predictors. A fab way to see is via a <strong>component-plus-residual plot</strong>, also known as a <strong>partial-residual plot</strong> (see Fox and Weisberg, 2019, pp. 410-412).</p>
<p>It’s a one-liner, showing the model prediction for that predictor as a straight dashed blue line and a local regression curve in magenta to help visualise the shape of the data. The magenta and blue ideally overlap.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="linear-regression-diagnostics.html#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crPlots</span>(mod_both)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-199-1.png" width="672" /></p>
<p>This is equivalent to the following line, which explicitly names all the predictors:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="linear-regression-diagnostics.html#cb286-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crPlots</span>(mod_both, <span class="at">terms =</span> <span class="sc">~</span> education <span class="sc">+</span> income_1000s)</span></code></pre></div>
<p>So you can select specific predictors by naming them, which can be helpful when you are struggling to squeeze all predictors onto the screen.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="linear-regression-diagnostics.html#cb287-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crPlots</span>(mod_both, <span class="at">terms =</span> <span class="sc">~</span> income_1000s)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-201-1.png" width="672" /></p>
<p>Each plot shows the predictor on the x-axis and the partial residuals on the y-axis. Each partial residual in the plot above is calculated as</p>
<p><span class="math display">\[\epsilon_i + \beta_{\mathtt{income\_1000s}} \mathtt{income\_1000s}_{i}\]</span></p>
<p>In other words, the residual, <span class="math inline">\(\epsilon_i\)</span>, plus the slope multiplied by income; i.e., it’s the prediction from this part of the model. These are also known as <em>component + residual plots</em>.</p>
<div id="activity-21" class="section level4" number="6.7.2.1">
<h4><span class="header-section-number">6.7.2.1</span> Activity</h4>
<p>What happens to the model if you use logged-income rather than income as a predictor?</p>
<ol style="list-style-type: lower-alpha">
<li>Calculate a new variable, with a name of your choice, which is equal to the log of <code>income_1000s</code> and add it to the data frame. To log data, using the <code>log</code> function.</li>
<li>Fit the regression model again, using this logged variable and education as predictors.</li>
<li>What impact does this have on the linearity of the predictor?</li>
<li>How do you now interpret the relationship between (logged) income and prestige?</li>
</ol>
</div>
<div id="answer-24" class="section level4" number="6.7.2.2">
<h4><span class="header-section-number">6.7.2.2</span> Answer</h4>
<p><strong>a. Calculate a new variable, with a name of your choice, which is equal to the log of <code>income_1000s</code> and add it to the data frame.</strong></p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="linear-regression-diagnostics.html#cb288-1" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>income_1000s_logged <span class="ot">&lt;-</span> <span class="fu">log</span>(dat<span class="sc">$</span>income_1000s)</span></code></pre></div>
<p><strong>b. Fit the regression model again, using this logged variable and education as predictors.</strong></p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="linear-regression-diagnostics.html#cb289-1" aria-hidden="true" tabindex="-1"></a>mod_both_again <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> education <span class="sc">+</span> income_1000s_logged, <span class="at">data =</span> dat)</span>
<span id="cb289-2"><a href="linear-regression-diagnostics.html#cb289-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_both_again)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = prestige ~ education + income_1000s_logged, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.0346  -4.5657  -0.1857   4.0577  18.1270 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         -16.1868     2.9662  -5.457 3.58e-07 ***
## education             4.0020     0.3115  12.846  &lt; 2e-16 ***
## income_1000s_logged  11.4375     1.4371   7.959 2.94e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.145 on 99 degrees of freedom
## Multiple R-squared:  0.831,  Adjusted R-squared:  0.8275 
## F-statistic: 243.3 on 2 and 99 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>c. What impact does this have on the linearity of the predictor?</strong></p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="linear-regression-diagnostics.html#cb291-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crPlots</span>(mod_both_again)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-204-1.png" width="672" /></p>
<p>This looks much better!</p>
<p><strong>d. How do you now interpret the relationship between (logged) income and prestige?</strong></p>
<p>Both predictors were statistically significant (see the summary above) so we can focus on the coefficients:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="linear-regression-diagnostics.html#cb292-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod_both_again) <span class="sc">%&gt;%</span></span>
<span id="cb292-2"><a href="linear-regression-diagnostics.html#cb292-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>##         (Intercept)           education income_1000s_logged 
##               -16.2                 4.0                11.4</code></pre>
<p>Prestige increases by 11.4 points for every unit increase in logged income, whilst holding education constant. Prestige increases by 4 points for every year of education, whilst holding logged income constant.</p>
<p>The logged predictor is challenging to interpret; when we get to logistic regression I will introduce another way to graph predictions which may make it easier to make sense of whether they have practical significance.</p>
</div>
</div>
</div>
<div id="checking-influence-leave-one-out-analyses" class="section level2" number="6.8">
<h2><span class="header-section-number">6.8</span> Checking influence: leave-one-out analyses</h2>
<p>Leave-one-out analyses check that results haven’t been unduly influenced by a small number of unusual data points. They do what the name suggests:</p>
<ol start="0" style="list-style-type: decimal">
<li>Fit a model.</li>
<li>For every row of data:
<ol style="list-style-type: lower-alpha">
<li>Remove the row.</li>
<li>Refit the model.</li>
<li>Calculate a statistic comparing the model on all data with the model which has this one row removed.</li>
<li>Replace the row and go onto the next one.</li>
</ol></li>
<li>Summarise the effect for every observation in the dataset. The end result will be as many leave-one-out statistics as there are rows in the data frame.</li>
<li>Then it is up to you, the analyst, to decide what to do with any data points identified.</li>
</ol>
<p>We will consider a range of leave-one-out statistics below, but first a picture which I hope is helpful.</p>
<p>Here is small made up dataset with a naughty data point up at the top right.</p>
<p><img src="R-notes_files/figure-html/unnamed-chunk-206-1.png" width="672" /></p>
<p>Here is an animation showing what happens to the regression slope when each row of data is removed; note what happens to the regression line when the sixth is dropped out:</p>
<p><img src="R-notes_files/figure-html/unnamed-chunk-207-.gif" width="672" /></p>
<p>There are many (many) ways to assess the impact. I will cover three below:</p>
<div id="residual-outliers" class="section level3" number="6.8.1">
<h3><span class="header-section-number">6.8.1</span> Residual outliers</h3>
<p>You can test whether there are any outlier residuals with a Bonferroni Outlier Test. This is equivalent to dropping each observation in turn and seeing whether it leads to a mean shift in model estimates. The “Bonferroni” part refers to an p-value adjustment that accounts for the number of tests carried out, equal to the number of rows in the dataset.</p>
<p>Here is how to use it – simply provide the model to test:</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="linear-regression-diagnostics.html#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="fu">outlierTest</span>(mod_both)</span></code></pre></div>
<pre><code>## No Studentized residuals with Bonferroni p &lt; 0.05
## Largest |rstudent|:
##     rstudent unadjusted p-value Bonferroni p
## 53 -2.596087           0.010879           NA</code></pre>
<p>The <em>unadjusted</em> p-value is less than 0.05; however, that p-value is an underestimate, given the large number of tests carried out, The Bonferroni-adjusted p-value is not provided since it is very high (it would be 1). So we have no evidence of any outliers.</p>
</div>
<div id="cooks-distance" class="section level3" number="6.8.2">
<h3><span class="header-section-number">6.8.2</span> Cook’s distance</h3>
<p>Cook’s distance measures the combined impact on all model coefficients (i.e., the intercept and slopes) of leaving out a row of data.</p>
<p>You can calculate it as follows (I will save the results onto <code>dat</code>):</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="linear-regression-diagnostics.html#cb296-1" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>mod_both_cooks <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(mod_both)</span>
<span id="cb296-2"><a href="linear-regression-diagnostics.html#cb296-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb296-3"><a href="linear-regression-diagnostics.html#cb296-3" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>mod_both_cooks <span class="sc">%&gt;%</span></span>
<span id="cb296-4"><a href="linear-regression-diagnostics.html#cb296-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##   [1] 0.00 0.28 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.03
##  [16] 0.01 0.00 0.00 0.03 0.06 0.02 0.00 0.00 0.07 0.05 0.04 0.03 0.00 0.05 0.00
##  [31] 0.03 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00
##  [46] 0.02 0.00 0.01 0.01 0.00 0.01 0.02 0.07 0.03 0.00 0.00 0.00 0.00 0.00 0.00
##  [61] 0.02 0.00 0.01 0.01 0.02 0.01 0.05 0.01 0.00 0.01 0.01 0.00 0.00 0.01 0.00
##  [76] 0.00 0.00 0.00 0.01 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03
##  [91] 0.02 0.01 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.00</code></pre>
<p>Some statisticians suggest that a value over 1 indicates trouble (we see another threshold shortly), so we can look at the maximum value:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="linear-regression-diagnostics.html#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(dat<span class="sc">$</span>mod_both_cooks)</span></code></pre></div>
<pre><code>## [1] 0.2796504</code></pre>
<p>That is fine.</p>
<p>Others suggest eyeing up the data and seeing if any values for Cook’s distance looks visually large relative to the others:</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="linear-regression-diagnostics.html#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dat<span class="sc">$</span>mod_both_cooks, <span class="at">ylab =</span> <span class="st">&quot;Cook&#39;s distance&quot;</span>)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-211-1.png" width="672" /></p>
<p>The “Index” ranges from 1 to the total number of values.</p>
<p>Just one stands out – which is also the maximum value we just identified. We can use <code>filter</code> to have a look:</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="linear-regression-diagnostics.html#cb301-1" aria-hidden="true" tabindex="-1"></a>dat <span class="sc">%&gt;%</span></span>
<span id="cb301-2"><a href="linear-regression-diagnostics.html#cb301-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(mod_both_cooks <span class="sc">&gt;</span> <span class="fl">0.25</span>)</span></code></pre></div>
<pre><code>##                occ education income women prestige type income_1000s
## 1 general managers     12.26  25879  4.02     69.1 prof       25.879
##   mod_both_resids mod_both_predicted income_1000s_logged mod_both_cooks
## 1        -10.0029            79.1029            3.253432      0.2796504</code></pre>
<p>Note that I chose the 0.25 threshold by simply looking at the graph.</p>
</div>
<div id="dfbeta-and-close-sibling-dfbetas" class="section level3" number="6.8.3">
<h3><span class="header-section-number">6.8.3</span> DFBETA and (close sibling) DFBETAS</h3>
<p>DFBETA values (my favourite) are calculated for the intercept and each slope. They simply denote the difference in the coefficients between models with versus without a particular row of data. If the coefficient without an observation is larger, then the DEBETA for that observation and predictor will be positive.</p>
<p>Calculate them with:</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="linear-regression-diagnostics.html#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dfbeta</span>(mod_both)  <span class="sc">%&gt;%</span></span>
<span id="cb303-2"><a href="linear-regression-diagnostics.html#cb303-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##     (Intercept) education income_1000s
## 1        -0.076     0.003        0.013
## 2        -0.408     0.152       -0.200
## 3        -0.086     0.011        0.003
## 4         0.015    -0.001        0.005
## 5        -0.436     0.057       -0.014
## 6        -0.299     0.033       -0.001
## 7        -0.349     0.046       -0.012
## 8        -0.094     0.007        0.005
## 9        -0.187     0.019        0.004
## 10       -0.003     0.000        0.000
## 11       -0.189     0.039       -0.019
## 12       -0.098     0.019       -0.006
## 13        0.305    -0.041        0.008
## 14        0.082    -0.011        0.003
## 15       -0.610     0.086       -0.028
## 16        0.274    -0.041        0.017
## 17        0.111    -0.002       -0.017
## 18        0.095    -0.015        0.006
## 19        0.646    -0.078        0.011
## 20       -0.797     0.127       -0.064
## 21       -0.551     0.055        0.007
## 22       -0.100     0.016       -0.008
## 23        0.023    -0.003        0.001
## 24        0.261     0.027       -0.092
## 25        0.739    -0.060       -0.033
## 26        0.307     0.000       -0.060
## 27       -0.332     0.071       -0.043
## 28       -0.049     0.000        0.004
## 29       -0.674     0.114       -0.058
## 30        0.056    -0.006        0.000
## 31       -0.408     0.078       -0.042
## 32        0.026     0.011       -0.007
## 33       -0.033     0.005       -0.001
## 34        0.004     0.001        0.002
## 35        0.007    -0.002        0.002
## 36        0.035    -0.013        0.011
## 37       -0.020     0.011       -0.009
## 38        0.002     0.006       -0.006
## 39       -0.010     0.005       -0.004
## 40       -0.208     0.011        0.003
## 41        0.320    -0.083        0.062
## 42        0.018    -0.015        0.014
## 43       -0.087     0.006        0.000
## 44       -0.037    -0.003        0.006
## 45       -0.014    -0.006        0.008
## 46        0.044    -0.041        0.034
## 47       -0.004     0.011       -0.009
## 48        0.038    -0.024        0.013
## 49        0.008    -0.022        0.021
## 50       -0.067     0.006       -0.003
## 51       -0.089     0.008       -0.015
## 52       -0.135    -0.021        0.036
## 53       -0.286    -0.043        0.080
## 54       -0.183    -0.024        0.044
## 55        0.006    -0.004       -0.002
## 56       -0.007    -0.001        0.000
## 57        0.012     0.000        0.001
## 58       -0.034     0.004       -0.003
## 59        0.013    -0.001        0.002
## 60        0.015    -0.001        0.000
## 61       -0.545     0.032        0.010
## 62        0.111    -0.008        0.007
## 63       -0.121    -0.015        0.031
## 64       -0.394     0.028        0.004
## 65       -0.632     0.049        0.001
## 66       -0.514     0.037        0.003
## 67        1.209    -0.099        0.005
## 68       -0.319     0.004        0.026
## 69       -0.162     0.015       -0.006
## 70        0.508    -0.040        0.002
## 71       -0.380     0.033       -0.006
## 72        0.177    -0.015        0.003
## 73       -0.175     0.010        0.005
## 74        0.469    -0.042        0.007
## 75        0.210    -0.017        0.001
## 76       -0.079     0.008       -0.005
## 77        0.230    -0.021        0.007
## 78       -0.046     0.004       -0.001
## 79        0.400    -0.039        0.013
## 80       -0.002     0.000        0.000
## 81        0.222    -0.020        0.007
## 82        0.579    -0.030       -0.014
## 83       -0.084    -0.001        0.005
## 84        0.356    -0.029        0.000
## 85        0.184    -0.016        0.004
## 86        0.110    -0.010        0.006
## 87        0.012    -0.001        0.000
## 88       -0.043     0.005       -0.003
## 89        0.151    -0.012        0.006
## 90        0.678    -0.079        0.043
## 91        0.705    -0.067        0.016
## 92        0.601    -0.061        0.019
## 93       -0.094     0.008       -0.001
## 94        0.297    -0.029        0.011
## 95       -0.176     0.014        0.000
## 96        0.005    -0.007        0.016
## 97        0.455    -0.053        0.030
## 98        0.230    -0.021        0.005
## 99       -0.336     0.025        0.000
## 100      -0.364     0.027       -0.001
## 101      -0.024     0.001        0.000
## 102       0.068    -0.004       -0.002</code></pre>
<p>Visualise them with:</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="linear-regression-diagnostics.html#cb305-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dfbetaPlots</span>(mod_both)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-214-1.png" width="672" /></p>
<p>To work out whether a DFBETA matters, you must think what the units of the predictors are. So for example there is an observation which reduces the slope for income in $1000s by about 0.2. Is that big enough to cause concern for a measure of prestige which ranges from 0 to 100? Maybe it is worth a look to see which occupation is responsible.</p>
<p>There is also a standardised version, DFBETAS (the S is for standardised, not a plural; pronounce it as “standardised D F beta”), which divides the DFBETA by the standard error of the slope in the model with the row of data removed.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="linear-regression-diagnostics.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dfbetas</span>(mod_both) <span class="sc">%&gt;%</span></span>
<span id="cb306-2"><a href="linear-regression-diagnostics.html#cb306-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##     (Intercept) education income_1000s
## 1         -0.02      0.01         0.06
## 2         -0.13      0.44        -0.90
## 3         -0.03      0.03         0.01
## 4          0.00      0.00         0.02
## 5         -0.14      0.16        -0.06
## 6         -0.09      0.09         0.00
## 7         -0.11      0.13        -0.05
## 8         -0.03      0.02         0.02
## 9         -0.06      0.05         0.02
## 10         0.00      0.00         0.00
## 11        -0.06      0.11        -0.09
## 12        -0.03      0.05        -0.03
## 13         0.09     -0.12         0.03
## 14         0.03     -0.03         0.01
## 15        -0.19      0.25        -0.12
## 16         0.08     -0.12         0.07
## 17         0.03     -0.01        -0.07
## 18         0.03     -0.04         0.03
## 19         0.20     -0.22         0.05
## 20        -0.25      0.37        -0.29
## 21        -0.17      0.16         0.03
## 22        -0.03      0.05        -0.03
## 23         0.01     -0.01         0.00
## 24         0.08      0.08        -0.41
## 25         0.23     -0.17        -0.15
## 26         0.10      0.00        -0.27
## 27        -0.10      0.21        -0.20
## 28        -0.02      0.00         0.02
## 29        -0.21      0.33        -0.26
## 30         0.02     -0.02         0.00
## 31        -0.13      0.23        -0.19
## 32         0.01      0.03        -0.03
## 33        -0.01      0.02        -0.01
## 34         0.00      0.00         0.01
## 35         0.00     -0.01         0.01
## 36         0.01     -0.04         0.05
## 37        -0.01      0.03        -0.04
## 38         0.00      0.02        -0.03
## 39         0.00      0.01        -0.02
## 40        -0.06      0.03         0.02
## 41         0.10     -0.24         0.28
## 42         0.01     -0.04         0.06
## 43        -0.03      0.02         0.00
## 44        -0.01     -0.01         0.02
## 45         0.00     -0.02         0.04
## 46         0.01     -0.12         0.16
## 47         0.00      0.03        -0.04
## 48         0.01     -0.07         0.06
## 49         0.00     -0.06         0.10
## 50        -0.02      0.02        -0.01
## 51        -0.03      0.02        -0.07
## 52        -0.04     -0.06         0.16
## 53        -0.09     -0.13         0.37
## 54        -0.06     -0.07         0.20
## 55         0.00     -0.01        -0.01
## 56         0.00      0.00         0.00
## 57         0.00      0.00         0.00
## 58        -0.01      0.01        -0.01
## 59         0.00      0.00         0.01
## 60         0.00      0.00         0.00
## 61        -0.17      0.09         0.04
## 62         0.03     -0.02         0.03
## 63        -0.04     -0.04         0.14
## 64        -0.12      0.08         0.02
## 65        -0.20      0.14         0.00
## 66        -0.16      0.11         0.01
## 67         0.38     -0.29         0.02
## 68        -0.10      0.01         0.12
## 69        -0.05      0.04        -0.03
## 70         0.16     -0.12         0.01
## 71        -0.12      0.10        -0.03
## 72         0.05     -0.04         0.01
## 73        -0.05      0.03         0.02
## 74         0.15     -0.12         0.03
## 75         0.07     -0.05         0.00
## 76        -0.02      0.02        -0.02
## 77         0.07     -0.06         0.03
## 78        -0.01      0.01        -0.01
## 79         0.12     -0.11         0.06
## 80         0.00      0.00         0.00
## 81         0.07     -0.06         0.03
## 82         0.18     -0.09        -0.07
## 83        -0.03      0.00         0.02
## 84         0.11     -0.08         0.00
## 85         0.06     -0.05         0.02
## 86         0.03     -0.03         0.03
## 87         0.00      0.00         0.00
## 88        -0.01      0.01        -0.01
## 89         0.05     -0.03         0.02
## 90         0.21     -0.23         0.20
## 91         0.22     -0.19         0.07
## 92         0.19     -0.17         0.08
## 93        -0.03      0.02         0.00
## 94         0.09     -0.08         0.05
## 95        -0.05      0.04         0.00
## 96         0.00     -0.02         0.07
## 97         0.14     -0.15         0.13
## 98         0.07     -0.06         0.02
## 99        -0.10      0.07         0.00
## 100       -0.11      0.08         0.00
## 101       -0.01      0.00         0.00
## 102        0.02     -0.01        -0.01</code></pre>
<p>Visualise DFBETAS with:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="linear-regression-diagnostics.html#cb308-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dfbetasPlots</span>(mod_both)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-216-1.png" width="672" /></p>
<p>Note how the graphs look identical to those for DFBETA except that the y-axis scale has changed.</p>
<p>Some statisticians argue that a DFBETAS (i.e., the standardised one) value over <span class="math inline">\(1\)</span> or below <span class="math inline">\(-1\)</span> is potentially troublesome, so should be inspected.</p>
</div>
<div id="view-them-all" class="section level3" number="6.8.4">
<h3><span class="header-section-number">6.8.4</span> View them all</h3>
<p>You can obtain a HUGE data frame of leave-one-out-analyses by using the <code>influence.measures</code> command. This gives the following measures.</p>
<table>
<thead>
<tr class="header">
<th>Measure</th>
<th>Thresholds used by R</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DFBETAS (for each model variable)</td>
<td><span class="math inline">\(|\mathtt{DFBETAS}| &gt; 1\)</span></td>
</tr>
<tr class="even">
<td>DFFIT</td>
<td><span class="math inline">\(|\mathtt{DFFIT}| &gt; 3 \sqrt{k/(n - k)}\)</span></td>
</tr>
<tr class="odd">
<td>covariance ratios (COVRATIO)</td>
<td><span class="math inline">\(|1 - \mathtt{COVRATIO}| &gt; 3k/(n - k)\)</span></td>
</tr>
<tr class="even">
<td>Cook’s distance</td>
<td>Over the median of <span class="math inline">\(F(k, n-k)\)</span></td>
</tr>
<tr class="odd">
<td>Diagonal elements of the hat matrix</td>
<td>Over <span class="math inline">\(3k/n\)</span></td>
</tr>
</tbody>
</table>
<p>The R help for this command is spectacularly poor so I have added in the thresholds <a href="https://github.com/wch/r-source/blob/trunk/src/library/stats/R/lm.influence.R">as per the current R code</a> (it’s all open source), where <span class="math inline">\(k\)</span> is the number of predictors (including the intercept) and <span class="math inline">\(n\)</span> is the number of observations. The absolute value of <span class="math inline">\(x\)</span>, written <span class="math inline">\(|x|\)</span>, means remove any negative sign, so <span class="math inline">\(|-42| = 42\)</span>. Any values over the relevant threshold are marked with an asterisk.</p>
<p>Bollen and Jackman (1985) provide alternative recommendations, e.g., <span class="math inline">\(2/\sqrt{n}\)</span> for DFBETAS. Others refuse to name a threshold and instead emphasise looking at the pattern of values and using subjective judgement to determine whether any are outlying.</p>
<p>Let’s have a look at all the influence measures for our model, <code>mod_both</code>.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="linear-regression-diagnostics.html#cb309-1" aria-hidden="true" tabindex="-1"></a>mod_both_influence <span class="ot">&lt;-</span> <span class="fu">influence.measures</span>(mod_both)</span>
<span id="cb309-2"><a href="linear-regression-diagnostics.html#cb309-2" aria-hidden="true" tabindex="-1"></a>mod_both_influence</span></code></pre></div>
<pre><code>## Influence measures of
##   lm(formula = prestige ~ education + income_1000s, data = dat) :
## 
##        dfb.1_  dfb.edct  dfb.i_10     dffit cov.r   cook.d     hat inf
## 1   -0.023428  0.008369  5.92e-02  0.098882 1.048 3.28e-03 0.02693    
## 2   -0.127604  0.439484 -9.00e-01 -0.921848 1.321 2.80e-01 0.27146   *
## 3   -0.026683  0.030933  1.15e-02  0.077727 1.035 2.03e-03 0.01564    
## 4    0.004730 -0.002131  2.34e-02  0.061706 1.034 1.28e-03 0.01217    
## 5   -0.135547  0.163813 -6.04e-02  0.201997 1.028 1.36e-02 0.03277    
## 6   -0.092549  0.093654 -3.14e-03  0.128644 1.063 5.55e-03 0.04179    
## 7   -0.108326  0.130767 -5.41e-02  0.153550 1.055 7.89e-03 0.03995    
## 8   -0.028984  0.021024  2.15e-02  0.052012 1.080 9.10e-04 0.04733    
## 9   -0.058001  0.053674  1.95e-02  0.099890 1.052 3.35e-03 0.02997    
## 10  -0.001008  0.000979  1.94e-04  0.001639 1.063 9.05e-07 0.03048    
## 11  -0.058743  0.110713 -8.53e-02  0.168031 1.002 9.36e-03 0.01810    
## 12  -0.030339  0.053875 -2.70e-02  0.097830 1.025 3.20e-03 0.01413    
## 13   0.094863 -0.117254  3.49e-02 -0.162101 1.022 8.75e-03 0.02361    
## 14   0.025302 -0.031457  1.30e-02 -0.038651 1.063 5.03e-04 0.03158    
## 15  -0.191087  0.248192 -1.24e-01  0.297268 0.986 2.90e-02 0.03303    
## 16   0.084795 -0.118001  7.46e-02 -0.138343 1.053 6.41e-03 0.03641    
## 17   0.034406 -0.005921 -7.45e-02 -0.100998 1.136 3.43e-03 0.09547   *
## 18   0.029378 -0.041536  2.73e-02 -0.048705 1.068 7.98e-04 0.03689    
## 19   0.201674 -0.223985  5.15e-02 -0.282897 1.007 2.64e-02 0.03777    
## 20  -0.250322  0.368606 -2.86e-01  0.419809 0.992 5.75e-02 0.05349    
## 21  -0.171334  0.157351  3.17e-02  0.244508 1.043 1.99e-02 0.04700    
## 22  -0.030898  0.046882 -3.41e-02  0.057072 1.062 1.10e-03 0.03238    
## 23   0.007169 -0.008766  3.87e-03 -0.010199 1.075 3.50e-05 0.04074    
## 24   0.080937  0.076089 -4.10e-01 -0.466055 1.261 7.25e-02 0.20340   *
## 25   0.231448 -0.172875 -1.48e-01 -0.385996 1.006 4.89e-02 0.05363    
## 26   0.095555 -0.000033 -2.67e-01 -0.352122 1.060 4.11e-02 0.07268    
## 27  -0.104151  0.206102 -1.95e-01  0.289319 0.959 2.73e-02 0.02522    
## 28  -0.015223  0.000708  1.68e-02 -0.034136 1.045 3.92e-04 0.01584    
## 29  -0.212773  0.332168 -2.61e-01  0.401687 0.940 5.20e-02 0.03606    
## 30   0.017326 -0.018125  1.44e-03 -0.024827 1.069 2.08e-04 0.03652    
## 31  -0.128317  0.226702 -1.90e-01  0.304204 0.950 3.01e-02 0.02528    
## 32   0.008028  0.032435 -3.32e-02  0.130383 0.993 5.63e-03 0.01066    
## 33  -0.010208  0.015309 -5.88e-03  0.025523 1.046 2.19e-04 0.01581    
## 34   0.001229  0.002525  7.03e-03  0.032990 1.039 3.66e-04 0.01096    
## 35   0.002016 -0.006535  7.89e-03 -0.011300 1.053 4.30e-05 0.02103    
## 36   0.010853 -0.037897  5.00e-02 -0.064902 1.053 1.42e-03 0.02598    
## 37  -0.006050  0.030210 -3.87e-02  0.060053 1.043 1.21e-03 0.01754    
## 38   0.000672  0.015874 -2.87e-02  0.036888 1.055 4.58e-04 0.02479    
## 39  -0.003224  0.014731 -1.86e-02  0.028660 1.048 2.76e-04 0.01787    
## 40  -0.064468  0.031375  1.56e-02 -0.100111 1.022 3.35e-03 0.01340    
## 41   0.100886 -0.240817  2.81e-01 -0.354356 0.954 4.07e-02 0.03281    
## 42   0.005578 -0.041646  6.38e-02 -0.083023 1.048 2.31e-03 0.02424    
## 43  -0.026796  0.016210 -7.76e-04 -0.039571 1.041 5.27e-04 0.01287    
## 44  -0.011527 -0.007312  2.47e-02 -0.043438 1.043 6.35e-04 0.01538    
## 45  -0.004469 -0.018502  3.64e-02 -0.051660 1.047 8.97e-04 0.01958    
## 46   0.013861 -0.119945  1.55e-01 -0.269456 0.909 2.33e-02 0.01512   *
## 47  -0.001148  0.030194 -3.91e-02  0.075892 1.032 1.93e-03 0.01364    
## 48   0.011822 -0.069139  5.78e-02 -0.186477 0.953 1.14e-02 0.01155    
## 49   0.002438 -0.063848  9.54e-02 -0.146894 1.010 7.18e-03 0.01711    
## 50  -0.020867  0.016934 -1.41e-02 -0.037097 1.041 4.63e-04 0.01271    
## 51  -0.027833  0.021911 -6.68e-02 -0.157596 0.982 8.20e-03 0.01220    
## 52  -0.042088 -0.060365  1.59e-01 -0.224638 0.981 1.66e-02 0.02103    
## 53  -0.091547 -0.125400  3.69e-01 -0.464748 0.872 6.81e-02 0.03105   *
## 54  -0.057555 -0.070047  1.99e-01 -0.278165 0.950 2.52e-02 0.02196    
## 55   0.001836 -0.010405 -1.02e-02 -0.066674 1.030 1.49e-03 0.01105    
## 56  -0.002038 -0.002340  6.57e-04 -0.018692 1.040 1.18e-04 0.00998    
## 57   0.003585 -0.001176  4.91e-03  0.019633 1.041 1.30e-04 0.01058    
## 58  -0.010555  0.011195 -1.14e-02 -0.017565 1.052 1.04e-04 0.02058    
## 59   0.004073 -0.003789  7.99e-03  0.016463 1.044 9.13e-05 0.01289    
## 60   0.004494 -0.002684 -1.04e-03  0.005532 1.055 1.03e-05 0.02256    
## 61  -0.171011  0.092960  4.36e-02 -0.231651 0.956 1.75e-02 0.01707    
## 62   0.034575 -0.023880  3.32e-02  0.099449 1.015 3.30e-03 0.01107    
## 63  -0.037589 -0.043482  1.38e-01 -0.173367 1.038 1.00e-02 0.03290    
## 64  -0.122230  0.079256  1.87e-02 -0.142306 1.034 6.77e-03 0.02570    
## 65  -0.196906  0.141189  2.46e-03 -0.218212 1.007 1.58e-02 0.02731    
## 66  -0.159958  0.107108  1.32e-02 -0.185839 1.010 1.15e-02 0.02319    
## 67   0.384108 -0.290933  2.39e-02  0.414395 0.902 5.47e-02 0.03011   *
## 68  -0.099258  0.012807  1.15e-01 -0.195151 1.009 1.26e-02 0.02443    
## 69  -0.050001  0.042853 -2.54e-02 -0.066229 1.040 1.47e-03 0.01687    
## 70   0.158012 -0.115778  9.18e-03  0.177763 1.014 1.05e-02 0.02347    
## 71  -0.117971  0.095052 -2.76e-02 -0.131785 1.033 5.81e-03 0.02360    
## 72   0.054762 -0.044123  1.28e-02  0.061175 1.051 1.26e-03 0.02360    
## 73  -0.054020  0.028264  2.34e-02 -0.070113 1.054 1.65e-03 0.02750    
## 74   0.145466 -0.120356  3.13e-02  0.154088 1.044 7.94e-03 0.03296    
## 75   0.065093 -0.049111  3.20e-03  0.070059 1.059 1.65e-03 0.03113    
## 76  -0.024506  0.021447 -2.27e-02 -0.049523 1.039 8.24e-04 0.01312    
## 77   0.071291 -0.060176  3.32e-02  0.092860 1.033 2.89e-03 0.01691    
## 78  -0.014151  0.012328 -6.57e-03 -0.017311 1.051 1.01e-04 0.01995    
## 79   0.124160 -0.111672  5.88e-02  0.144669 1.030 6.99e-03 0.02440    
## 80  -0.000579  0.000457 -1.64e-04 -0.000704 1.050 1.67e-07 0.01786    
## 81   0.068768 -0.057364  3.02e-02  0.088902 1.034 2.65e-03 0.01684    
## 82   0.182926 -0.087059 -6.57e-02  0.268211 0.917 2.32e-02 0.01597    
## 83  -0.026035 -0.001804  2.09e-02 -0.079813 1.024 2.13e-03 0.01081    
## 84   0.110300 -0.081606 -6.14e-04  0.118292 1.056 4.69e-03 0.03506    
## 85   0.056802 -0.046744  1.81e-02  0.066283 1.046 1.48e-03 0.02060    
## 86   0.034145 -0.027442  2.69e-02  0.069259 1.031 1.61e-03 0.01218    
## 87   0.003639 -0.003079  9.35e-04  0.003854 1.067 5.00e-06 0.03381    
## 88  -0.013165  0.013295 -1.15e-02 -0.019240 1.053 1.25e-04 0.02119    
## 89   0.046698 -0.033770  2.49e-02  0.086927 1.023 2.53e-03 0.01163    
## 90   0.211996 -0.229395  1.95e-01  0.287493 0.993 2.72e-02 0.03354    
## 91   0.219723 -0.191586  7.30e-02  0.236191 1.013 1.85e-02 0.03227    
## 92   0.186715 -0.173845  8.41e-02  0.201891 1.041 1.36e-02 0.03942    
## 93  -0.029161  0.021401 -2.52e-03 -0.033420 1.052 3.76e-04 0.02133    
## 94   0.091979 -0.083887  5.04e-02  0.113012 1.036 4.28e-03 0.02185    
## 95  -0.054561  0.038658 -5.22e-05 -0.061911 1.051 1.29e-03 0.02358    
## 96   0.001690 -0.021106  6.89e-02  0.083169 1.070 2.33e-03 0.04120    
## 97   0.141671 -0.152423  1.32e-01  0.197000 1.023 1.29e-02 0.03016    
## 98   0.071064 -0.060197  2.30e-02  0.079219 1.050 2.11e-03 0.02518    
## 99  -0.104242  0.071422  1.24e-03 -0.122924 1.029 5.05e-03 0.02029    
## 100 -0.112988  0.076393 -2.55e-03 -0.139650 1.014 6.49e-03 0.01727    
## 101 -0.007405  0.003961 -1.36e-03 -0.014894 1.041 7.47e-05 0.01062    
## 102  0.021023 -0.010209 -7.91e-03  0.029911 1.048 3.01e-04 0.01739</code></pre>
<p>That is Too Much Information.</p>
<p>The first three are the DFBETAS values, with abbreviated variable names:</p>
<ul>
<li><code>dfb.1_</code> (intercept)</li>
<li><code>dfb.edct</code> (education)</li>
<li><code>dfb.i_10</code> (income in 1000s)</li>
</ul>
<p>Cook’s distance is over to the right as <code>cook.d</code>.</p>
<p>One way to deal with this mess is to look only at those rows where any of the measures cross their threshold, which we get with a handy <code>summary</code> command (hurrah!):</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="linear-regression-diagnostics.html#cb311-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_both_influence)</span></code></pre></div>
<pre><code>## Potentially influential observations of
##   lm(formula = prestige ~ education + income_1000s, data = dat) :
## 
##    dfb.1_ dfb.edct dfb.i_10 dffit   cov.r   cook.d hat    
## 2  -0.13   0.44    -0.90    -0.92_*  1.32_*  0.28   0.27_*
## 17  0.03  -0.01    -0.07    -0.10    1.14_*  0.00   0.10_*
## 24  0.08   0.08    -0.41    -0.47    1.26_*  0.07   0.20_*
## 46  0.01  -0.12     0.16    -0.27    0.91_*  0.02   0.02  
## 53 -0.09  -0.13     0.37    -0.46    0.87_*  0.07   0.03  
## 67  0.38  -0.29     0.02     0.41    0.90_*  0.05   0.03</code></pre>
<p>There are no problems for any DFBETAS or Cook’s distance. Exercise to the interested reader to see if the values for DFFIT, covariance ratios, or diagonals of the hat matrix are a concern.</p>
</div>
<div id="so-er-what-should-we-do-with-potentially-influential-observations" class="section level3" number="6.8.5">
<h3><span class="header-section-number">6.8.5</span> So, er, what should we do with “potentially influential” observations…?</h3>
<p>Now we are back to the art of analysis.</p>
<p>You could try removing all the potentially influence observations by using <code>slice</code> with a <code>-</code> like so:</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="linear-regression-diagnostics.html#cb313-1" aria-hidden="true" tabindex="-1"></a>sliced_dat <span class="ot">&lt;-</span> dat <span class="sc">%&gt;%</span></span>
<span id="cb313-2"><a href="linear-regression-diagnostics.html#cb313-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">17</span>,<span class="dv">24</span>,<span class="dv">46</span>,<span class="dv">53</span>,<span class="dv">67</span>))</span></code></pre></div>
<p>Then refit the model:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="linear-regression-diagnostics.html#cb314-1" aria-hidden="true" tabindex="-1"></a>mod_both_sliced <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> education <span class="sc">+</span> income_1000s, <span class="at">data =</span> sliced_dat)</span>
<span id="cb314-2"><a href="linear-regression-diagnostics.html#cb314-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_both_sliced)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = prestige ~ education + income_1000s, data = sliced_dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9509  -4.3412  -0.1208   4.9519  16.3430 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -8.0248     3.0527  -2.629     0.01 *  
## education      4.0955     0.3453  11.862  &lt; 2e-16 ***
## income_1000s   1.6756     0.3010   5.566  2.5e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.212 on 93 degrees of freedom
## Multiple R-squared:  0.8077, Adjusted R-squared:  0.8035 
## F-statistic: 195.3 on 2 and 93 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Both predictors are still statistically significant… Have the coefficients changed?</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="linear-regression-diagnostics.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(<span class="st">&quot;Original&quot;</span>     <span class="ot">=</span> <span class="fu">coef</span>(mod_both),</span>
<span id="cb316-2"><a href="linear-regression-diagnostics.html#cb316-2" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Outliers out&quot;</span> <span class="ot">=</span> <span class="fu">coef</span>(mod_both_sliced)) <span class="sc">%&gt;%</span></span>
<span id="cb316-3"><a href="linear-regression-diagnostics.html#cb316-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##              Original Outliers out
## (Intercept)     -6.85        -8.02
## education        4.14         4.10
## income_1000s     1.36         1.68</code></pre>
<p>The coefficient for education is about the same; the coefficient for income has increased a little. At the moment, I don’t have any substantive reason to exclude these six values. Maybe if you have a developed theory of occupations then looking in more detail at the potentially outlying ones will help…? Here’s how to see them – I’m using <code>slice</code> again, this time without the <code>-</code> which means to keep rather than exclude the listed rows. I have also used <code>select</code> to focus on variables which I thought might help work out what is going on.</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="linear-regression-diagnostics.html#cb318-1" aria-hidden="true" tabindex="-1"></a>dat <span class="sc">%&gt;%</span></span>
<span id="cb318-2"><a href="linear-regression-diagnostics.html#cb318-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">17</span>,<span class="dv">24</span>,<span class="dv">46</span>,<span class="dv">53</span>,<span class="dv">67</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb318-3"><a href="linear-regression-diagnostics.html#cb318-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(occ, education, income_1000s, prestige, mod_both_predicted)</span></code></pre></div>
<pre><code>##                occ education income_1000s prestige mod_both_predicted
## 1 general managers     12.26       25.879     69.1           79.10290
## 2          lawyers     15.77       19.263     82.3           84.61986
## 3       physicians     15.96       25.308     87.2           93.63422
## 4       collectors     11.20        4.741     29.4           45.94489
## 5         newsboys      9.62        0.918     14.8           34.20399
## 6          farmers      6.84        3.643     44.1           26.41107</code></pre>
<p>Influential observations may indicate a more systemic problem with the model, e.g., non-linearity, especially if a large proportion of observations show up. They may also be qualitatively interesting and worthy of further research!</p>
</div>
</div>
<div id="checking-the-variance-inflation-factors-vifs" class="section level2 tabset" number="6.9">
<h2><span class="header-section-number">6.9</span> Checking the variance inflation factors (VIFs)</h2>
<p>Multicollinearity is a long word meaning that two or more predictors are highly linearly correlated with each other. This is a problem for interpreting coefficients since we interpret each one whilst holding the others constant. But if they are highly correlated, then holding other predictors constant is challenging!</p>
<p>There is a command called <code>vif</code> in the <code>car</code> package which calculates variance inflation factors for us and can be used to check for multicolinearity:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="linear-regression-diagnostics.html#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(mod_both)</span></code></pre></div>
<pre><code>##    education income_1000s 
##     1.500598     1.500598</code></pre>
<p>We want the VIFs close to 1. If they are 4 then that is cause for mild worry and 9 probably signals that something has to be done, e.g., removing a predictor.</p>
<p>We can interpret the meaning of VIFs by taking their square root: this says how many times wider the 95% confidence intervals are compared to what they would be with uncorrelated predictors:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="linear-regression-diagnostics.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">vif</span>(mod_both))</span></code></pre></div>
<pre><code>##    education income_1000s 
##     1.224989     1.224989</code></pre>
<p>So the trouble thresholds I provided above lead to confidence intervals that are twice (VIF = 4; <span class="math inline">\(\sqrt{4}=2\)</span>) or three times (VIF = 9; <span class="math inline">\(\sqrt{9}=3\)</span>) the width of those for uncorrelated predictors.</p>
<div id="activity-22" class="section level3" number="6.9.1">
<h3><span class="header-section-number">6.9.1</span> Activity</h3>
<p>Try fitting a model predicting prestige which has education, income (in 1000s) <em>and</em> logged income in thousands. Before looking, what do you suspect might happen to the VIFs…? Check and interpret the answer.</p>
</div>
<div id="answer-25" class="section level3" number="6.9.2">
<h3><span class="header-section-number">6.9.2</span> Answer</h3>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="linear-regression-diagnostics.html#cb324-1" aria-hidden="true" tabindex="-1"></a>viffed_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> education <span class="sc">+</span></span>
<span id="cb324-2"><a href="linear-regression-diagnostics.html#cb324-2" aria-hidden="true" tabindex="-1"></a>                     income_1000s <span class="sc">+</span></span>
<span id="cb324-3"><a href="linear-regression-diagnostics.html#cb324-3" aria-hidden="true" tabindex="-1"></a>                     income_1000s_logged,</span>
<span id="cb324-4"><a href="linear-regression-diagnostics.html#cb324-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> dat)</span>
<span id="cb324-5"><a href="linear-regression-diagnostics.html#cb324-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">vif</span>(viffed_model))</span></code></pre></div>
<pre><code>##           education        income_1000s income_1000s_logged 
##            1.230514            2.231250            2.177696</code></pre>
<p>The width of the confidence intervals for both income variables is double what they would be if all predictors were uncorrelated, which suggests something is up… The logging has meant that the predictors are not perfectly linearly correlated, but it is challenging to interpret income whilst holding logged income constant.</p>
</div>
</div>
<div id="the-challenge" class="section level2 tabset" number="6.10">
<h2><span class="header-section-number">6.10</span> The challenge</h2>
<p>As promised, here is a modelling challenge for you:</p>
<div id="activity-23" class="section level3" number="6.10.1">
<h3><span class="header-section-number">6.10.1</span> Activity</h3>
<ol style="list-style-type: lower-alpha">
<li>Fit a model predicting prestige from education, income (in $1000s, logged), and the percentage of women in the occupation</li>
<li>Does this model fit better than the model without the percentage of women added?</li>
<li>Is there multicollinearity?</li>
<li>Are the relationships linear?</li>
<li>Try adding the percentage of women squared as an additional predictor and checking for linearity again.</li>
<li>Does the model you just fitted (in <em>e</em>) explain statistically significantly more variance than the model with only income and education as predictors?</li>
<li>Check the DFBETAS values (again for the model fitted in <em>e</em>) – are any concerning?</li>
</ol>
</div>
<div id="answer-26" class="section level3" number="6.10.2">
<h3><span class="header-section-number">6.10.2</span> Answer</h3>
<p><strong>a. Fit a model predicting prestige from education, income (in $1000s, logged), and the percentage of women in the occupation</strong></p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="linear-regression-diagnostics.html#cb326-1" aria-hidden="true" tabindex="-1"></a>challenge_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> education <span class="sc">+</span></span>
<span id="cb326-2"><a href="linear-regression-diagnostics.html#cb326-2" aria-hidden="true" tabindex="-1"></a>                      income_1000s_logged <span class="sc">+</span></span>
<span id="cb326-3"><a href="linear-regression-diagnostics.html#cb326-3" aria-hidden="true" tabindex="-1"></a>                      women,</span>
<span id="cb326-4"><a href="linear-regression-diagnostics.html#cb326-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> dat)</span></code></pre></div>
<p><strong>b. Does this model fit better than the model without the percentage of women added?</strong></p>
<p>I’ve lost track of what models I have fitted above, so here is the relevant simpler model again:</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="linear-regression-diagnostics.html#cb327-1" aria-hidden="true" tabindex="-1"></a>without_gender_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> education <span class="sc">+</span> income_1000s_logged,</span>
<span id="cb327-2"><a href="linear-regression-diagnostics.html#cb327-2" aria-hidden="true" tabindex="-1"></a>                         <span class="at">data =</span> dat)</span></code></pre></div>
<p>Compare the two models with an F-test, simpler first:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="linear-regression-diagnostics.html#cb328-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(without_gender_mod, challenge_mod)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: prestige ~ education + income_1000s_logged
## Model 2: prestige ~ education + income_1000s_logged + women
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     99 5053.6                           
## 2     98 4929.9  1    123.75 2.4601   0.12</code></pre>
<p>There is no statistically significant improvement in model fit, <span class="math inline">\(F(1,98) = 2.5\)</span>, <span class="math inline">\(p = .12\)</span>.</p>
<p><strong>c. Is there multicollinearity?</strong></p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="linear-regression-diagnostics.html#cb330-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(challenge_mod)</span></code></pre></div>
<pre><code>##           education income_1000s_logged               women 
##            1.877097            2.572283            1.806431</code></pre>
<p>We can interpret the VIFs by taking their square root: this says how many times wider the 95% confidence intervals would be compared with uncorrelated predictors:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="linear-regression-diagnostics.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">vif</span>(challenge_mod))</span></code></pre></div>
<pre><code>##           education income_1000s_logged               women 
##            1.370072            1.603834            1.344035</code></pre>
<p>So the largest VIF is for income, and the correlations in predictors mean its confidence interval is about 1.6 times wider.</p>
<p>Based on finger-in-the-wind subjective judgement, I am going to conclude that this doesn’t matter. Though it may do if I wanted a particularly precise estimate.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="linear-regression-diagnostics.html#cb334-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Confint</span>(challenge_mod) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                     Estimate  2.5 % 97.5 %
## (Intercept)           -18.14 -24.48 -11.79
## education               3.73   3.03   4.43
## income_1000s_logged    13.44   9.64  17.24
## women                   0.05  -0.01   0.11</code></pre>
<p><strong>d. Are the relationships linear?</strong></p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="linear-regression-diagnostics.html#cb336-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crPlots</span>(challenge_mod)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-232-1.png" width="576" /></p>
<p>It looks like a bit of a curve for the predictor of percentage women in the occupation, which might suggest adding a squared term – which coincidentally we try in the next question…</p>
<p><strong>e. Try adding the percentage of women squared as an additional predictor and checking for linearity again</strong></p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="linear-regression-diagnostics.html#cb337-1" aria-hidden="true" tabindex="-1"></a>challenge_mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(prestige <span class="sc">~</span> education <span class="sc">+</span></span>
<span id="cb337-2"><a href="linear-regression-diagnostics.html#cb337-2" aria-hidden="true" tabindex="-1"></a>                       income_1000s_logged <span class="sc">+</span></span>
<span id="cb337-3"><a href="linear-regression-diagnostics.html#cb337-3" aria-hidden="true" tabindex="-1"></a>                       women <span class="sc">+</span></span>
<span id="cb337-4"><a href="linear-regression-diagnostics.html#cb337-4" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">I</span>(women<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb337-5"><a href="linear-regression-diagnostics.html#cb337-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> dat)</span></code></pre></div>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="linear-regression-diagnostics.html#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crPlots</span>(challenge_mod2)</span></code></pre></div>
<p><img src="R-notes_files/figure-html/unnamed-chunk-234-1.png" width="576" /></p>
<p>Now the relationships are much more linear.</p>
<p><strong>f. Does the model you just fitted (in <em>e</em>) explain statistically significantly more variance than the model with only income and education as predictors?</strong></p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="linear-regression-diagnostics.html#cb339-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(without_gender_mod, challenge_mod2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: prestige ~ education + income_1000s_logged
## Model 2: prestige ~ education + income_1000s_logged + women + I(women^2)
##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
## 1     99 5053.6                              
## 2     97 4679.8  2    373.88 3.8748 0.02405 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Yes it does, <span class="math inline">\(F(2, 97) = 3.9\)</span>, <span class="math inline">\(p = .024\)</span>.</p>
<p><strong>g. Check the DFBETAS values (again for the model fitted in <em>e</em>) – are any concerning?</strong></p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="linear-regression-diagnostics.html#cb341-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">influence.measures</span>(challenge_mod2))</span></code></pre></div>
<pre><code>## Potentially influential observations of
##   lm(formula = prestige ~ education + income_1000s_logged + women +      I(women^2), data = dat) :
## 
##    dfb.1_ dfb.edct dfb.i_10 dfb.womn dfb.I(^2 dffit   cov.r   cook.d hat    
## 2   0.07   0.08    -0.15     0.01    -0.03    -0.17    1.16_*  0.01   0.10  
## 20  0.07   0.69    -0.63    -0.32     0.13     0.80_*  0.89    0.12   0.10  
## 53 -0.13  -0.15     0.23     0.06     0.00    -0.24    1.46_*  0.01   0.29_*
## 63  0.30   0.32    -0.53    -0.32     0.30     0.70_*  1.28_*  0.10   0.24_*
## 67  0.53  -0.16    -0.18    -0.20     0.11     0.58    0.74_*  0.06   0.04  
## 68  0.05   0.03    -0.07     0.02    -0.04     0.09    1.18_*  0.00   0.11  
## 84  0.03  -0.07     0.05    -0.01     0.05     0.12    1.18_*  0.00   0.11</code></pre>
<p>No |DFBETAS| values are over 1, if that is your definition of concerning.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="categorical-predictors-and-interactions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["R-notes.pdf", "R-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
